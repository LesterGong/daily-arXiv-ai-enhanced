<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 123]
- [cs.CL](#cs.CL) [Total: 60]
- [cs.AI](#cs.AI) [Total: 48]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: PolypSeg-GradCAM is an explainable deep learning framework that integrates U-Net with Grad-CAM for transparent polyp segmentation in colonoscopy images, achieving high accuracy while providing interpretable visualizations.


<details>
  <summary>Details</summary>
Motivation: Colorectal cancer is a major health concern with polyps as critical precursors. Manual polyp segmentation is labor-intensive and variable, while existing deep learning methods lack interpretability needed for clinical adoption.

Method: The framework combines U-Net architecture with Gradient-weighted Class Activation Mapping (Grad-CAM) for explainable polyp segmentation. Trained and evaluated on the Kvasir-SEG dataset containing 1000 annotated endoscopic images.

Result: Achieved robust segmentation performance with mean IoU of 0.9257 on test set and consistently high Dice coefficients (F-score > 0.96) on training/validation sets. Grad-CAM visualizations confirmed predictions were guided by clinically relevant regions.

Conclusion: PolypSeg-GradCAM represents a step toward reliable, trustworthy AI-assisted colonoscopy by coupling high segmentation accuracy with interpretability, potentially improving early colorectal cancer prevention.

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [2] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: PerceptronCARE is a deep learning-based teleophthalmology application that uses multiple CNN architectures for automated diabetic retinopathy detection from retinal images, achieving 85.4% accuracy with cloud-based scalability for real-time screening in underserved areas.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy is a leading cause of vision loss globally, particularly affecting underserved regions where access to specialized healthcare is limited. There is a need for automated, scalable screening solutions to enable early diagnosis and reduce healthcare costs.

Method: The system was developed using multiple convolutional neural networks (ResNet-18, EfficientNet-B0, and SqueezeNet) to find the optimal balance between accuracy and computational efficiency. The application integrates cloud-based scalability, secure patient data management, and a multi-user framework.

Result: The final model achieved 85.4% accuracy in classifying diabetic retinopathy severity, enabling real-time screening capabilities suitable for clinical and telemedicine settings.

Conclusion: PerceptronCARE demonstrates the potential of AI-driven telemedicine solutions to expand access to diabetic retinopathy screening, particularly in remote and resource-constrained environments, facilitating early diagnosis and improving healthcare outcomes.

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [3] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: SIM is a data-intrinsic regularization framework that uses inverse mapping to reconstruct inputs from transformed outputs, reducing information loss and improving gradient flow. The efficient variant ρSIM uses patch-level sampling and projection to lower complexity.


<details>
  <summary>Details</summary>
Motivation: Conventional regularization techniques rely on heuristics and are less reliable across diverse settings. There's a need for more effective, model-agnostic regularization methods that can enhance representation learning.

Method: Proposes Self Identity Mapping (SIM) framework with inverse mapping mechanism. Introduces ρSIM variant using patch-level feature sampling and projection-based reconstruction to reduce computational complexity while maintaining effectiveness.

Result: ρSIM shows consistent improvements across image classification, few-shot prompt learning, and domain generalization tasks. It's orthogonal to existing regularization methods and works well in dense-to-dense tasks (semantic segmentation, image translation) and non-visual domains (audio classification, time series anomaly detection).

Conclusion: SIM/ρSIM is an effective, model-agnostic, task-agnostic regularization framework that can be seamlessly integrated as a plug-and-play module to enhance representation learning across various architectures and domains.

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [4] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: MAGIA is a momentum-based adaptive correction framework for gradient inversion attacks that enables high-fidelity multi-image reconstruction in large batch scenarios without requiring labels or auxiliary information.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of gradient inversion in single-round averaged gradient (SAG) regimes where per-sample cues are entangled within batch mean gradients, overcoming limitations of prior methods that fail in large batch scenarios.

Method: Uses momentum-based adaptive correction with two innovations: 1) closed-form combinatorial rescaling for tighter optimization bounds, and 2) momentum-based mixing of whole-batch and subset losses for reconstruction robustness. Probes random data subsets to sense latent per-image signals.

Result: Significantly outperforms advanced methods, achieving high-fidelity multi-image reconstruction in large batch scenarios where prior works fail, with computational footprint comparable to standard solvers.

Conclusion: MAGIA provides an effective label-free framework for gradient inversion attacks that works robustly in challenging SAG regimes, demonstrating superior performance over existing approaches.

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [5] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: Baseer is a vision-language model fine-tuned specifically for Arabic document OCR, achieving state-of-the-art performance with WER of 0.25 by leveraging domain-specific adaptation of general-purpose MLLMs.


<details>
  <summary>Details</summary>
Motivation: Arabic document OCR is challenging due to cursive script, diverse fonts, diacritics, and right-to-left orientation. Existing MLLMs perform poorly on Arabic, creating a need for specialized solutions.

Method: Fine-tuning a pre-trained MLLM using decoder-only strategy on large-scale dataset combining synthetic and real-world Arabic documents, while preserving general visual features. Also created Misraj-DocOCR benchmark for evaluation.

Result: Baseer significantly outperforms existing open-source and commercial solutions, achieving WER of 0.25 and establishing new state-of-the-art in Arabic document OCR.

Conclusion: Domain-specific adaptation of general-purpose MLLMs is highly beneficial for high-accuracy OCR on morphologically rich languages like Arabic, establishing a strong baseline for future research.

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [6] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: A novel deep learning framework transforms sparse InSAR time-series data into dense spatio-temporal tensors, enabling accurate ground deformation forecasting using a hybrid CNN-LSTM model.


<details>
  <summary>Details</summary>
Motivation: Forecasting future ground deformation from sparse InSAR time-series data is challenging but crucial for urban infrastructure stability and geological hazard mitigation.

Method: Hybrid CNN-LSTM model designed to simultaneously learn spatial patterns and temporal dependencies from generated spatio-temporal tensors, benchmarked against LightGBM and LASSO regression.

Result: The proposed architecture provides significantly more accurate and spatially coherent forecasts than baseline models, establishing a new performance benchmark for deformation forecasting.

Conclusion: Spatio-temporal deep learning is effective for high-resolution deformation forecasting, with interpretability analysis showing baseline models default to simplistic patterns while the integrated approach captures complex dynamics.

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [7] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: The Scrapbook framework generates extensive datasets to test AI models' understanding of basic concepts like object recognition, positions, and attributes through diverse linguistic questions.


<details>
  <summary>Details</summary>
Motivation: To systematically validate AI models' comprehension of fundamental concepts before tackling complex tasks, addressing gaps in current evaluation methods.

Method: Developed the Scrapbook framework that generates datasets with numerous questions about individual concepts and wide linguistic variations to probe model understanding.

Result: Contemporary models perform well in object recognition but struggle with positional information and constrained questions. MobileVLM-V2 showed significant answer disagreements, while other models exhibited affirmative bias and difficulties with geometric shapes and positions.

Conclusion: The Scrapbook framework provides a valuable tool for generating diverse datasets to systematically assess and improve AI model performance, revealing specific areas needing enhancement in concept understanding.

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [8] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: Empirical analysis shows substantial information loss in vision-language-vision pipelines, with 99.3% of samples exhibiting perceptual degradation and 91.5% showing structural loss when using natural language as an intermediate representation.


<details>
  <summary>Details</summary>
Motivation: To quantify the degradation that occurs when visual content passes through textual intermediation in multimodal AI systems, as this information loss remains poorly understood despite increasing integration in creative workflows.

Method: Generated 150 image pairs through describe-then-generate pipeline and applied existing metrics (LPIPS, SSIM, color distance) to measure information preservation across perceptual, structural, and chromatic dimensions.

Result: 99.3% of samples showed substantial perceptual degradation and 91.5% demonstrated significant structural information loss, indicating measurable and consistent limitations in contemporary multimodal systems.

Conclusion: The describe-then-generate bottleneck represents a significant limitation in multimodal AI systems, with empirical evidence showing consistent information loss across perceptual and structural dimensions when using natural language as an intermediate representation.

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [9] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: AI-driven workflow using satellite imagery to automatically infer rooftop attributes in small island developing states, achieving high F1 scores for roof pitch and material classification.


<details>
  <summary>Details</summary>
Motivation: Address data gap in structural building information for disaster risk reduction in climate-vulnerable regions like the Caribbean where such data is often unavailable.

Method: Compare geospatial foundation models with shallow classifiers against fine-tuned deep learning models for rooftop classification, and assess impact of additional training data from neighboring SIDS.

Result: Best models achieve F1 scores of 0.88 for roof pitch classification and 0.83 for roof material classification.

Conclusion: Combined with local capacity building, this approach provides SIDS with novel capabilities to use AI and Earth Observation data for more efficient, evidence-based urban governance.

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [10] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: The paper proposes VLA-LPAF, a lightweight module that enhances Visual-Language-Action (VLA) models' perspective adaptivity by fusing multiview observations in latent space using only 2D data.


<details>
  <summary>Details</summary>
Motivation: VLA models face perspective heterogeneity issues due to varied camera views across environments, which constrains their generality. Different perspectives result in significant visual feature differences that limit model performance.

Method: VLA-LPAF is finetuned using images from a single view and fuses other multiview observations in the latent space. The framework is instantiated with RoboFlamingo to create RoboFlamingo-LPAF.

Result: RoboFlamingo-LPAF achieves significant improvements: ~8% task success rate on CALVIN, 15% on LIBERO, and 30% on a customized simulation benchmark. Real-world tasks demonstrate developed view-adaptive characteristics.

Conclusion: VLA-LPAF effectively and efficiently bridges the gap caused by perspective inconsistency in VLA models, enabling better generalization across different camera views and environments.

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [11] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: URNet is an uncertainty-aware refinement network for event-based stereo depth estimation that uses local-global refinement and KL divergence-based uncertainty modeling to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer advantages like high temporal resolution, high dynamic range, and low latency compared to conventional frame-based cameras, but require specialized methods for accurate depth estimation.

Method: Proposes URNet with a local-global refinement module to capture both fine-grained local details and long-range global context, plus a KL divergence-based uncertainty modeling method to enhance prediction reliability.

Result: Extensive experiments on the DSEC dataset show URNet consistently outperforms state-of-the-art methods in both qualitative and quantitative evaluations.

Conclusion: The proposed URNet framework effectively addresses event-based stereo depth estimation by combining local-global refinement with uncertainty modeling, demonstrating superior performance over existing methods.

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [12] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: Visionerves is a hybrid AI framework that automatically recognizes peripheral nerves from MRI data using deep learning segmentation and symbolic spatial reasoning, achieving significant improvements over standard tractography for endometriosis patients.


<details>
  <summary>Details</summary>
Motivation: Endometriosis causes chronic pelvic pain with potential nerve involvement, but current imaging methods struggle to visualize peripheral nerves effectively. Conventional tractography requires manual ROI selection and lacks anatomical knowledge integration.

Method: A two-phase hybrid AI approach: (A) deep learning model for automatic anatomical structure segmentation, and (B) tractography and nerve recognition using symbolic spatial reasoning with fuzzy spatial relationships to encode anatomical knowledge.

Result: Tested on lumbosacral plexus in 10 women with endometriosis, Visionerves showed up to 25% Dice score improvement over standard tractography and reduced spatial errors to less than 5 mm.

Conclusion: Visionerves provides an automatic, reproducible method for detailed nerve analysis, enabling non-invasive diagnosis of endometriosis-related neuropathy and other nerve-involved conditions.

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [13] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: V-SenseDrive is the first privacy-preserving multimodal driver behavior dataset collected in Pakistan, combining smartphone sensor data with synchronized road-facing video to capture diverse driving behaviors in heterogeneous road conditions.


<details>
  <summary>Details</summary>
Motivation: Road traffic accidents are a major public health challenge in countries like Pakistan with mixed traffic conditions. Existing datasets from developed countries lack representation of behavioral diversity in emerging economies and often violate privacy by recording drivers' faces.

Method: Data was collected using a custom Android app that captures high-frequency accelerometer, gyroscope, and GPS streams alongside continuous road-facing video, with precise time alignment. The dataset covers three driving behaviors (normal, aggressive, risky) on various road types including urban arterials, secondary roads, and motorways.

Result: V-SenseDrive provides a structured dataset with raw, processed, and semantic layers, enabling multimodal analysis of driver behavior in real-world Pakistani driving conditions while preserving privacy.

Conclusion: This dataset fills a critical gap in global driver behavior datasets by representing real-world driving in Pakistan and provides groundwork for context-aware intelligent transportation solutions and ADAS development.

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [14] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: Qianfan-VL is a series of multimodal large language models (3B-70B parameters) that achieves state-of-the-art performance through domain enhancement techniques, multi-stage training, and high-precision data synthesis.


<details>
  <summary>Details</summary>
Motivation: To develop effective multimodal models with enhanced domain-specific capabilities while maintaining strong general performance, suitable for diverse enterprise deployment scenarios.

Method: Multi-stage progressive training and high-precision data synthesis pipelines, with models trained entirely on Baidu's Kunlun P800 chips achieving over 90% scaling efficiency on 5000 chips.

Result: State-of-the-art performance on benchmarks including CCBench, SEEDBench IMG, ScienceQA, MMStar, OCRBench (873), DocVQA (94.75%), and MathVista (78.6%). 8B and 70B variants feature long chain-of-thought capabilities for superior mathematical reasoning and logical inference.

Conclusion: The work establishes an effective methodology for developing domain-enhanced multimodal models, demonstrating the capability of large-scale AI infrastructure to train SOTA-level models with high efficiency.

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [15] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: HazeFlow is a novel ODE-based framework that reformulates atmospheric scattering model as an ODE to improve real-world image dehazing, using Rectified Flow for optimal trajectory mapping and Markov Chain Brownian Motion for realistic haze generation.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for image dehazing struggle with generalization to real-world scenarios due to lack of paired training data and domain gap. Traditional physics-based methods using Atmospheric Scattering Model fail to handle real-world complexities and diverse haze patterns.

Method: Proposes HazeFlow, an ODE-based framework that reformulates ASM as an ordinary differential equation, learning optimal ODE trajectory to map hazy to clean images with single inference step. Introduces non-homogeneous haze generation using Markov Chain Brownian Motion to create realistic training data.

Result: Extensive experiments show HazeFlow achieves state-of-the-art performance across various real-world dehazing benchmark datasets, demonstrating enhanced adaptability to diverse real-world scenarios.

Conclusion: HazeFlow effectively bridges the domain gap in real-world dehazing by combining physics-grounded learning with ODE-based optimization and realistic haze simulation, providing superior performance with efficient single-step inference.

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [16] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: This paper presents a compressed EcoWeedNet model for agricultural weed detection using structured channel pruning, quantization-aware training, and TensorRT acceleration, achieving significant size reduction and speed improvements while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Deploying deep learning models in agriculture is challenging due to limited resources on edge devices, requiring efficient models that can run on constrained hardware.

Method: The authors used structured channel pruning, quantization-aware training (QAT), and NVIDIA TensorRT acceleration on Jetson Orin Nano to compress EcoWeedNet, overcoming challenges from complex architectural elements like residual shortcuts, attention mechanisms, concatenations, and CSP blocks.

Result: The compressed model achieved 68.5% size reduction, 3.2 GFLOPs computation reduction, and 184 FPS inference speed (28.7% faster than baseline). On CottonWeedDet12 dataset, it outperformed YOLO11n and YOLO12n with 83.7% precision, 77.5% recall, and 85.9% mAP50.

Conclusion: The pruned EcoWeedNet with 39.5% pruning ratio proves to be both efficient and effective for precision agriculture applications, demonstrating successful deployment on resource-constrained edge devices.

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [17] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: A novel multimodal learning framework that integrates enhanced modality dropout and contrastive learning to handle modality imbalance and missingness in medical diagnosis, achieving state-of-the-art performance particularly in single-modality scenarios.


<details>
  <summary>Details</summary>
Motivation: Medical diagnoses increasingly use multimodal data, but real-world limitations include modality imbalance and missing modalities. Models need to effectively fuse heterogeneous information while remaining robust to missing data.

Method: Proposes a framework with learnable modality tokens for missingness-aware fusion, augments unimodal contrastive objectives with fused multimodal representations, and integrates enhanced modality dropout with contrastive learning.

Result: Achieves state-of-the-art performance on large-scale clinical datasets for disease detection and prediction, especially in challenging scenarios with only single modality available. Successfully integrates with CT foundation model, demonstrating adaptability.

Conclusion: The approach is effective, efficient, and generalizable for multimodal learning, offering a scalable, low-cost solution with significant potential for real-world clinical applications.

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [18] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: Systematic evaluation of 9 segmentation architectures for pulmonary embolism (PE) segmentation shows 3D U-Net with ResNet encoder performs best, CNN models outperform ViT models, and pretraining can hurt segmentation performance.


<details>
  <summary>Details</summary>
Motivation: To conduct a comprehensive performance audit of various segmentation architectures for pulmonary embolism detection in CTPA scans, identifying the most effective approaches and understanding key factors affecting segmentation performance.

Method: Used a densely annotated in-house dataset of 490 CTPA scans to evaluate 9 segmentation architectures from CNN and ViT families under unified testing framework, comparing pretrained vs randomly initialized weights.

Result: Best model achieved mean Dice score of 0.7131, detecting 181 emboli with 49 false positives and 28 false negatives on 60 test scans. 3D models outperformed 2D, CNN models beat ViT models, and training from scratch often performed better than pretraining.

Conclusion: 3D U-Net with ResNet encoder is optimal for PE segmentation, distal emboli remain challenging, and PE classification/segmentation rely on different features, suggesting pretraining on classification tasks may not benefit segmentation.

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [19] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: A graph neural network that separates temporal dynamics from static handshape configurations for improved handshape recognition in American Sign Language.


<details>
  <summary>Details</summary>
Motivation: Handshapes are fundamental in signed languages but computational approaches rarely model them explicitly, limiting recognition accuracy and linguistic analysis.

Method: Novel graph neural network combining anatomically-informed graph structures with contrastive learning to address challenges in handshape recognition, including subtle interclass distinctions and temporal variations.

Result: Established first benchmark for structured handshape recognition in signing sequences, achieving 46% accuracy across 37 handshape classes (baseline methods achieved 25%).

Conclusion: The approach successfully improves handshape recognition accuracy and enables better linguistic analysis of signed languages through explicit handshape modeling.

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [20] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: This paper investigates how different classification tasks affect out-of-distribution (OOD) detection performance in fetal ultrasound imaging, showing that optimal task selection depends on the type of OOD shift (image characteristic vs anatomical feature) and that good OOD detection doesn't necessarily translate to optimal abstained prediction.


<details>
  <summary>Details</summary>
Motivation: Reliable OOD detection is crucial for safe deployment of deep learning models in fetal ultrasound due to heterogeneous image characteristics and clinical settings. While existing research focuses on uncertainty quantification methods, this work examines the impact of the classification task itself on OOD detection performance.

Method: The study conducts experiments with eight uncertainty quantification methods across four different classification tasks to evaluate OOD detection performance under different ID-OOD criteria.

Result: OOD detection performance significantly varies with the classification task, and the best task depends on whether the OOD sample results from image characteristic shift or anatomical feature shift. Superior OOD detection does not guarantee optimal abstained prediction.

Conclusion: Task selection and uncertainty strategies must be aligned with specific downstream applications in medical image analysis, as the relationship between OOD detection and abstained prediction performance is complex and task-dependent.

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [21] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: OrthoLoC is a new large-scale dataset for UAV visual localization using orthographic geodata, addressing domain shifts between aerial imagery and geospatial data, with a novel refinement technique (AdHoP) that improves matching accuracy.


<details>
  <summary>Details</summary>
Motivation: Enable high-precision visual localization in resource-constrained environments (no internet/GPS) where large image databases or 3D models are impractical, by leveraging lightweight orthographic geodata that's increasingly available from governmental sources.

Method: Created OrthoLoC dataset with 16,425 UAV images from Germany and US with multiple modalities, using paired structure to decouple image retrieval from feature matching. Introduced AdHoP refinement technique that can be integrated with any feature matcher.

Result: Comprehensive evaluation shows impact of domain shifts, data resolutions, and covisibility on localization. AdHoP improves matching by up to 95% and reduces translation error by up to 63%.

Conclusion: OrthoLoC enables fair benchmarking of visual localization solutions and demonstrates that orthographic geodata is a viable lightweight alternative to traditional approaches, with AdHoP significantly enhancing matching performance.

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [22] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: SSDnet is a zero-shot anomaly detection method that uses single-image self-reconstruction with patch-based training and perceptual loss, achieving state-of-the-art performance without external training data.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of anomaly detection when training data or reference samples are unavailable, focusing on scenarios where only the test image itself is provided.

Method: Patch-based training framework where the input image is directly fed into the network for self-reconstruction with masking, patch shuffling, and Gaussian noise to prevent identity mapping. Uses perceptual loss based on inner-product similarity.

Result: Achieves 0.99 AUROC and 0.60 AUPRC on MVTec-AD, and 0.98 AUROC and 0.67 AUPRC on fabric dataset, outperforming state-of-the-art methods.

Conclusion: SSDnet provides an effective zero-shot anomaly localization solution that leverages deep image priors without requiring external training data, labels, or references.

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [23] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: A novel Bengali captioning pipeline with tri-loss objective (PAL+InfoNCE+OT) improves vision-language grounding for low-resource languages by aligning real and synthetic patch descriptors and reducing spurious matches.


<details>
  <summary>Details</summary>
Motivation: Grounding vision-language models in low-resource languages like Bengali is challenging due to scarce paired data, translation pivot issues, and English-centric pretraining that ignores target-language semantics.

Method: Uses a compute-aware pipeline with frozen MaxViT for visual patches, Bengali-native mBART-50 decoder, and lightweight bridge. Core innovation is tri-loss objective: Patch-Alignment Loss (PAL) for patch descriptor alignment, InfoNCE for global real-synthetic separation, and Sinkhorn-based OT for balanced fine-grained patch correspondence.

Result: Achieves strong performance on Flickr30k-1k (BLEU-4 12.29, METEOR 27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14, BERTScore-F1 75.40), outperforming CE baselines and reducing real-synthetic centroid gap by 41%.

Conclusion: The PAL+InfoNCE+OT synergy effectively improves grounding in low-resource languages, demonstrating significant gains in Bengali captioning while addressing the challenges of scarce data and semantic alignment.

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [24] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV is a compact, real-time camera-only BEV framework that distills UniAD's full-stack autonomous driving capabilities into a 28M-parameter model, achieving 78% parameter reduction while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between large-scale multi-modal perception-planning models and deployment-ready real-time autonomy by creating a resource-efficient camera-only solution that retains full-stack driving intelligence.

Method: Uses model-agnostic, multi-stage distillation combining feature-level, output-level, and adaptive region-aware supervision to transfer knowledge from UniAD to a lightweight BEV representation.

Result: Achieves 39.0 mAP for detection, 1.08 minADE for motion forecasting, 0.32 collision rate, runs 5x faster (11 FPS) than UniAD with only camera input.

Conclusion: Demonstrates that full-stack driving intelligence can be effectively retained in resource-constrained settings, enabling real-time autonomous driving with camera-only input.

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [25] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: This paper introduces a new labeling strategy for motion-blurred balls in racket sports, placing the ball at the center of the blur streak instead of the leading edge, and releases a table tennis ball detection dataset showing improved detection performance.


<details>
  <summary>Details</summary>
Motivation: Existing labeling conventions mark balls at the leading edge of motion blur, introducing asymmetry and ignoring valuable motion cues correlated with velocity, which reduces detection clarity for fast-moving objects.

Method: Proposes a new labeling strategy that places the ball at the center of the blur streak and explicitly annotates blur attributes. Introduces BlurBall, a model that jointly estimates ball position and motion blur attributes using attention mechanisms like Squeeze-and-Excitation over multi-frame inputs.

Result: The new labeling approach consistently enhances detection performance across various models. BlurBall achieves state-of-the-art results in ball detection and enables more reliable trajectory prediction.

Conclusion: Leveraging motion blur attributes not only improves ball detection accuracy but also benefits real-time sports analytics through better trajectory prediction capabilities.

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [26] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MVP is a training-free pipeline that uses compressed-domain motion vectors to propagate OWLv2 detections from keyframes to intermediate frames, reducing computational cost while maintaining strong open-vocabulary detection performance in videos.


<details>
  <summary>Details</summary>
Motivation: Running large open-vocabulary detectors on every video frame is accurate but computationally expensive. The goal is to reduce detector invocations while preserving detection quality in videos.

Method: Invokes OWLv2 only on fixed-interval keyframes and propagates detections to intermediate frames using compressed-domain motion vectors with 3x3 grid aggregation, area-growth check, and optional single-class switch. Requires no labels, fine-tuning, or training.

Result: On ILSVRC2015-VID: mAP@0.5=0.609, mAP@[0.5:0.95]=0.316. Close to framewise OWLv2-Large at loose IoU thresholds (0.747/0.721 vs 0.784/0.780 at 0.2/0.3 IoU). Outperforms tracker-based propagation methods under same keyframe schedule.

Conclusion: Compressed-domain propagation is a practical way to reduce detector invocations while maintaining strong zero-shot coverage in videos, offering label-free open-vocabulary detection that competes with supervised methods.

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [27] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: This paper investigates color robustness in HDR lighting estimation methods for AR applications, showing that simple preprocessing with a white balance network improves color accuracy without retraining existing models.


<details>
  <summary>Details</summary>
Motivation: Color robustness is a critical but often overlooked factor for achieving visual realism in AR applications using HDR lighting estimation from single images. Current evaluations typically conflate color with other lighting attributes.

Method: The study systematically evaluates several adaptation strategies using a novel HDR dataset with diverse lighting colors. Instead of proposing new algorithms, it explores whether simple preprocessing techniques can enhance color accuracy of existing lighting estimation models.

Result: Preprocessing input images with a pre-trained white balance network significantly improves color robustness, outperforming other strategies across all tested scenarios. This approach requires no retraining of the lighting estimation model.

Conclusion: The white balance preprocessing technique demonstrates strong generality, successfully improving color accuracy when applied to three state-of-the-art lighting estimation methods from recent literature.

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [28] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: A training-free framework using vision language models for zero-shot detection of check fields like signatures and MICR lines, eliminating need for large labeled datasets.


<details>
  <summary>Details</summary>
Motivation: Check fraud detection requires accurate field localization, but traditional methods depend on large labeled datasets which are scarce due to privacy and proprietary concerns.

Method: Leverages vision language model (VLM) with multimodal large language model (MLLM) for zero-shot detection of check components without training.

Result: Strong performance on 110 diverse checks, demonstrating generalization across multiple formats and layouts.

Conclusion: Framework enables deployment in financial settings and can bootstrap labeled datasets for specialized real-time detection models.

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [29] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: VLMs perform poorly on corrupted/occluded charts, showing hallucinations and overconfidence. CHART NOISe dataset introduces corruption, occlusion, and reverse inconsistency testing to benchmark and improve chart understanding robustness.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks assume clean charts and fact-based queries, but real-world charts often have distortions and require complex reasoning. Current VLMs show vulnerabilities when faced with degraded inputs.

Method: Introduce CHART NOISe dataset combining chart corruptions, occlusions, and multiple choice questions with prompt reverse inconsistency testing. Evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro on this benchmark.

Result: VLMs show sharp performance drops under corruption/occlusion, with increased hallucinations (value fabrication, trend misinterpretation, entity confusion) and overconfidence in degraded settings.

Conclusion: CHART NOISe establishes a rigorous testbed for advancing robustness in chart understanding, exposing systematic vulnerabilities and proposing mitigation strategies like quality filtering and occlusion detection.

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [30] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: A neural representation framework for 4D-MRI that replaces conventional phase binning with continuous motion modeling using two synergistic networks, achieving faster and more accurate respiratory motion capture.


<details>
  <summary>Details</summary>
Motivation: Conventional 4D-MRI reconstruction methods struggle with temporal variability, complicate workflows, and have heavy computational loads. There's a need for more efficient and accurate respiratory motion capture for radiation therapy planning.

Method: Uses two networks: Spatial Anatomy Network (SAN) for continuous 3D anatomical representation, and Temporal Motion Network (TMN) guided by Transformer-derived respiratory signals for deformation fields. This template- and phase-free approach models motion as smooth continuous deformation.

Result: Reduces processing time from ~5 hours to 15 minutes training, enables inference of each 3D volume in under 1 second. Accurately captures both regular and irregular respiratory patterns while preserving anatomical fidelity of vessels and bronchi.

Conclusion: The framework achieves superior performance over conventional methods and demonstrates strong potential for 4D radiation therapy planning and real-time adaptive treatment applications.

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [31] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: Evaluation of five Kalman filter-based tracking methods for fast-moving tiny objects like racquetballs, revealing fundamental limitations in handling unpredictable motion patterns with significant tracking drift.


<details>
  <summary>Details</summary>
Motivation: Fast-moving tiny objects like racquetballs present challenging tracking problems due to unpredictable movement patterns and small visual marks, particularly relevant for sport robotics applications where accurate tracking can improve robot perception and planning.

Method: Evaluated five state-of-the-art Kalman filter-based tracking methods (OCSORT, DeepOCSORT, ByteTrack, BoTSORT, StrongSORT) using a custom dataset of 10,000 annotated racquetball frames at 720p-1280p resolution, analyzing inference speed and update frequency effects on tracking accuracy.

Result: DeepOCSORT achieved lowest tracking error (ADE: 31.15 pixels) while ByteTrack demonstrated fastest processing (26.6ms). However, all trackers showed significant tracking drift with spatial errors ranging 3-11cm (ADE: 31-114 pixels), indicating fundamental limitations in handling unpredictable motion patterns.

Conclusion: Current Kalman filter-based tracking approaches require substantial improvements, with error rates 3-4x higher than standard benchmarks, highlighting the need for specialized methodologies for fast-moving tiny object tracking applications.

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [32] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop is a motion-aware adaptive cropping module for efficient video action recognition that uses motion vectors from compressed video to locate motion-dense regions, producing training-free, parameter-free crops that improve accuracy or reduce computational costs.


<details>
  <summary>Details</summary>
Motivation: To enable efficient video action recognition in the compressed domain by leveraging readily available motion vectors to focus computational resources on motion-dense regions, reducing processing overhead while maintaining or improving accuracy.

Method: A lightweight pipeline with denoising & merge (DM), Monte Carlo sampling (MCS), and adaptive cropping (AC) via motion-density submatrix search. The module identifies motion-dense regions using H.264 motion vectors and applies a single clip-level crop to all I-frames at inference.

Result: On UCF101, MoCrop improves accuracy or reduces compute: +3.5% Top-1 accuracy at equal FLOPs, or +2.4% Top-1 accuracy with 26.5% fewer FLOPs. Applied to CoViAR, it reaches 89.2% Top-1 accuracy at original cost and 88.5% with reduced compute from 11.6 to 8.5 GFLOPs.

Conclusion: MoCrop demonstrates strong generality across multiple backbones (ResNet-50, MobileNet-V3, EfficientNet-B1, Swin-B) and is practical for real-time deployment in compressed domain video action recognition, offering significant efficiency gains without compromising performance.

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [33] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: CAFC-SE is a codebook-based adaptive feature compression framework that uses vector quantization to map visual features to discrete indices, enabling effective low-bitrate image analysis for edge-cloud systems.


<details>
  <summary>Details</summary>
Motivation: Existing methods perform poorly under low-bitrate conditions by either retaining redundant details or learning over-concentrated symbol distributions, limiting analysis performance in edge-cloud systems.

Method: Proposes CAFC-SE framework that uses Vector Quantization to map continuous visual features to discrete indices via a codebook, selectively transmitting them to the cloud while preserving informative visual patterns.

Result: Extensive experiments demonstrate superiority in terms of rate and accuracy, showing the method is less vulnerable to low-bitrate conditions compared to existing approaches.

Conclusion: The proposed CAFC-SE framework effectively addresses low-bitrate compression challenges by preserving essential visual patterns through codebook-based feature quantization, achieving better analysis performance in edge-cloud systems.

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [34] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: MK-UNet is an ultra-lightweight CNN architecture for medical image segmentation that achieves superior performance with significantly fewer parameters and computational requirements compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To create a highly efficient medical image segmentation solution suitable for resource-limited settings like point-of-care devices, addressing the need for real-time, high-fidelity diagnostics with minimal computational footprint.

Method: Uses multi-kernel depth-wise convolution blocks (MKDC) to process images through multiple kernels and capture multi-resolution spatial relationships, combined with sophisticated attention mechanisms including channel, spatial, and grouped gated attention.

Result: Achieves higher accuracy than SOTA methods across six binary medical imaging benchmarks with only 0.316M parameters and 0.314G FLOPs, outperforming TransUNet with 333× fewer parameters and UNeXt with 4.7× fewer parameters while improving DICE scores up to 6.7%.

Conclusion: MK-UNet represents a paradigm shift in lightweight medical image segmentation, offering unparalleled performance-efficiency trade-off for real-time diagnostics in resource-constrained environments.

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [35] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat is a novel approach for deformable surgical navigation that couples intraoperative 3D reconstruction with preoperative CT data using 3D Gaussians rigged to a CT mesh.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between surgical video and volumetric patient data by enabling real-time deformation tracking during surgery.

Method: Rigs 3D Gaussians to a CT mesh and performs joint optimization of Gaussian parameters and mesh deformation through photometric supervision. Each Gaussian is parametrized relative to its parent mesh triangle to enforce alignment.

Result: Demonstrated effectiveness on visceral pig surgeries and synthetic human liver data, showing sensible deformations of preoperative CT on monocular RGB data.

Conclusion: BridgeSplat successfully enables deformable surgical navigation by coupling intraoperative 3D reconstruction with preoperative CT data, with potential for real-time surgical guidance applications.

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [36] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: Proposes DGLE framework for source-free domain adaptation in semantic segmentation, using diffusion models to propagate high-quality pseudo-labels from limited seeds instead of optimizing noisy full sets.


<details>
  <summary>Details</summary>
Motivation: Existing SFDA methods struggle with noisy pseudo-labels when optimizing entire sets simultaneously. Need better approach to generate high-quality pseudo-labels without access to source domain data.

Method: DGLE framework: 1) Fuse pseudo-labels using confidence filtering and super-resolution to get high-quality seeds; 2) Use diffusion model to propagate seeds to complete pseudo-label set while maintaining quality.

Result: Method avoids direct optimization of noisy pseudo-label sets, significantly improves pseudo-label quality, and enhances target domain performance.

Conclusion: DGLE effectively addresses SFDA challenges by starting from quality seeds and propagating via diffusion models, overcoming limitations of simultaneous optimization approaches.

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [37] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: This paper proposes using hyperbolic space embeddings for Coarse-To-Fine Few-Shot Class-Incremental Learning (C2FSCIL), achieving improved accuracy for both coarse and fine classes by leveraging hyperbolic geometry's superior hierarchical representation capabilities.


<details>
  <summary>Details</summary>
Motivation: Hyperbolic space offers better representation for hierarchical data than Euclidean space. The authors aim to enhance the "coarse-to-fine" paradigm in C2FSCIL by embedding the feature extractor in hyperbolic space to better capture hierarchical relationships between classes.

Method: The method uses the Poincaré ball model to transform images into hyperbolic feature vectors. It introduces hyperbolic contrastive loss and hyperbolic fully-connected layers for optimization and classification. Maximum entropy distribution in hyperbolic space is used to generate augmented features and mitigate overfitting in few-shot scenarios.

Result: Experiments on C2FSCIL benchmarks demonstrate that the proposed method effectively improves both coarse and fine class accuracies compared to baseline approaches.

Conclusion: Embedding feature extractors in hyperbolic space provides significant benefits for hierarchical learning tasks like C2FSCIL, with hyperbolic geometry enabling better representation of coarse-to-fine relationships and improved performance under few-shot conditions.

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [38] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: A geometry-aware two-stage framework for object removal that eliminates both target objects and their causal visual artifacts (shadows, reflections) by decoupling geometry removal and appearance rendering.


<details>
  <summary>Details</summary>
Motivation: Existing image editing methods either fail to remove causal visual artifacts not explicitly masked or lack controllability and may over-erase other objects, due to ignoring the causal relationship between object geometry and visual effects.

Method: Two-stage framework: (1) Geometry removal using strictly mask-aligned supervision to remove objects from depth/geometry with strong constraints, (2) Appearance rendering conditioned on updated geometry to implicitly handle causal visual effects. Uses preference-driven objective with positive/negative sample pairs.

Result: Achieves state-of-the-art performance in removing both objects and associated artifacts on two popular benchmarks.

Conclusion: The proposed geometry-aware approach effectively addresses limitations of appearance-based methods by leveraging the causal relationship between object geometry and visual effects, enabling more complete and controllable object removal.

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [39] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: SEGA is a transferable black-box attack method for NR-IQA models that uses Gaussian smoothing and ensemble gradients to improve transferability while maintaining imperceptibility.


<details>
  <summary>Details</summary>
Motivation: Existing white-box attacks on NR-IQA models have poor transferability to black-box scenarios where target models are inaccessible, limiting their practical utility.

Method: Proposes SEGA (Signed Ensemble Gaussian black-box Attack) which approximates target model gradients by applying Gaussian smoothing to source models and ensembling their smoothed gradients, with a perturbation filter mask to ensure imperceptibility.

Result: Experimental results on CLIVE dataset demonstrate SEGA's superior transferability compared to existing methods, enabling successful black-box attacks on NR-IQA models.

Conclusion: SEGA effectively addresses the transferability challenge in attacking NR-IQA models, providing a practical solution for black-box scenarios and valuable insights for robust system design.

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [40] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: HadaSmileNet introduces a parameter-free Hadamard multiplicative fusion framework that integrates transformer representations with physiological D-Markers for smile emotion recognition, achieving state-of-the-art results with 26% parameter reduction compared to multi-task learning approaches.


<details>
  <summary>Details</summary>
Motivation: Existing multi-task learning frameworks for smile emotion recognition suffer from computational inefficiencies due to auxiliary task supervision and complex loss balancing requirements. The paper aims to develop a more efficient and effective approach for distinguishing genuine vs posed emotions.

Method: Proposes HadaSmileNet, a feature fusion framework that directly integrates transformer-based representations with physiologically grounded D-Markers through parameter-free multiplicative interactions. Evaluated 15 fusion strategies and found Hadamard multiplicative fusion to be optimal.

Result: Achieves new state-of-the-art results across four benchmark datasets: UvA-NEMO (88.7%, +0.8), MMI (99.7%), SPOS (98.5%, +0.7), and BBC (100%, +5.0). Shows 26% parameter reduction and simplified training compared to multi-task alternatives.

Conclusion: The framework provides enhanced discriminative power through direct domain knowledge integration, making it suitable for practical deployment in real-time affective computing applications requiring computational efficiency.

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [41] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: A novel event-guided framework for joint human-scene reconstruction from monocular event cameras using 3D Gaussian Splatting, addressing motion blur in fast motion scenarios.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer microsecond temporal resolution advantages over RGB cameras for dynamic human reconstruction, especially under fast motion where RGB frames suffer from motion blur.

Method: Uses a unified set of 3D Gaussians with learnable semantic attributes; human Gaussians undergo deformation while scene Gaussians remain static. Introduces event-guided loss matching simulated brightness changes with event streams to combat blur.

Result: Achieves state-of-the-art human-scene reconstruction on ZJU-MoCap-Blur and MMHPSD-Blur datasets, with significant improvements in PSNR/SSIM and reduced LPIPS, particularly for high-speed subjects.

Conclusion: The approach eliminates need for external human masks and simplifies Gaussian set management, demonstrating superior performance for dynamic human reconstruction in blurry scenarios.

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [42] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: Live-E2T is a real-time threat monitoring framework that achieves both high performance and explainability through semantic tuple decomposition, online event deduplication, and LLM-based reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to simultaneously meet real-time performance and decision explainability requirements for threat monitoring in video streams.

Method: Three synergistic mechanisms: 1) Deconstructing video frames into Human-Object-Interaction-Place semantic tuples, 2) Online event deduplication and updating to filter redundancies, 3) Fine-tuning LLM with Chain-of-Thought for transparent reasoning.

Result: Significantly outperforms state-of-the-art methods on XD-Violence and UCF-Crime datasets in threat detection accuracy, real-time efficiency, and explainability.

Conclusion: Live-E2T successfully bridges the gap between real-time performance and explainability in threat monitoring systems through its unified framework.

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [43] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: This paper addresses the gap between general and aesthetic visual understanding in MLLMs by introducing PhotoCritique dataset, PhotoEye model with multi-view vision fusion, and PhotoBench benchmark for professional aesthetic evaluation.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with aesthetic visual understanding, focusing too much on object identification rather than aesthetic elements like color, lighting, and composition. Real-world scenarios require photographic expertise that existing models lack.

Method: Created PhotoCritique dataset from professional photographer discussions, developed PhotoEye model with language-guided multi-view vision fusion mechanism, and established PhotoBench benchmark for comprehensive aesthetic evaluation.

Result: The proposed model demonstrates clear advantages over existing models on both existing benchmarks and the new PhotoBench benchmark, showing improved aesthetic understanding capabilities.

Conclusion: The paper successfully bridges the gap between general and aesthetic visual understanding in MLLMs through specialized dataset creation, novel model architecture, and professional benchmarking, advancing the field of aesthetic visual analysis.

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [44] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: An XMem-based tumor segmentation framework for real-time MRI-guided radiotherapy that achieves reasonable performance despite lost experimental records.


<details>
  <summary>Details</summary>
Motivation: To improve precision of tumor tracking during MRI-guided radiotherapy for enhanced accuracy and safety of cancer treatments.

Method: Leverages XMem model (memory-augmented architecture) to segment tumors across long cine-MRI sequences with efficient memory mechanisms for real-time tracking.

Result: Demonstrated reasonable segmentation performance and satisfied clinical real-time requirements based on preliminary impressions (quantitative results unavailable due to lost records).

Conclusion: The XMem-based framework contributes to improving tumor tracking precision in MRI-guided radiotherapy, though complete experimental validation is currently unavailable.

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [45] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: SSCM model for multi-contrast MRI super-resolution that enhances spatial-semantic consistency through dynamic spatial warping, semantic-aware token aggregation, and spatial-frequency fusion


<details>
  <summary>Details</summary>
Motivation: Existing methods insufficiently model spatial-semantic consistency and underuse frequency-domain information, leading to poor alignment and inadequate high-frequency detail recovery in MRI super-resolution

Method: Proposes Spatial-Semantic Consistent Model (SSCM) with three components: Dynamic Spatial Warping Module for inter-contrast alignment, Semantic-Aware Token Aggregation Block for long-range consistency, and Spatial-Frequency Fusion Block for fine structure restoration

Result: Achieves state-of-the-art performance on public and private datasets with fewer parameters while ensuring spatially and semantically consistent reconstructions

Conclusion: SSCM effectively addresses spatial-semantic consistency challenges in multi-contrast MRI super-resolution, improving anatomical detail preservation and reconstruction quality

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [46] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: OraPO is a compact framework for radiology report generation that uses RL-only training with oracle-educated preference optimization and FactScore-based rewards, achieving SOTA performance with significantly reduced data and computational requirements.


<details>
  <summary>Details</summary>
Motivation: Current radiology report generation methods require large datasets and oversized models, making them data- and compute-intensive. The authors aim to create an efficient solution that works under constrained budgets.

Method: Proposes Oracle-educated GRPO (OraPO) with FactScore-based reward (FactS). OraPO enables single-stage RL-only training by converting failed explorations into preference supervision via a lightweight oracle. FactS extracts atomic clinical facts and checks entailment against ground truth for dense, interpretable sentence-level rewards.

Result: Achieves state-of-the-art performance on CheXpert Plus dataset (0.341 F1) with 2-3 orders of magnitude less training data using a small base VLM on modest hardware.

Conclusion: The compact OraPO framework significantly improves learning efficiency on clinically challenging cases while maintaining high performance, making radiology report generation more accessible under resource constraints.

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [47] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: AMSF is a training-free framework that enables controllable fusion of multiple reference styles in diffusion models through adaptive semantic token decomposition and similarity-aware re-weighting.


<details>
  <summary>Details</summary>
Motivation: Existing reference-based methods are limited to single style images and lack principled mechanisms to balance multiple stylistic influences, preventing hybrid aesthetics and scalability.

Method: Encodes all style images and textual hints with semantic token decomposition module adaptively injected into cross-attention layers, plus similarity-aware re-weighting module that recalibrates attention allocation at each denoising step.

Result: Qualitative and quantitative evaluations show AMSF consistently outperforms state-of-the-art approaches and scales seamlessly to two or more styles without fine-tuning.

Conclusion: AMSF represents a practical step toward expressive multi-style generation in diffusion models with balanced and user-controllable blends.

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [48] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: MLF-4DRCNet is a novel two-stage framework for 3D object detection using multi-level fusion of 4D radar and camera data, addressing limitations of existing fusion methods by incorporating point-, scene-, and proposal-level information.


<details>
  <summary>Details</summary>
Motivation: Existing 4D radar-camera fusion methods adopt BEV fusion paradigms designed for LiDAR-camera fusion, neglecting radar's sparse and noisy point clouds and restricting fusion to coarse scene-level integration, which limits standalone 4D radar application in 3D object detection.

Method: Proposes MLF-4DRCNet with three key modules: Enhanced Radar Point Encoder (ERPE) for point-level fusion using Triple-Attention Voxel Feature Encoder, Hierarchical Scene Fusion Pooling (HSFP) for scene-level fusion with deformable attention, and Proposal-Level Fusion Enhancement (PLFE) for refining region proposals by fusing image features.

Result: Achieves state-of-the-art performance on View-of-Delft (VoD) and TJ4DRadSet datasets, with performance comparable to LiDAR-based models on VoD dataset.

Conclusion: The multi-level fusion approach effectively addresses radar's inherent drawbacks and enables comprehensive feature representation, demonstrating that 4D radar-camera fusion can achieve performance comparable to LiDAR-based methods.

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [49] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: PDLS is a training-free framework that uses dual latent steering to improve image inversion in diffusion models, balancing structural fidelity and semantic accuracy without per-image optimization.


<details>
  <summary>Details</summary>
Motivation: Current single-latent vector methods for image inversion struggle to balance structural fidelity with semantic accuracy, leading to semantic drift issues like blurred details or incorrect attributes.

Method: PDLS decomposes inversion into structural and semantic paths, formulates dual guidance as an optimal control problem, and uses Linear Quadratic Regulator (LQR) to dynamically steer the generative trajectory at each step.

Result: Extensive experiments on FFHQ-1K and ImageNet-1K show PDLS produces more faithful reconstructions than single-latent baselines across various inversion tasks including deblurring, super-resolution, and inpainting.

Conclusion: PDLS effectively prevents semantic drift while preserving fine details, demonstrating superior performance in image inversion tasks compared to existing methods.

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [50] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: Prima is a vision language model for neuroimaging that uses hierarchical vision architecture trained on 220,000 MRI studies, achieving 92.0 mean diagnostic AUC across 52 neurological disorders and outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Address the growing demand for MRI studies that strains health systems, prolongs turnaround times, and disproportionately impacts low-resource and rural settings.

Method: Developed Prima using a large academic health system as data engine, trained on over 220,000 MRI studies with hierarchical vision architecture for general and transferable MRI features.

Result: In a 1-year health system-wide study with 30K MRI studies, Prima achieved mean diagnostic AUC of 92.0 across 52 radiologic diagnoses, outperforming other AI models. It demonstrated algorithmic fairness across sensitive groups.

Conclusion: Prima demonstrates transformative potential for health system-scale VLMs in advancing AI-driven healthcare by providing explainable diagnoses, worklist prioritization, and mitigating health system biases.

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [51] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: UiG is a novel reasoning framework that integrates understanding capabilities into the generation process for text-to-image models, using image editing as a bridge to enhance generation quality.


<details>
  <summary>Details</summary>
Motivation: Current Chain-of-Thought methods separate understanding and generation processes, limiting their ability to guide unified models in addressing generative deficiencies. There's a need to leverage strong understanding capabilities to reinforce image generation performance.

Method: UiG uses 'Image Editing' as a bridge to infuse understanding into generation. It verifies generated images, incorporates model understanding into editing instructions, and enhances images step by step by gradually infusing understanding into the generation process.

Result: UiG demonstrates significant performance improvement in text-to-image generation, achieving a 3.92% gain on the long prompt setting of the TIIF benchmark compared to existing text-to-image reasoning methods.

Conclusion: The Understanding-in-Generation framework successfully integrates understanding capabilities during the reasoning process, effectively mitigating limitations of generative abilities in unified models for text-to-image generation.

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [52] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: This paper presents a benchmark for depth estimation models on endoscopic images and introduces EndoSynth, a synthetic dataset that improves model accuracy when used for fine-tuning.


<details>
  <summary>Details</summary>
Motivation: There is a lack of robust benchmarks and high-quality datasets for depth estimation in endoscopic images, despite advancements in foundation models.

Method: The authors create a comprehensive benchmark of state-of-the-art depth estimation models evaluated on real endoscopic images and introduce EndoSynth, a synthetic dataset with ground truth metric depth and segmentation masks.

Result: Fine-tuning depth foundation models using the synthetic EndoSynth dataset significantly boosts accuracy on most unseen real endoscopic data.

Conclusion: This work advances depth estimation for endoscopic images by providing both a benchmark and synthetic dataset, serving as an important resource for future research.

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [53] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: LEAF-Mamba: A state space model for RGB-D salient object detection that addresses local semantics deficiency and cross-modality fusion issues through local emphatic SSM and adaptive fusion modules, achieving superior performance with linear complexity.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-D SOD methods using CNNs have limited receptive fields while Vision Transformers suffer from quadratic complexity. Direct application of state space models (Mamba) to RGB-D SOD leads to deficient local semantics and inadequate cross-modality fusion.

Method: Proposes LEAF-Mamba with two key components: 1) Local Emphatic State Space Module (LE-SSM) to capture multi-scale local dependencies for both RGB and depth modalities, 2) SSM-based Adaptive Fusion Module (AFM) for complementary cross-modality interaction and reliable integration.

Result: Extensive experiments show LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in both efficacy and efficiency. Also achieves excellent performance on RGB-T SOD task, demonstrating strong generalization ability.

Conclusion: The proposed LEAF-Mamba effectively balances performance and computational efficiency for RGB-D SOD by addressing local semantics and cross-modality fusion challenges through novel SSM-based modules.

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [54] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: A lightweight food image classification algorithm combining Window Multi-Head Attention Mechanism and Spatial Attention Mechanism to reduce computational complexity while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of large parameters and high computational complexity in Vision Transformer models for food image classification, which is crucial for automated quality control and intelligent agricultural production.

Method: Proposed algorithm integrates Window Multi-Head Attention Mechanism (WMHAM) to capture local/global features efficiently through window partitioning, and Spatial Attention Mechanism (SAM) to emphasize key spatial regions for better feature representation.

Result: Achieved 95.24% accuracy on Food-101 and 94.33% on Vireo Food-172 datasets, with significant reduction in parameters and FLOPs compared to baseline methods.

Conclusion: The approach effectively balances computational efficiency and classification performance, making it suitable for deployment in resource-constrained environments.

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [55] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: OSDA is a three-stage framework for open-set land-cover analysis in remote sensing that enables annotation-free discovery, segmentation, and description of novel objects using SAM and MLLM models.


<details>
  <summary>Details</summary>
Motivation: To address the need for fine-grained spatial localization and semantically open categorization in remote sensing, enabling detection and segmentation of novel objects without categorical supervision and assigning interpretable semantic labels through multimodal reasoning.

Method: A three-stage pipeline: (1) precise discovery and mask extraction with fine-tuned SAM, (2) semantic attribution and contextual description via fine-tuned MLLM, (3) LLM-as-judge and manual scoring for evaluation. The framework is architecture-agnostic and label-free.

Result: The framework provides a scalable and interpretable solution for dynamic land-cover monitoring, showing strong potential for automated cartographic updating and large-scale earth observation analysis.

Conclusion: OSDA successfully combines pixel-level accuracy with high-level semantic understanding to address key challenges in open-world remote sensing interpretation, supporting robust evaluation across diverse satellite imagery without requiring manual annotation.

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [56] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: The paper presents PlantCLEF 2021, a plant identification challenge focused on improving automated identification in biodiversity-rich but data-poor tropical regions by leveraging herbarium collections through cross-domain classification.


<details>
  <summary>Details</summary>
Motivation: Current automated plant identification systems are biased toward North America and Europe, while biodiversity-rich tropical regions lack sufficient field photo data. Herbarium collections contain centuries of botanical data from these regions that could bridge this gap.

Method: Cross-domain classification using a dataset of ~1,000 species from the Guiana Shield region, with training on hundreds of thousands of herbarium sheets and a few thousand field photos, plus morphological/functional trait data. Test set consisted exclusively of field photos.

Result: The challenge assessed how herbarium collections can improve automated plant identification in data-poor regions through cross-domain learning approaches.

Conclusion: Herbarium collections represent a valuable resource for enhancing automated plant identification systems in biodiversity-rich but data-scarce tropical regions, addressing current geographical biases in training data.

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [57] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: AGSwap is a novel method for cross-category object fusion in text-to-image generation that addresses visual chaos and semantic inconsistency through adaptive group swapping and updating mechanisms, supported by a new large-scale benchmark dataset COF.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image generation methods struggle with fusing cross-category objects, producing biased, visually chaotic, or semantically inconsistent results due to overlapping artifacts and poor integration. The field also lacks comprehensive benchmark datasets.

Method: AGSwap consists of two key components: (1) Group-wise Embedding Swapping that fuses semantic attributes through feature manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism guided by balance evaluation scores. The method is supported by the Cross-category Object Fusion (COF) dataset built on ImageNet-1K and WordNet.

Result: Extensive experiments show that AGSwap outperforms state-of-the-art compositional T2I methods, including GPT-Image-1, on both simple and complex prompts.

Conclusion: AGSwap provides an effective solution for coherent cross-category object fusion in text-to-image generation, with the COF dataset serving as a valuable benchmark for future research in this domain.

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [58] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: The LifeCLEF 2019 Plant Identification challenge focused on automated plant identification for data-deficient regions, specifically the Guiana shield and Northern Amazon rainforest, comparing system performance with expert botanists.


<details>
  <summary>Details</summary>
Motivation: Current automated plant identification systems are limited to well-documented species (tens of thousands), while Earth has nearly 369K plant species. The challenge aimed to address the identification gap for biodiverse but data-deficient tropical regions.

Method: The challenge used a dataset of 10,000 species from the Guiana shield and Northern Amazon rainforest. Participants developed automated identification systems, and their performance was compared against expert tropical flora botanists.

Result: The paper presents the evaluation results comparing various automated identification systems with human expert performance on tropical plant species identification.

Conclusion: The challenge provided valuable insights into the current capabilities and limitations of automated plant identification systems for data-deficient biodiverse regions, highlighting areas for future improvement.

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [59] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: RSVG-ZeroOV is a training-free framework for zero-shot open-vocabulary remote sensing visual grounding that uses frozen foundation models without fine-tuning, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing RSVG methods are limited to closed-set vocabularies and require expensive datasets and fine-tuning. The goal is to enable open-world applicability without training costs.

Method: Three-stage framework: (1) Overview with VLM cross-attention maps for semantic correlation, (2) Focus using diffusion model priors for structural information, (3) Evolve with attention evolution module to purify segmentation masks.

Result: Extensive experiments show RSVG-ZeroOV consistently outperforms existing weakly-supervised and zero-shot methods without task-specific training.

Conclusion: The proposed framework offers an efficient, scalable solution for open-vocabulary RSVG by effectively leveraging frozen foundation models in a zero-shot setting.

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [60] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: The paper proposes an Attribute Prompt Composition (APC) framework for Object Re-Identification that uses textual semantics to enhance both discrimination and generalization across domains.


<details>
  <summary>Details</summary>
Motivation: Existing ReID models are limited to either single-domain (overfitting to domain-specific features) or cross-domain scenarios (suppressing identity-specific cues through normalization strategies), which restricts real-world applicability.

Method: The framework includes an Attribute Prompt Generator with Semantic Attribute Dictionary and Prompt Composition Module, plus a Fast-Slow Training Strategy that balances ReID-specific discrimination with generalizable representation learning from Vision-Language Models.

Result: Extensive experiments on conventional and Domain Generalized ReID datasets show the framework surpasses state-of-the-art methods in both discrimination and generalization performance.

Conclusion: The APC framework effectively addresses limitations of existing ReID models by leveraging textual semantics and balanced training strategies, achieving superior performance in both discrimination and generalization across domains.

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [61] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: OTCCLIP is an Optimal Transport-based framework that defends against data poisoning attacks on CLIP models by reconstructing image-caption pairs using fine-grained feature alignment.


<details>
  <summary>Details</summary>
Motivation: Previous defense methods for CLIP poisoning attacks rely solely on global representations, overlooking fine-grained features which can introduce incorrect image-caption pairs and harm pre-training performance.

Method: Proposes an optimal transport-based distance measure between fine-grained visual and textual feature sets, reassigns captions based on this distance, and employs optimal transport-based objective functions for inter- and intra-modality fine-grained alignment.

Result: OTCCLIP successfully decreases attack success rates of poisoning attacks and significantly improves CLIP's zero-shot and linear probing performance compared to previous methods when trained on poisoned datasets.

Conclusion: The optimal transport-based approach effectively addresses limitations of previous methods by leveraging fine-grained feature alignment, providing robust defense against poisoning attacks while maintaining CLIP's performance.

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [62] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: LFI is a cognitive-inspired framework that enables better knowledge transfer from vision language models to visual foundation models by modeling visual understanding as an interactive process rather than just using result-oriented approaches.


<details>
  <summary>Details</summary>
Motivation: Current visual foundation models struggle to effectively transfer knowledge from vision language models because they focus on final results rather than the underlying interaction processes that VLMs excel at modeling through cross-modal representations.

Method: The approach uses Interaction Queries to maintain persistent relational structures across network layers and interaction-based supervision derived from cross-modal attention mechanisms of pre-trained VLMs.

Result: Achieved significant improvements: 3.3 and 1.6 mAP/2.4 AP gains on TinyImageNet classification and COCO detection/segmentation, with 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Also showed 2.7x better semantic consistency in human evaluations.

Conclusion: LFI framework enables more faithful and efficient knowledge transfer from VLMs to VFMs by explicitly modeling interaction processes, leading to better generalization across diverse vision tasks with minimal parameter overhead and faster convergence.

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [63] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: HyPSAM is a novel hybrid prompt-driven segment anything model for RGB-thermal salient object detection that leverages SAM's zero-shot capabilities through dynamic fusion and refinement networks.


<details>
  <summary>Details</summary>
Motivation: RGB-T SOD faces challenges in learning precise boundaries and complete objects due to insufficient feature fusion and data scarcity. The paper aims to overcome limitations of fixed-parameter kernels and enhance multi-modal feature representation.

Method: Proposes a dynamic fusion network (DFNet) with dynamic convolution and multi-branch decoding for adaptive cross-modality interaction, and a plug-and-play refinement network (P2RNet) that uses hybrid prompts (text, mask, box) to guide SAM in refining saliency maps.

Result: Extensive experiments on three public datasets demonstrate state-of-the-art performance. HyPSAM shows remarkable versatility by integrating with different RGB-T SOD methods to achieve significant performance gains.

Conclusion: The method highlights the potential of prompt engineering in RGB-T SOD and provides a general optimization strategy that can be applied to various existing methods.

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [64] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: TriFusion-AE: A multimodal cross-attention autoencoder that integrates text, depth maps, and LiDAR point clouds to improve robustness against noise and adversarial attacks in autonomous driving perception.


<details>
  <summary>Details</summary>
Motivation: Raw LiDAR point clouds are vulnerable to noise, occlusion, and adversarial corruptions, and existing autoencoders degrade under challenging real-world conditions.

Method: Uses multimodal cross-attention to align semantic cues from text, geometric features from multi-view depth maps, and spatial structure from LiDAR point clouds for joint representation learning.

Result: Achieves significantly more robust reconstruction under strong adversarial attacks and heavy noise where CNN-based autoencoders collapse, with limited gains under mild perturbations.

Conclusion: The multimodal fusion framework is model-agnostic and enables seamless integration with any CNN-based point cloud autoencoder for improved robustness in low-data deployment scenarios.

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [65] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: COLT enhances open-source video LLMs with continual tool usage capabilities, enabling automatic acquisition of tool-use ability in evolving tool streams without catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing video LLM methods assume fixed tool repositories and struggle with real-world environments where tool data is perpetually evolving and streaming in.

Method: COLT incorporates a learnable tool codebook as tool-specific memory, dynamically selecting relevant tools based on similarity between user instruction and tool features. Uses VideoToolBench dataset for instruction tuning.

Result: Extensive experiments on video LLM benchmarks and VideoToolBench demonstrate state-of-the-art performance.

Conclusion: COLT successfully enables video LLMs to handle continually evolving tool streams while maintaining performance on previously learned tools.

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [66] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS is a training-free method that enhances sparse-view 3D Gaussian Splatting reconstruction by using diffusion models to remove artifacts and complete missing content while maintaining multi-view consistency.


<details>
  <summary>Details</summary>
Motivation: Current methods for sparse-view 3DGS reconstruction struggle with multi-view consistency, leading to blurred structures and implausible details when using generative priors to address insufficient visual information.

Method: Proposes FixingGS with a distillation approach that delivers accurate and cross-view coherent diffusion priors, combined with an adaptive progressive enhancement scheme to refine reconstructions in under-constrained regions.

Result: Extensive experiments show FixingGS surpasses state-of-the-art methods with superior visual quality and reconstruction performance.

Conclusion: FixingGS effectively addresses the limitations of sparse-view 3DGS reconstruction by providing training-free enhancement through improved diffusion priors and adaptive refinement.

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


### [67] [Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models](https://arxiv.org/abs/2509.18763)
*Xijun Wang,Junyun Huang,Rayyan Abdalla,Chengyuan Zhang,Ruiqi Xian,Dinesh Manocha*

Main category: cs.CV

TL;DR: Bi-VLM addresses the computational demands of vision-language models by proposing ultra-low-bit weight quantization (≤2 bits) using Gaussian quantiles to separate weights into outlier and inlier subsets, achieving significant efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: The substantial computational cost and memory requirements of VLMs restrict their applicability in hardware-constrained environments, creating a critical gap between model demands and achievable weight precision.

Method: Proposes Bi-VLM with non-uniform weight separation based on Gaussian quantiles, grouping weights into outlier (salient) and multiple inlier (unsalient) subsets. Uses saliency-aware hybrid quantization algorithm with different constraints on scaler and binary matrices based on saliency metrics.

Result: For language model part: outperforms SOTA by 3%-47% on visual question answering across 4 benchmarks and 3 models. For overall VLM: outperforms SOTA by 4%-45%. Token pruning reveals 90%-99% redundancy in image tokens, enabling further efficiency improvements.

Conclusion: Bi-VLM successfully bridges the gap between VLM computational demands and ultra-low-bit precision, demonstrating significant performance improvements while enabling additional efficiency gains through token pruning.

Abstract: We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.

</details>


### [68] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: DiSSECT introduces a discrete self-supervision framework using multi-scale vector quantization to improve medical image representation learning by suppressing shortcut learning and enhancing transferability.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods for medical imaging rely on complex architectures, anatomy-specific priors, or heavily tuned augmentations, making them prone to shortcut learning and limiting scalability, especially in modalities like chest X-rays where anatomical similarity is high and pathology is subtle.

Method: DiSSECT integrates multi-scale vector quantization into the SSL pipeline to impose a discrete representational bottleneck, which constrains the model to learn repeatable, structure-aware features while suppressing view-specific or low-utility patterns.

Result: DiSSECT achieves strong performance on both classification and segmentation tasks with minimal or no fine-tuning, showing high label efficiency in low-label regimes across multiple public medical imaging datasets.

Conclusion: The framework demonstrates robustness and generalizability compared to existing state-of-the-art approaches, providing efficient clinical transferable representations for medical image analysis.

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [69] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: Proposes structured reflection for tool-augmented LLMs to explicitly diagnose errors and generate corrective actions, improving multi-turn tool interaction reliability.


<details>
  <summary>Details</summary>
Motivation: Current self-reflection methods are fragile in multi-turn interactions, often leading to repeated mistakes after failures. Existing approaches rely on heuristic prompts or one-way reasoning without proper error diagnosis and repair learning.

Method: Introduces structured reflection as an explicit, controllable action. Combines DAPO and GSPO objectives with a tailored reward scheme for tool use, optimizing the Reflect-Call-Final strategy. Uses Tool-Reflection-Bench for evaluation with programmatic checks.

Result: Experiments on BFCL v3 and Tool-Reflection-Bench show significant improvements in multi-turn tool-call success, error recovery, and reduced redundant calls.

Conclusion: Making reflection explicit and directly optimizing it enhances tool interaction reliability and provides a reproducible learning path from failures.

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [70] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: A real-time deer detection system using thermal imaging, deep learning, and V2X communication achieves 98.84% mAP and under 100ms latency to prevent deer-vehicle collisions.


<details>
  <summary>Details</summary>
Motivation: Deer-vehicle collisions cause 2.1M incidents annually with 440 fatalities, 59K injuries, and $10B in damages, plus contribute to declining deer populations.

Method: Integration of thermal imaging, deep learning on 12,000 thermal deer images, and CV2X communication for real-time detection and driver warnings.

Result: 98.84% mAP, 95.44% precision, 95.96% recall; maintains 88-92% accuracy in challenging conditions vs <60% for visible light cameras; <100ms end-to-end latency.

Conclusion: Establishes viable technological pathway for reducing deer-vehicle collisions through thermal imaging and connected vehicles.

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [71] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: VIR-Bench is a novel benchmark for evaluating multimodal large language models' geospatial-temporal intelligence using 200 travel videos, addressing the gap in long-distance travel understanding that current benchmarks overlook.


<details>
  <summary>Details</summary>
Motivation: Current video benchmarks focus on indoor scenes or short-range outdoor activities, leaving long-distance travel challenges unexplored. Mastering extended geospatial-temporal trajectories is critical for real-world applications like embodied-AI planning and navigation.

Method: The authors present VIR-Bench, consisting of 200 travel videos that frame itinerary reconstruction as a challenging task. They evaluate state-of-the-art MLLMs and conduct an in-depth case study developing a prototype travel-planning agent.

Result: Experimental results show that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores on VIR-Bench, highlighting the difficulty of handling videos spanning extended spatial and temporal scales. The travel-planning agent shows markedly improved itinerary recommendations.

Conclusion: VIR-Bench effectively benchmarks MLLMs' geospatial-temporal capabilities and translates into concrete performance gains in user-facing applications, demonstrating the benchmark's practical utility beyond mere evaluation.

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [72] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: SAADi is a framework that aligns diffusion models with downstream surgical vision tasks by fine-tuning on preferred vs non-preferred synthetic image pairs, overcoming data memorization issues and improving classification and segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity in surgical deep learning by overcoming diffusion model memorization problems that lead to inconsistent samples and poor downstream performance.

Method: Constructs preferred/non-preferred synthetic image pairs and performs lightweight fine-tuning of diffusion models to align generation with downstream objectives.

Result: Consistent gains of 7-9% in classification and 2-10% in segmentation across three surgical datasets, with iterative refinement boosting performance by 4-10%.

Conclusion: Task-aware alignment is key for mitigating data scarcity and advancing surgical vision applications, overcoming sample degradation issues in baseline approaches.

Abstract: The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.

</details>


### [73] [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
*Zijian Ling,Han Zhang,Yazhuo Zhou,Jiahao Cui*

Main category: cs.CV

TL;DR: ColorBlindnessEval is a benchmark for evaluating Vision-Language Models' robustness using Ishihara-like color blindness test images to test numerical recognition in visually adversarial scenarios.


<details>
  <summary>Details</summary>
Motivation: To assess VLMs' ability to handle complex visual patterns and adversarial contexts, particularly numerical recognition in color-based visual challenges similar to human color blindness tests.

Method: Created a dataset of 500 Ishihara-like images with numbers 0-99 using varying color combinations. Evaluated 9 VLMs using Yes/No and open-ended prompts, comparing performance with human participants.

Result: VLMs showed limitations in interpreting numbers in adversarial contexts, with prevalent hallucination issues. Performance was inferior to human participants in these visually challenging scenarios.

Conclusion: Current VLMs need improved robustness for complex visual environments. ColorBlindnessEval provides a valuable benchmarking tool for enhancing VLM reliability in real-world applications requiring high accuracy.

Abstract: This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.

</details>


### [74] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: Proposes KMDS-Net, a model-based neural network for dynamic PET image denoising that combines kernel space-based multidimensional sparse modeling with neural networks for adaptive parameter optimization.


<details>
  <summary>Details</summary>
Motivation: Dynamic PET imaging faces challenges in achieving high image quality due to limited statistics in short temporal frames, requiring effective denoising methods to improve temporal and spatial resolution.

Method: Develops a kernel space-based multidimensional sparse (KMDS) model using inter-frame spatial correlation and intra-frame structural consistency, then replaces parameter estimation with neural networks to create an end-to-end KMDS-Net for adaptive optimization.

Result: Extensive experiments on simulated and real data show KMDS-Net exhibits strong denoising performance for dynamic PET, outperforming previous baseline methods.

Conclusion: The proposed neural KMDS-Net can effectively achieve high temporal and spatial resolution for dynamic PET imaging through adaptive denoising.

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [75] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: Citrus-V is a multimodal medical foundation model that integrates detection, segmentation, and chain-of-thought reasoning for comprehensive clinical imaging analysis in a single framework.


<details>
  <summary>Details</summary>
Motivation: Existing medical imaging models are narrowly focused and require multiple specialized networks, limiting generalization. Clinical applications demand precise visual grounding, multimodal integration, and chain-of-thought reasoning.

Method: Proposes a novel multimodal training approach combining image analysis with textual reasoning, integrating detection, segmentation, and multimodal chain-of-thought reasoning. Releases a curated open-source data suite covering reasoning, detection, segmentation, and document understanding tasks.

Result: Citrus-V outperforms existing open-source medical models and expert-level imaging systems across multiple benchmarks, delivering unified pipeline from visual grounding to clinical reasoning.

Conclusion: The model enables pixel-level lesion localization, structured report generation, and physician-like diagnostic inference, supporting precise lesion quantification, automated reporting, and reliable second opinions.

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [76] [Surgical Video Understanding with Label Interpolation](https://arxiv.org/abs/2509.18802)
*Garam Kim,Tae Kyeong Jeong,Juyoun Park*

Main category: cs.CV

TL;DR: A novel framework combining optical flow-based segmentation label interpolation with multi-task learning to address temporal-spatial imbalance in surgical scene understanding for robot-assisted surgery.


<details>
  <summary>Details</summary>
Motivation: Real surgical scenes involve complex temporal dynamics and diverse instrument interactions that limit comprehensive understanding. Current approaches suffer from sparse spatial supervision due to high annotation costs, creating temporal-spatial imbalance where long-term annotations are available for every frame but short-term annotations are only provided for key frames.

Method: Proposes optical flow-based segmentation label interpolation where optical flow estimated from annotated key frames is used to propagate labels to adjacent unlabeled frames, combined with multi-task learning to enrich sparse spatial supervision and balance temporal and spatial information.

Result: The integration improves both the accuracy and efficiency of surgical scene understanding, enhancing the utility of robot-assisted surgery.

Conclusion: The proposed framework effectively addresses the temporal-spatial imbalance in surgical data annotation, leading to better surgical scene understanding and improved robot-assisted surgery performance.

Abstract: Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.

</details>


### [77] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: Hyper-Bagel is a unified acceleration framework that speeds up multimodal understanding and generation tasks using speculative decoding and multi-stage distillation, achieving 2x+ speedup in understanding and 16.67x-22x speedup in generation while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Address the computational overhead of iterative diffusion denoising and autoregressive decoding in unified multimodal models as contexts integrate increasingly numerous interleaved multimodal tokens.

Method: Uses divide-and-conquer strategy with speculative decoding for next-token prediction and multi-stage distillation for diffusion denoising. Develops both lossless 6-NFE model and highly efficient 1-NFE model using adversarial distillation with human feedback learning.

Result: Achieves over 2x speedup in multimodal understanding, 16.67x speedup in text-to-image generation, and 22x speedup in image editing while preserving original model quality. The 1-NFE model enables near real-time interactive editing and generation.

Conclusion: Hyper-Bagel provides significant computational efficiency improvements for multimodal AI tasks, making complex multimodal interactions seamless and instantaneous through advanced acceleration techniques.

Abstract: Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.

</details>


### [78] [Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography](https://arxiv.org/abs/2509.18839)
*Gianmarco Spinaci,Lukas Klic,Giovanni Colavizza*

Main category: cs.CV

TL;DR: This study evaluates multimodal LLMs and VLMs for Christian iconography classification, finding that Gemini-2.5 Pro and GPT-4o outperform ResNet50 baselines, with performance varying based on dataset characteristics and prompt enrichment strategies.


<details>
  <summary>Details</summary>
Motivation: To assess whether general-purpose multimodal models can interpret Christian iconography typically handled by supervised classifiers, and evaluate their potential as metadata curation tools in digital humanities workflows.

Method: Benchmarking study using three datasets (ArtDL, ICONCLASS, Wikidata) filtered to top 10 classes. Models tested under three conditions: class labels only, Iconclass descriptions, and few-shot learning with 5 exemplars. Compared against ResNet50 baselines.

Result: Gemini-2.5 Pro and GPT-4o outperformed ResNet50 baselines. Accuracy dropped significantly on Wikidata dataset where Siglip performed best. Class descriptions improved zero-shot performance, while few-shot learning generally produced lower results with minimal accuracy increments.

Conclusion: General-purpose multimodal LLMs are capable of classification in complex cultural heritage domains, supporting their application as metadata curation tools in digital humanities. Future research should focus on prompt optimization and expanding to other classification strategies.

Abstract: This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.

</details>


### [79] [ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction](https://arxiv.org/abs/2509.18840)
*Ismael Elsharkawi,Hossam Sharara,Ahmed Rafea*

Main category: cs.CV

TL;DR: The paper proposes LRGC, a learnable reparameterized graph construction method for Vision Graph Neural Networks that uses key-query attention and soft-threshold reparameterization for differentiable edge selection, outperforming existing ViG models on ImageNet-1k.


<details>
  <summary>Details</summary>
Motivation: Traditional ViG models rely on non-learnable statistical methods for graph construction that may not select optimal neighborhoods and require hyper-parameter tuning. There's a need for a learnable, hyper-parameter-free graph construction approach.

Method: LRGC applies key-query attention between all node pairs and uses soft-threshold reparameterization for edge selection, making the graph construction differentiable and learnable through training without requiring hyper-parameters.

Result: The proposed ViG-LRGC approach outperforms state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark dataset.

Conclusion: LRGC provides an effective learnable graph construction method for ViG that removes bias from traditional clustering/thresholding methods and allows adaptive threshold learning during training.

Abstract: Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.

</details>


### [80] [Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model](https://arxiv.org/abs/2509.18891)
*Xueyu Liu,Xiaoyi Zhang,Guangze Shi,Meilin Liu,Yexin Lai,Yongfei Wu,Mingqiang Wei*

Main category: cs.CV

TL;DR: Point Prompt Defender is an adversarial reinforcement learning framework that automatically optimizes point prompts for SAM using an attack-for-defense paradigm to improve segmentation robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing approaches rely on heuristic or manually crafted prompts, limiting scalability and generalization of SAM's performance.

Method: Uses dual-space graph representation of image patches, with attacker and defender agents trained using Deep Q-Networks. The attacker learns to degrade SAM's performance while the defender learns to suppress disruptive prompts.

Result: Extensive experiments show the framework effectively improves SAM's robustness and generalization across diverse tasks without retraining.

Conclusion: Establishes a flexible, interpretable, and plug-and-play framework for prompt-based segmentation that enhances SAM performance.

Abstract: Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.

</details>


### [81] [SmartWilds: Multimodal Wildlife Monitoring Dataset](https://arxiv.org/abs/2509.18894)
*Jenna Kline,Anirudh Potlapally,Bharath Pillai,Tanishka Wani,Rugved Katole,Vedant Patil,Penelope Covey,Hari Subramoni,Tanya Berger-Wolf,Christopher Stewart*

Main category: cs.CV

TL;DR: SmartWilds is the first multimodal wildlife monitoring dataset combining synchronized drone imagery, camera trap photos/videos, and bioacoustic recordings from a 220-acre safari park, supporting AI research for conservation.


<details>
  <summary>Details</summary>
Motivation: To address critical needs in endangered species research, conservation ecology, and habitat management by providing comprehensive multimodal environmental monitoring data.

Method: Four-day synchronized data collection across three modalities (drone imagery, camera traps, bioacoustic recordings) in a 220-acre pasture containing various species including endangered animals and native Ohio wildlife.

Result: Comparative analysis shows complementary strengths of different sensor modalities for landuse patterns, species detection, behavioral analysis, and habitat monitoring. Reproducible protocols for multimodal wildlife monitoring were established.

Conclusion: SmartWilds provides open datasets to advance conservation computer vision research, with future releases planned to include GPS tracking data, citizen science contributions, and expanded seasonal coverage.

Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.

</details>


### [82] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: RS3DBench is a novel benchmark dataset for 3D understanding of remote sensing images, containing 54,951 image-depth map pairs with text descriptions, plus a state-of-the-art depth estimation model based on stable diffusion.


<details>
  <summary>Details</summary>
Motivation: Existing remote sensing datasets lack comprehensive depth information or precise alignment between depth data and images, limiting the development of 3D vision models for remote sensing applications.

Method: Created RS3DBench dataset with pixel-level aligned depth maps and textual descriptions covering diverse geographical contexts. Also developed a remotely sensed depth estimation model using stable diffusion's multimodal fusion capabilities.

Result: The dataset provides a comprehensive tool for training and evaluating 3D visual perception models. The proposed depth estimation model achieves state-of-the-art performance on the new benchmark.

Conclusion: This work significantly contributes to advancing 3D visual perception models and geographic AI in remote sensing, with all resources made publicly available.

Abstract: In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.

</details>


### [83] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: DeblurSplat: A novel method that combines event cameras with 3D Gaussian Splatting for motion deblurring without requiring Structure-from-Motion, achieving high-fidelity novel view synthesis with superior efficiency.


<details>
  <summary>Details</summary>
Motivation: To address motion blur in 3D scene reconstruction by eliminating the need for SfM, which introduces cumulative errors from inaccurate camera poses, and leveraging event cameras' high temporal resolution for better dynamic scene capture.

Method: 1) Uses DUSt3R's dense stereo module to obtain initial point clouds directly from blurred images, bypassing SfM and camera pose estimation. 2) Integrates event streams to decode latent sharp images, providing fine-grained supervision for scene optimization during 3D Gaussian Splatting reconstruction.

Result: Extensive experiments show DeblurSplat generates high-fidelity novel views and achieves significant rendering efficiency improvements over state-of-the-art deblur 3D-GS methods across various scenes.

Conclusion: The proposed SfM-free approach successfully addresses motion blur in 3D reconstruction by combining event camera data with direct point cloud initialization, demonstrating superior performance and efficiency compared to existing methods.

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.

</details>


### [84] [MoiréNet: A Compact Dual-Domain Network for Image Demoiréing](https://arxiv.org/abs/2509.18910)
*Shuwei Guo,Simin Luan,Yan Ke,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: MoiréNet is a U-Net-based CNN framework that integrates frequency and spatial domain features to remove moiré patterns from digital images, achieving state-of-the-art performance with high parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: Moiré patterns from aliasing between display pixels and camera sensors create anisotropic, multi-scale artifacts that are challenging to remove from digital images.

Method: Uses Directional Frequency-Spatial Encoder (DFSE) to detect moiré orientation via directional difference convolution, and Frequency-Spatial Adaptive Selector (FSAS) for feature-adaptive suppression.

Result: Achieves state-of-the-art performance on public datasets with only 5.513M parameters (48% reduction vs ESDNet-L), combining superior restoration quality with parameter efficiency.

Conclusion: MoiréNet is well-suited for resource-constrained applications like smartphone photography, industrial imaging, and augmented reality due to its efficiency and effectiveness.

Abstract: Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.

</details>


### [85] [Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation](https://arxiv.org/abs/2509.18912)
*Yunzhe Shen,Kai Peng,Leiye Liu,Wei Ji,Jingjing Li,Miao Zhang,Yongri Piao,Huchuan Lu*

Main category: cs.CV

TL;DR: FAVS introduces a frequency-aware framework for audio-visual segmentation that addresses modality contradictions through frequency-domain decomposition and cross-modal consistency.


<details>
  <summary>Details</summary>
Motivation: Current AVS methods overlook inherent frequency-domain contradictions between audio (noise in high-frequency) and visual (rich details in high-frequency) modalities, leading to suboptimal performance.

Method: Proposes Frequency-Aware Audio-Visual Segmentation (FAVS) with two modules: Frequency-Domain Enhanced Decomposer (FDED) for modality-specific feature discrimination, and Synergistic Cross-Modal Consistency (SCMC) using mixture-of-experts for semantic consistency.

Result: Achieves state-of-the-art performance on three benchmark datasets, with qualitative visualizations confirming effectiveness of FDED and SCMC modules.

Conclusion: Reformulating AVS as frequency-domain decomposition problem with the proposed FAVS framework successfully addresses modality contradictions and improves segmentation performance.

Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.

</details>


### [86] [xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision](https://arxiv.org/abs/2509.18913)
*Nguyen Van Tu,Pham Nguyen Hai Long,Vo Hoai Viet*

Main category: cs.CV

TL;DR: This paper provides a comprehensive survey of four representative explainable AI (xAI) approaches for visual perception tasks: Saliency Maps, Concept Bottleneck Models, Prototype-based methods, and Hybrid approaches, analyzing their mechanisms, strengths, limitations, and evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for image analysis are often "black-box" systems with opaque decision-making processes, raising reliability concerns in critical applications. The field of xAI has emerged to provide human-understandable explanations for how AI models process and make decisions.

Method: The paper conducts a survey analysis of four key xAI approaches: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM), (iii) Prototype-based methods, and (iv) Hybrid approaches, examining their underlying mechanisms and evaluation metrics.

Result: The survey provides a comprehensive overview and comparative analysis of different xAI techniques, highlighting their respective strengths and limitations for visual perception tasks.

Conclusion: This survey serves as a guide for future research and applications in explainable AI for visual perception, helping researchers and practitioners understand and select appropriate xAI methods for their specific needs.

Abstract: Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.

</details>


### [87] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: This paper proposes a denoising diffusion probabilistic model (DDPM) with novel noise scheduling and time-step embedding techniques to generate high-quality synthetic LiDAR data for autonomous vehicle perception systems.


<details>
  <summary>Details</summary>
Motivation: Real-world LiDAR data collection is time-consuming and often affected by noise and sparsity due to adverse weather or sensor limitations, which hinders autonomous vehicle perception performance.

Method: The authors apply a DDPM enhanced with novel noise scheduling and time-step embedding techniques to generate synthetic LiDAR point clouds. These modifications improve the denoising process and the model's temporal awareness for producing more realistic point clouds.

Result: Extensive evaluation on IAMCV and KITTI-360 datasets using four performance metrics shows superior performance over most state-of-the-art baselines. The method effectively mitigates effects of noisy and sparse LiDAR data, producing diverse point clouds with rich spatial relationships and structural detail.

Conclusion: The proposed enhanced DDPM approach successfully generates high-quality synthetic LiDAR data that can improve autonomous vehicle perception systems by addressing real-world data collection challenges.

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [88] [Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset](https://arxiv.org/abs/2509.18919)
*Chuni Liu,Hongjie Li,Jiaqi Du,Yangyang Hou,Qian Sun,Lei Jin,Ke Xu*

Main category: cs.CV

TL;DR: AGSSP is a novel pretraining paradigm for metallic surface defect detection that uses anomaly priors to guide representation learning, overcoming domain gaps from natural images and limitations of existing self-supervised methods.


<details>
  <summary>Details</summary>
Motivation: Traditional pretraining faces domain gaps with natural images like ImageNet, while self-supervised pretraining on industrial data fails to distinguish subtle defects from complex background noise and textures.

Method: Two-stage framework: (1) pretrain backbone by distilling knowledge from anomaly maps to capture defect-salient features, (2) pretrain detector using pseudo-defect boxes from anomaly maps. Uses knowledge-enhanced method to generate high-quality anomaly maps and large-scale industrial dataset of 120,000 images.

Result: AGSSP consistently enhances performance across various settings, achieving up to 10% improvement in mAP@0.5 and 11.4% in mAP@0.5:0.95 compared to ImageNet-based models.

Conclusion: AGSSP effectively resolves the pretraining dilemma in metallic surface defect detection by explicitly guiding representation learning through anomaly priors, demonstrating significant performance improvements over existing approaches.

Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.

</details>


### [89] [Audio-Driven Universal Gaussian Head Avatars](https://arxiv.org/abs/2509.18924)
*Kartik Teotia,Helge Rhodin,Mohit Mendiratta,Hyeongwoo Kim,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: First method for audio-driven universal photorealistic avatar synthesis using Universal Head Avatar Prior (UHAP) that captures both geometric and appearance variations from audio inputs, outperforming geometry-only approaches.


<details>
  <summary>Details</summary>
Motivation: Previous approaches only mapped audio to geometric deformations while ignoring appearance variations. There's a need for a method that can generate photorealistic avatars with both accurate lip synchronization and nuanced expressive details.

Method: Combines person-agnostic speech model with Universal Head Avatar Prior (UHAP) trained on cross-identity multi-view videos. Uses monocular encoder for efficient personalization and maps raw audio directly into UHAP latent expression space that encodes both geometry and appearance.

Result: Generates highly realistic avatars with precise lip synchronization, eyebrow movement, gaze shifts, and realistic mouth interior appearance. Outperforms competing geometry-only methods in lip-sync accuracy, image quality, and perceptual realism.

Conclusion: This is the first generalizable audio-driven avatar model that accounts for detailed appearance modeling and rendering, demonstrating superior performance over existing approaches.

Abstract: We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.

</details>


### [90] [SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines](https://arxiv.org/abs/2509.18926)
*Pamela Osuna-Vargas,Altug Kamacioglu,Dominik F. Aschauer,Petros E. Vlachos,Sercan Alipek,Jochen Triesch,Simon Rumpel,Matthias Kaschube*

Main category: cs.CV

TL;DR: A machine learning pipeline for automated detection, tracking, and analysis of dendritic spines in 3D+time microscopy data to study synaptic dynamics in learning and memory.


<details>
  <summary>Details</summary>
Motivation: Large-scale analysis of dendritic spine dynamics is challenging and labor-intensive, despite spines being crucial structural components of synapses that reflect synaptic efficacy and are important for understanding learning and memory.

Method: Modular pipeline combining transformer-based detection, depth-tracking with spatial features, time-tracking using spatial consistency, and feature extraction for biologically relevant spine properties. Validated on open-source and new annotated datasets.

Result: The method was validated on multiple datasets, including newly published detection/depth-tracking and time-tracking datasets (first of their kind). Code, data, and pre-trained weights are publicly available.

Conclusion: The pipeline establishes a baseline for scalable, end-to-end analysis of dendritic spine dynamics, addressing challenges in biological data analysis and enabling future research in neural basis of learning and memory.

Abstract: Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.

</details>


### [91] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: A novel zero-shot image classification framework that combines vision-language models and pre-trained visual models in a self-learning cycle without requiring labeled training data.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods rely on extensive annotated datasets, which are often scarce in practical scenarios. Vision-language models and transfer learning offer promising solutions to address this data scarcity problem.

Method: Uses a confidence-based pseudo-labeling strategy to train a lightweight classifier directly on test data. The VLM identifies high-confidence samples, and a pre-trained visual model enhances their visual representations. These features iteratively train the classifier to capture complementary semantic and visual cues without supervision.

Result: Experimental evaluations on ten diverse datasets demonstrate that the approach outperforms baseline zero-shot methods.

Conclusion: The proposed framework effectively addresses data scarcity by combining VLMs and pre-trained visual models in a self-learning cycle, achieving superior performance without VLM fine-tuning or large language models.

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [92] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: MirrorScene3D dataset and ReflectiveGS method address 3D reconstruction challenges in mirror-rich environments by leveraging mirror reflections as complementary viewpoints rather than treating them as artifacts.


<details>
  <summary>Details</summary>
Motivation: Mirrors pose unique challenges for 3D reconstruction and novel view synthesis as they introduce view-dependent distortions. Existing methods that focus on symmetry mapping overlook the valuable information carried by mirror reflections, which can fill in absent details and enhance reconstruction quality.

Method: Proposes ReflectiveGS, an extension of 3D Gaussian Splatting that utilizes mirror reflections as complementary viewpoints. Also introduces MirrorScene3D dataset with 1256 high-quality images and annotated mirror masks for benchmarking.

Result: Experiments show ReflectiveGS outperforms existing methods in SSIM, PSNR, LPIPS metrics and training speed on the MirrorScene3D dataset.

Conclusion: The approach sets a new benchmark for 3D reconstruction in mirror-rich environments by effectively leveraging mirror reflections to enhance scene geometry and recover absent details.

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.

</details>


### [93] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: This paper presents a deep learning approach using YOLO detection algorithm and GAN-based data augmentation to localize the biliary tract in laparoscopic cholecystectomy images to prevent bile duct injuries.


<details>
  <summary>Details</summary>
Motivation: Laparoscopic cholecystectomy, while offering faster recovery and better cosmetic results, carries a higher risk of bile duct injury which significantly impacts quality of life and survival. Improving intraoperative visualization of the bile duct is essential to avoid such complications.

Method: The authors constructed and annotated an image database to train the YOLO detection algorithm for biliary tract localization. They employed classical data augmentation techniques and proposed using Generative Adversarial Networks (GANs) to generate synthetic training data.

Result: Experimental results were discussed, though specific performance metrics are not provided in the abstract. The paper also includes ethical considerations regarding the proposed approach.

Conclusion: The deep learning-based approach shows promise for improving bile duct visualization during laparoscopic cholecystectomy, potentially reducing the risk of bile duct injuries through better intraoperative localization.

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [94] [Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images](https://arxiv.org/abs/2509.18973)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: Prompt-DAS is a promptable multitask framework for domain adaptive segmentation of organelle instances from EM images, enabling flexible prompt usage for UDA, WDA, and interactive segmentation.


<details>
  <summary>Details</summary>
Motivation: To enable annotation-efficient learning for large-scale electron microscopy organelle segmentation by leveraging prompt-based approaches inspired by SAM, but with more flexibility and reduced annotation requirements.

Method: Proposes Prompt-DAS framework that uses point prompts during training and testing, incorporates auxiliary center-point detection, and employs prompt-guided contrastive learning. Can work with full points, sparse points, or no points on instances.

Result: Comprehensive experiments on challenging benchmarks show Prompt-DAS outperforms existing UDA, WDA, and SAM-based approaches.

Conclusion: The proposed Prompt-DAS framework effectively enables flexible domain adaptation for organelle segmentation with reduced annotation requirements through prompt-based multitask learning.

Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.

</details>


### [95] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: This paper proposes RoSe, a robust self-supervised stereo matching method that addresses performance degradation under adverse weather conditions by injecting visual foundation model priors and using scene correspondence learning with synthetic weather datasets.


<details>
  <summary>Details</summary>
Motivation: Self-supervised stereo matching methods perform poorly under adverse weather conditions (night, rain, fog) due to CNN feature extractors struggling with degraded regions and photometric consistency assumptions failing in these scenarios.

Method: The method injects robust priors from visual foundation models into CNN feature extractors and introduces scene correspondence priors using synthetic stereo datasets with clear/adverse image pairs. It uses a two-step training paradigm: robust self-supervised scene correspondence learning and adverse weather distillation.

Result: Extensive experiments show the method outperforms existing state-of-the-art self-supervised stereo matching methods under adverse weather conditions.

Conclusion: The proposed RoSe framework effectively improves stereo matching robustness in adverse weather by leveraging foundation model priors and scene correspondence learning, demonstrating superior performance compared to existing methods.

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [96] [Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](https://arxiv.org/abs/2509.19003)
*Honghao Chen,Xingzhou Lou,Xiaokun Feng,Kaiqi Huang,Xinlong Wang*

Main category: cs.CV

TL;DR: This paper introduces Chain of Step (CoS) reasoning for vision-language models, enabling fine-grained structured reasoning with step-level evaluation and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing chain of thought reasoning in vision-language models operates at coarse-grained levels, making it difficult to evaluate intermediate reasoning quality and perform fine-grained structured reasoning.

Method: Proposes a framework with step-level reasoning data, process reward model (PRM), and reinforcement learning training to enable accurate assessment of reasoning step quality.

Result: The models achieve strong baselines with consistent improvements on challenging vision-language benchmarks, and the analysis reveals insights about inference-time scaling.

Conclusion: This work establishes a baseline for vision-language models and provides insights into more complex multimodal reasoning, with the framework being simple, effective, and fully transparent.

Abstract: Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.

</details>


### [97] [Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](https://arxiv.org/abs/2509.19028)
*Ioannis Sarafis,Alexandros Papadopoulos,Anastasios Delopoulos*

Main category: cs.CV

TL;DR: A weakly supervised semantic segmentation method for food images using ViT-generated CAMs as prompts for SAM, achieving mIoU of 0.54 on FoodSeg103 dataset without pixel-level annotations.


<details>
  <summary>Details</summary>
Motivation: To develop a food image segmentation approach that eliminates the need for expensive pixel-level annotations by leveraging zero-shot capabilities of SAM and attention mechanisms of ViTs.

Method: Uses Swin Transformer to generate class activation maps (CAMs) from image-level annotations, which serve as prompts for SAM to produce segmentation masks. Explores image preprocessing and single/multi-mask generation strategies to improve mask quality.

Result: Achieved mIoU of 0.54 on FoodSeg103 dataset with multi-mask strategy, generating average 2.4 masks per image (excluding background).

Conclusion: The approach can accelerate food image annotation and serve as a component in food/nutrition tracking applications, demonstrating effective weakly supervised segmentation.

Abstract: In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.

</details>


### [98] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: DyL-UNet is a dynamic learning-based U-Net architecture that achieves temporally stable echocardiographic segmentation by incorporating Echo-Dynamics Graph and Cardiac Phase-Dynamics Attention to address frame-to-frame jitter issues.


<details>
  <summary>Details</summary>
Motivation: Echocardiography suffers from deformation and speckle noise causing temporal instability in segmentation, which weakens functional estimates and impairs clinical interpretability despite high single-frame accuracy.

Method: Proposes DyL-UNet with Echo-Dynamics Graph (EDG) for dynamic feature extraction, multiple Swin-Transformer encoder-decoder branches, and Cardiac Phase-Dynamics Attention (CPDA) at skip connections to enforce temporal consistency.

Result: Extensive experiments on CAMUS and EchoNet-Dynamic datasets show DyL-UNet maintains comparable segmentation accuracy to existing methods while achieving superior temporal consistency.

Conclusion: DyL-UNet provides a reliable solution for automated clinical echocardiography by ensuring both accurate and temporally stable segmentation.

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.

</details>


### [99] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: HyKid is an open-source dataset for pediatric hydrocephalus evaluation, featuring 3D MRIs with expert-annotated segmentations of brain tissues including choroid plexus, and structured clinical data extracted using RAG framework.


<details>
  <summary>Details</summary>
Motivation: Address the lack of publicly available, expert-annotated datasets for pediatric hydrocephalus evaluation, particularly those with choroid plexus segmentation.

Method: Created HyKid dataset from 48 pediatric patients with hydrocephalus, providing 3D MRIs reconstructed from routine low-resolution images using slice-to-volume algorithm, with manually corrected segmentations by an experienced neurologist, and structured clinical data extracted using Retrieval-Augmented Generation framework.

Result: Found strong correlation between choroid plexus volume and total CSF volume, achieving excellent performance in predictive model (AUC = 0.87).

Conclusion: HyKid provides a high-quality benchmark for neuroimaging algorithms development and reveals choroid plexus-related features in hydrocephalus assessments.

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [100] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: WaveletGaussian is an efficient framework for sparse-view 3D Gaussian object reconstruction that shifts diffusion to the wavelet domain, using diffusion only on low-resolution components and lightweight refinement for high-frequency details.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) performs poorly in sparse-view settings, and existing diffusion-based repair methods are computationally expensive due to fine-tuning and repair steps.

Method: Applies diffusion only to low-resolution LL wavelet subband, refines high-frequency subbands with lightweight network, and uses efficient online random masking strategy instead of leave-one-out for training pair curation.

Result: Achieves competitive rendering quality on Mip-NeRF 360 and OmniObject3D datasets while substantially reducing training time compared to previous methods.

Conclusion: WaveletGaussian provides an efficient alternative for sparse-view 3DGS reconstruction by leveraging wavelet domain processing and optimized training strategies.

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.

</details>


### [101] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: MsFIN is a multi-scale feature interaction network for early accident anticipation from dashcam videos that addresses occlusion challenges and complex temporal behavioral cues through multi-scale feature aggregation and transformer-based interactions.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in dashcam accident prediction including modeling feature-level interactions among occluded traffic participants and capturing complex asynchronous multi-temporal behavioral cues preceding accidents.

Method: Proposes MsFIN with three layers: multi-scale feature aggregation using short/mid/long-term temporal scales with transformer architecture, temporal feature processing under causal constraints, and multi-scale feature post fusion of scene and object features.

Result: Experiments on DAD and DADA datasets show MsFIN significantly outperforms state-of-the-art single-scale models in both prediction correctness and earliness, with ablation studies validating each module's effectiveness.

Conclusion: The network achieves superior performance through multi-scale feature fusion and contextual interaction modeling, demonstrating the importance of comprehensive temporal scale processing for early accident anticipation.

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [102] [3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference](https://arxiv.org/abs/2509.19082)
*Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: Sa2VA-i is an improved version of Sa2VA that fixes training-inference inconsistencies, achieving state-of-the-art results on multiple video segmentation benchmarks with significant performance gains.


<details>
  <summary>Details</summary>
Motivation: The original Sa2VA model underperforms on referring video object segmentation tasks due to inconsistencies between training and inference procedures.

Method: Proposed Sa2VA-i which rectifies the identified inconsistencies in the original Sa2VA model while using the same checkpoints.

Result: Sa2VA-i achieves improvements of +11.6 J&F on MeViS, +1.4 on Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS. The Sa2VA-i-1B model performs on par with the original Sa2VA-26B model on MeViS.

Conclusion: The work demonstrates the importance of implementation details and provides valuable insights for referring video segmentation, with code and updated models made available.

Abstract: Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i

</details>


### [103] [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087)
*Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova*

Main category: cs.CV

TL;DR: A training-free approach to adapt generalist multimodal models (like Gemini2.5) for multi-spectral remote sensing tasks without requiring specialized training, enabling zero-shot performance on land cover classification.


<details>
  <summary>Details</summary>
Motivation: Multi-spectral imagery is valuable for remote sensing but requires costly specialized models. Generalist multimodal models can't process multi-spectral data despite their powerful capabilities, creating a gap between specialized sensors and general AI models.

Method: Proposes a training-free approach that adapts multi-spectral inputs to the visual space of RGB-trained multimodal models and injects domain-specific information as instructions, demonstrated with Gemini2.5.

Result: Achieves strong zero-shot performance gains on popular remote sensing benchmarks for land cover and land use classification, showing easy adaptability of Gemini2.5 to new inputs.

Conclusion: Enables geospatial professionals to leverage powerful multimodal models with specialized sensor data, benefiting from rich reasoning capabilities without costly retraining.

Abstract: Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.

</details>


### [104] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: An adversarially-refined VQ-GAN framework with dense motion tokenization for compressing human motion heatmaps while preserving fine-grained details, outperforming baselines in reconstruction quality and temporal stability.


<details>
  <summary>Details</summary>
Motivation: Continuous human motion understanding is challenging due to high dimensionality and redundancy. Efficient compression and representation are needed for analyzing complex motion dynamics.

Method: Combines dense motion tokenization with adversarial refinement to eliminate reconstruction artifacts like motion smearing and temporal misalignment. Uses VQ-GAN framework for spatio-temporal heatmap compression.

Result: Outperforms dVAE baseline by 9.31% SSIM and reduces temporal instability by 37.1% on CMU Panoptic dataset. Analysis shows 2D motion requires 128-token vocabulary while 3D motion needs 1024-token codebook for optimal representation.

Conclusion: The method establishes practical deployment feasibility for diverse motion analysis applications with superior reconstruction quality and motion complexity analysis capabilities.

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [105] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: This paper investigates zero-shot capabilities of multimodal large language models (MLLMs) for traffic accident detection using infrastructure camera images, evaluating models like Pixtral, Gemini, and Gemma 3 with enhanced visual analytics.


<details>
  <summary>Details</summary>
Motivation: Traffic safety requires timely accident detection, and infrastructure-based vision sensors offer scalable solutions. The research aims to minimize reliance on extensive labeled datasets by exploring MLLMs' zero-shot capabilities.

Method: Used simulated DeepAccident dataset from CARLA to address data scarcity. Evaluated Gemini 1.5/2.0, Gemma 3, and Pixtral models without fine-tuning. Integrated YOLO for object detection, Deep SORT for tracking, and SAM for segmentation into enhanced prompts.

Result: Pixtral achieved best performance with F1-score of 0.71 and 83% recall. Gemini models gained precision (up to 90%) but suffered F1 and recall losses. Gemma 3 showed most balanced performance with minimal metric fluctuation.

Conclusion: MLLMs integrated with advanced visual analytics have substantial potential for real-world automated traffic monitoring systems, demonstrating effective zero-shot accident detection capabilities.

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [106] [Track-On2: Enhancing Online Point Tracking with Memory](https://arxiv.org/abs/2509.19115)
*Görkay Aydemir,Weidi Xie,Fatma Güney*

Main category: cs.CV

TL;DR: Track-On2 is a transformer-based model for online long-term point tracking that processes frames causally using memory mechanisms, achieving state-of-the-art results on synthetic and real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of long-term point tracking under significant appearance changes, motion, and occlusion in real-time streaming applications, requiring consistent point identification without future frame access.

Method: Extends Track-On with architectural refinements, more effective memory usage, and improved synthetic training strategies. Uses causal processing with memory mechanisms for temporal coherence, and performs coarse patch-level classification followed by refinement at inference.

Result: Achieves state-of-the-art results across five synthetic and real-world benchmarks, surpassing prior online trackers and even strong offline methods that use bidirectional context.

Conclusion: Causal, memory-based architectures trained purely on synthetic data provide scalable solutions for real-world point tracking, demonstrating effectiveness in handling drift and occlusions without requiring future frames.

Abstract: In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2

</details>


### [107] [KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments](https://arxiv.org/abs/2509.19129)
*Adam Romlein,Benjamin X. Hou,Yuval Boss,Cynthia L. Christman,Stacie Koslovsky,Erin E. Moreland,Jason Parham,Anthony Hoogs*

Main category: cs.CV

TL;DR: KAMERA is a multi-camera, multi-spectral system for real-time detection of seals and polar bears in aerial surveys, achieving 80% faster processing than previous methods with full open-source availability.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and accuracy in aerial wildlife surveys for ice-associated seals and polar bears in Arctic regions, addressing the need for faster data processing and better detection capabilities.

Method: Uses rigorous calibration and hardware synchronization of multiple cameras and spectra for object detection, with all data annotated with metadata and mapped onto a world plane for area estimation.

Result: Achieves up to 80% reduction in dataset processing time compared to previous methods, enabling real-time detection and accurate surveyed area estimates.

Conclusion: KAMERA successfully provides an efficient solution for wildlife monitoring and hopes to inspire similar mapping and detection efforts in the scientific community through open-source availability.

Abstract: We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.

</details>


### [108] [NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit](https://arxiv.org/abs/2509.19156)
*Maurf Hassan,Steven Davy,Muhammad Zawish,Owais Bin Zuber,Nouman Ashraf*

Main category: cs.CV

TL;DR: NeuCODEX is a neuromorphic co-inference architecture that reduces data transmission and energy consumption for Spiking Neural Networks (SNNs) by jointly optimizing spatial and temporal redundancy through spike-driven compression and dynamic early-exit mechanisms.


<details>
  <summary>Details</summary>
Motivation: Full SNN inference at the edge faces latency and energy constraints from fixed timestep overheads, while existing edge-cloud co-inference systems suffer from high latency and feature transmission costs.

Method: NeuCODEX incorporates a learned spike-driven compression module to reduce data transmission and employs a dynamic early-exit mechanism to adaptively terminate inference based on output confidence. It was prototyped on ResNet-18 and VGG-16 backbones in a real edge-to-cloud testbed.

Result: The system reduces data transfer by up to 2048x, edge energy consumption by over 90%, and end-to-end latency by up to 3x compared to edge-only inference, with a negligible accuracy drop of less than 2% on datasets including CIFAR10, Caltech, CIFAR10-DVS, and N-Caltech.

Conclusion: NeuCODEX enables practical, high-performance SNN deployment in resource-constrained environments by effectively addressing the challenges of edge-cloud co-inference systems.

Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.

</details>


### [109] [YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives](https://arxiv.org/abs/2509.19166)
*Siddharth Gupta,Jitin Singla*

Main category: cs.CV

TL;DR: YOLO-LAN is a YOLO-based polyp detection pipeline that outperforms existing methods on colorectal cancer screening datasets, achieving high precision in real-time polyp detection during colonoscopy.


<details>
  <summary>Details</summary>
Motivation: Manual polyp detection in colonoscopy is inconsistent and prone to oversight, so deep learning-based object detection offers a more accurate and real-time solution for colorectal cancer screening.

Method: Proposed YOLO-LAN pipeline using YOLO-based architecture trained with M2IoU loss, versatile data augmentations, and negative data to replicate real clinical situations.

Result: Achieved mAP$_{50}$ of 0.9619 and mAP$_{50:95}$ of 0.8599 with YOLOv12, and mAP$_{50}$ of 0.9540 and mAP$_{50:95}$ of 0.8487 with YOLOv8 on Kvasir-seg dataset, showing significant improvement in precision.

Conclusion: The method demonstrates robustness across polyp sizes and precise location detection, making it clinically relevant for AI-assisted colorectal screening.

Abstract: Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.

</details>


### [110] [The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC](https://arxiv.org/abs/2509.19183)
*Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han*

Main category: cs.CV

TL;DR: Analysis of SeC (enhanced SAM-2 framework) for semi-supervised video object segmentation, achieving 1st place in MOSEv2 track with 39.89% JF score


<details>
  <summary>Details</summary>
Motivation: To address complex semi-supervised video object segmentation challenges in the MOSEv2 track of LSVOS Challenge

Method: Analyzed and adapted SeC framework with long-term memory (preserves temporal continuity) and concept-aware memory (supplies semantic priors to suppress distractors)

Result: Achieved JF score of 39.89% on test set, ranking 1st in MOSEv2 track

Conclusion: Long-term memory and concept-aware memory together effectively address core challenges in semi-supervised VOS by maintaining temporal consistency and semantic understanding

Abstract: This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.

</details>


### [111] [Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](https://arxiv.org/abs/2509.19191)
*Yueyan Li,Chenggong Zhao,Zeyuan Zang,Caixia Yuan,Xiaojie Wang*

Main category: cs.CV

TL;DR: This paper analyzes Vision-Language Models (VLMs) by deconstructing visual processing into object recognition and spatial perception pathways, inspired by human vision's dual-stream hypothesis. It reveals a two-stage object recognition process and identifies geometric structures in spatial perception, leading to practical improvements in decoding efficiency and spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs process images through serialization, which differs from human parallel vision processing. Their opaque internal mechanisms limit understanding and architectural innovation. The research aims to provide deeper insights into VLM internals by studying visual processing through the lens of human vision theory.

Method: The study deconstructs VLM visual processing into object recognition (converting images to text token maps) and spatial perception (analyzing positional representations). It identifies a two-stage object recognition process and derives geometric structures for spatial perception. Based on findings, it develops an instruction-agnostic token compression algorithm using a plug-and-play visual decoder and a RoPE scaling technique.

Result: The research validates that VLM object recognition unfolds as a two-stage process from shallow to deep layers (attribute recognition to semantic disambiguation). It theoretically derives and empirically verifies the geometric structure underlying positional representations. The proposed methods improve decoding efficiency and enhance spatial reasoning capabilities.

Conclusion: The work provides deeper understanding of VLM internals and offers clear design principles for future architectures. The findings validate the dual-stream approach to studying visual processing and demonstrate practical improvements in model efficiency and reasoning capabilities.

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.

</details>


### [112] [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://arxiv.org/abs/2509.19203)
*Ioanna Ntinou,Alexandros Xenos,Yassine Ouali,Adrian Bulat,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: This paper introduces a vision-free, single-encoder retrieval pipeline that replaces traditional text-to-image retrieval with a text-to-text paradigm using VLLM-generated structured image descriptions, achieving state-of-the-art performance while addressing modality gap and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: Traditional contrastively-trained VLMs like CLIP exhibit shallow language understanding, modality gaps due to dual-encoder design, computational expense from web-collected data, and privacy concerns. The authors aim to challenge the necessity of vision encoders for retrieval tasks.

Method: Proposes a vision-free retrieval pipeline that uses VLLM-generated structured image descriptions instead of raw images, migrating from text-to-image to text-to-text retrieval. The approach requires only a few hours of calibration on two GPUs.

Result: The vision-free retriever matches and often surpasses traditional multimodal models, achieving state-of-the-art zero-shot performance on multiple retrieval and compositionality benchmarks with models as small as 0.3B parameters. It reduces modality gap, improves compositionality, and performs better on short and long caption queries.

Conclusion: The text-to-text paradigm with structured image descriptions provides significant advantages over traditional multimodal retrieval, including reduced modality gap, improved compositionality, better privacy protection, and superior performance with smaller models, demonstrating that vision encoders may not be necessary for retrieval tasks.

Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP

</details>


### [113] [Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](https://arxiv.org/abs/2509.19207)
*Israfel Salazar,Desmond Elliott,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: Training for compositionality improves long-caption understanding and vice versa, but gains depend on data quality and model design. High-quality long-caption data enables strong performance in both capabilities.


<details>
  <summary>Details</summary>
Motivation: Understanding long, dense captions remains challenging for vision-language models. The paper hypothesizes that compositionality (reasoning about object-attribute bindings and relationships) is key to this capability.

Method: Train and evaluate models targeting compositionality and long-caption understanding separately, then analyze their bidirectional relationship and sensitivity to data quality and design choices.

Result: Reveals bidirectional relationship: compositional training improves long-caption retrieval, and long-caption training promotes compositionality. Gains are sensitive to data quality and model architecture.

Conclusion: Compositional understanding and long-caption understanding are intertwined capabilities that can be jointly learned through training on dense, grounded descriptions. High-quality data enables strong performance in both tasks.

Abstract: Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.

</details>


### [114] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: A framework using synthetic RGB imagery, limited real annotations, and GAN-based cross-modality alignment to improve plant segmentation in thermal images for field phenotyping.


<details>
  <summary>Details</summary>
Motivation: Accurate plant segmentation in thermal imagery is challenging due to low contrast between plants and weeds and frequent occlusions in outdoor environments.

Method: Trained models on 1,128 synthetic images of crop/weed mixtures, integrated 5 real segmented field images using various sampling strategies, and used CycleGAN-turbo for RGB-to-thermal translation to enable cross-modal alignment.

Result: Maximum relative improvement of 22% for weed class and 17% for plant class compared to full real-data baseline when combining synthetic data with limited real annotations.

Conclusion: Combining synthetic data with limited manual annotations and cross-domain translation via generative models can significantly boost segmentation performance in complex field environments for multi-model imagery.

Abstract: Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.

</details>


### [115] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: This paper proposes a continual learning approach for face forgery detection using a Developmental Mixture of Experts (MoE) architecture with LoRA models to adapt to evolving forgery techniques while preventing catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of digital face generation and manipulation techniques outpaces existing detection models, requiring a solution that can quickly adapt to new forgery types with limited data while retaining knowledge of previous types.

Method: Uses a Developmental MoE architecture with LoRA models organized into Real-LoRA for genuine face knowledge and multiple Fake-LoRAs for different forgery types. Employs orthogonal learning directions and gradient integration to prevent forgetting.

Result: Experimental results show effectiveness under both dataset and manipulation type incremental protocols, demonstrating successful adaptation to new forgery types.

Conclusion: The proposed continual learning framework effectively addresses the challenge of evolving face forgery techniques by enabling incremental learning while maintaining detection capabilities for previously learned forgery types.

Abstract: The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.

</details>


### [116] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O is a unified multi-modal Masked Diffusion Model that supports both image understanding and generation tasks, including object grounding, image editing, and high-resolution synthesis, outperforming existing models with faster inference.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal diffusion models like MMaDa and Muddit are limited to simple image-level tasks and low-resolution generation. Lavida-O aims to overcome these limitations by enabling advanced capabilities such as object grounding and high-resolution synthesis.

Method: Lavida-O uses a Masked Diffusion Model with novel techniques including Elastic Mixture-of-Transformer architecture, universal text conditioning, and stratified sampling. It leverages understanding capabilities to improve generation through planning and iterative self-reflection.

Result: Lavida-O achieves state-of-the-art performance on benchmarks like RefCOCO (object grounding), GenEval (text-to-image generation), and ImgEdit (image editing), outperforming models such as Qwen2.5-VL and FluxKontext-dev with faster inference.

Conclusion: Lavida-O demonstrates that a unified multimodal diffusion model can effectively combine understanding and generation tasks, offering superior performance and efficiency compared to existing autoregressive and continuous diffusion models.

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.

</details>


### [117] [ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](https://arxiv.org/abs/2509.19245)
*Benedetta Liberatori,Alessandro Conti,Lorenzo Vaquero,Yiming Wang,Elisa Ricci,Paolo Rota*

Main category: cs.CV

TL;DR: The paper introduces Concept-based Video Similarity estimation (ConViS), a new task for comparing videos using interpretable similarity scores across semantic concepts, along with ConViS-Bench benchmark and analysis of state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Current video similarity models rely on broad global scores, but humans compare videos by considering multiple aspects (actions, locations, etc.). This nuanced understanding hasn't been thoroughly studied and presents challenges for existing models.

Method: Proposed ConViS task computes interpretable similarity scores across predefined semantic concepts. Created ConViS-Bench benchmark with annotated video pairs containing concept-level scores and textual descriptions. Benchmarked state-of-the-art models to evaluate their alignment with human judgments.

Result: Significant performance differences were found on ConViS, showing that some concepts present greater challenges for video similarity estimation. The benchmark provides insights into model capabilities and limitations.

Conclusion: ConViS enables human-like reasoning about video similarity and supports applications like concept-conditioned video retrieval. ConViS-Bench serves as a valuable resource for advancing language-driven video understanding research.

Abstract: What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.

</details>


### [118] [Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies](https://arxiv.org/abs/2509.19258)
*Dheerendranath Battalapalli,Apoorva Safai,Maria Jaramillo,Hyemin Um,Gustavo Adalfo Pineda Ortiz,Ulas Bagci,Manmeet Singh Ahluwalia,Marwa Ismail,Pallavi Tiwari*

Main category: cs.CV

TL;DR: GrRAiL is a graph-based radiomic learning method that captures intralesional heterogeneity on MRI by analyzing spatial relationships among sub-region clusters, outperforming existing methods in distinguishing tumor recurrence from radiation effects in brain tumors and stratifying pancreatic neoplasms.


<details>
  <summary>Details</summary>
Motivation: Current radiomics methods aggregate features across lesion regions and miss complex spatial relationships, making it difficult to reliably distinguish confounding pathologies from malignant neoplasms on routine imaging.

Method: GrRAiL identifies clusters of sub-regions using per-voxel radiomic measurements, then computes graph-theoretic metrics to quantify spatial associations among clusters, creating weighted graphs that encode higher-order spatial relationships within lesions.

Result: In multi-institutional evaluations across three clinical use cases (glioblastoma, brain metastasis, pancreatic IPMNs), GrRAiL consistently outperformed state-of-the-art baselines with test accuracies of 78%, 74%, and 75% respectively, showing >10% improvements over comparators.

Conclusion: GrRAiL effectively captures intralesional heterogeneity and provides a clinically feasible approach for disambiguating confounding pathologies from malignancy, demonstrating significant improvements over existing radiomics and graph-based methods.

Abstract: A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.

</details>


### [119] [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](https://arxiv.org/abs/2509.19259)
*Markos Diomataris,Berat Mert Albaba,Giorgio Becherini,Partha Ghosh,Omid Taheri,Michael J. Black*

Main category: cs.CV

TL;DR: CLOPS is the first human avatar that uses egocentric vision to perceive surroundings and navigate, generating human-like motion by decoupling low-level motion skills from high-level visual control.


<details>
  <summary>Details</summary>
Motivation: Current human motion generation methods use task-specific perception that differs from human perception. The paper argues that generating human-like avatar behavior requires human-like perception through egocentric vision.

Method: Two-stage approach: 1) Train a motion prior model on large motion capture dataset for low-level motion skills, 2) Train a policy using Q-learning to map egocentric visual inputs to high-level control commands for the motion prior.

Result: Avatars demonstrate human-like motion characteristics, such as walking to avoid obstacles in their visual field, showing that egocentric vision can generate realistic human behavior.

Conclusion: Equipping avatars with human-like sensors, particularly egocentric vision, is promising for training avatars that behave like humans.

Abstract: The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.

</details>


### [120] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: This paper addresses layout-to-image generation challenges with overlapping bounding boxes, introducing OverLayScore metric and OverLayBench benchmark to evaluate model performance on complex overlaps, and proposes CreatiLayout-AM as a solution.


<details>
  <summary>Details</summary>
Motivation: Current layout-to-image generation methods struggle with layouts containing significant overlap between bounding boxes, particularly with large overlapping regions and instances with minimal semantic distinction, which degrade generation quality.

Method: The authors introduce OverLayScore to quantify overlapping complexity, create OverLayBench benchmark with balanced distribution across OverLayScore levels, and propose CreatiLayout-AM model fine-tuned on amodal mask dataset to handle complex overlaps.

Result: Analysis shows existing benchmarks are biased toward simpler cases with low OverLayScore values, limiting their effectiveness in evaluating model performance under challenging overlapping conditions.

Conclusion: The contributions provide groundwork for more robust layout-to-image generation under realistic and challenging scenarios with complex bounding box overlaps.

Abstract: Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [121] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: A self-distillation framework that distills 3D knowledge from video diffusion models into 3D Gaussian Splatting representation, enabling 3D scene generation from text or single images without requiring multi-view training data.


<details>
  <summary>Details</summary>
Motivation: Current 3D reconstruction methods require real-world multi-view data which is not always available, and video diffusion models are limited to 2D applications despite their strong imagination capabilities.

Method: Augments RGB decoder with 3DGS decoder supervised by RGB decoder output, trained purely with synthetic data from video diffusion models. Supports text-to-3D and image-to-3D generation, and extends to dynamic scenes from monocular video.

Result: Achieves state-of-the-art performance in both static and dynamic 3D scene generation.

Conclusion: The framework successfully bridges 2D video diffusion models with explicit 3D representations, enabling real-time 3D scene synthesis without multi-view training data requirements.

Abstract: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

</details>


### [122] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: VolSplat introduces a voxel-aligned Gaussian prediction paradigm that overcomes limitations of pixel-aligned 3D Gaussian Splatting, achieving state-of-the-art novel view synthesis with improved multi-view consistency and geometric accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing pixel-aligned 3D Gaussian Splatting methods suffer from view dependency, biased density distributions, and alignment errors due to their reliance on 2D pixel-to-3D Gaussian mapping, especially when handling occlusions or low-texture regions.

Method: VolSplat replaces pixel alignment with voxel-aligned Gaussians by directly predicting Gaussians from a 3D voxel grid, enabling adaptive density control based on 3D scene complexity and eliminating error-prone 2D feature matching.

Result: Experiments on RealEstate10K and ScanNet benchmarks show state-of-the-art performance with more plausible Gaussian reconstructions, improved geometric consistency, and enhanced novel-view rendering quality.

Conclusion: VolSplat establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for wider research applications in 3D scene understanding.

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

</details>


### [123] [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](https://arxiv.org/abs/2509.19300)
*Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing*

Main category: cs.CV

TL;DR: CAR-Flow is a lightweight method that improves conditional generative modeling by reparameterizing source and target distributions to shorten probability paths, reducing model complexity while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current diffusion and flow-based methods require models to learn both mass transport and conditional injection simultaneously, which is demanding. The goal is to ease this burden by conditioning the source and target distributions.

Method: Proposes Condition-Aware Reparameterization for Flow Matching (CAR-Flow) - a learned shift that conditions the source, target, or both distributions to shorten the probability path the model must learn.

Result: On ImageNet-256, CAR-Flow reduces FID from 2.07 to 1.68 when equipped with SiT-XL/2, while adding less than 0.6% additional parameters. Visual and quantitative improvements shown on synthetic data.

Conclusion: CAR-Flow effectively reduces the learning burden on conditional generative models by shortening probability paths through learned reparameterization, achieving better performance with minimal parameter overhead.

Abstract: Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [124] [Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs](https://arxiv.org/abs/2509.18113)
*Xin Hu,Yue Kang,Guanzi Yao,Tianze Kang,Mengjie Wang,Heyao Liu*

Main category: cs.CL

TL;DR: This paper introduces a unified multi-task learning framework with dynamic prompt scheduling to address generalization limitations in large language models, using a prompt pool and task-aware scheduling to enhance cross-task semantic capture and reduce task interference.


<details>
  <summary>Details</summary>
Motivation: To overcome the generalization limitations of large language models in multi-task and cross-domain settings, particularly addressing the dependency on fixed prompt templates in prior methods like SPoT.

Method: Proposes a dynamic prompt scheduling mechanism with a prompt pool, task-aware scheduling strategy, task embeddings with gating mechanism for prompt fusion, and joint multi-task learning optimization with automatic scheduling weight learning.

Result: The method significantly improves performance on language understanding and knowledge reasoning tasks, maintains model stability, and enhances transferability across different task numbers and prompt temperature variations.

Conclusion: The proposed prompt scheduling approach demonstrates strong applicability and effectiveness for unified multi-task modeling and cross-domain adaptation, effectively mitigating task interference and negative transfer.

Abstract: This study addresses the generalization limitations commonly observed in
large language models under multi-task and cross-domain settings. Unlike prior
methods such as SPoT, which depends on fixed prompt templates, our study
introduces a unified multi-task learning framework with dynamic prompt
scheduling mechanism. By introducing a prompt pool and a task-aware scheduling
strategy, the method dynamically combines and aligns prompts for different
tasks. This enhances the model's ability to capture semantic differences across
tasks. During prompt fusion, the model uses task embeddings and a gating
mechanism to finely control the prompt signals. This ensures alignment between
prompt content and task-specific demands. At the same time, it builds flexible
sharing pathways across tasks. In addition, the proposed optimization objective
centers on joint multi-task learning. It incorporates an automatic learning
strategy for scheduling weights, which effectively mitigates task interference
and negative transfer. To evaluate the effectiveness of the method, a series of
sensitivity experiments were conducted. These experiments examined the impact
of prompt temperature parameters and task number variation. The results confirm
the advantages of the proposed mechanism in maintaining model stability and
enhancing transferability. Experimental findings show that the prompt
scheduling method significantly improves performance on a range of language
understanding and knowledge reasoning tasks. These results fully demonstrate
its applicability and effectiveness in unified multi-task modeling and
cross-domain adaptation.

</details>


### [125] [GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models](https://arxiv.org/abs/2509.18122)
*Yue Zhang,Jiaxin Zhang,Qiuyu Ren,Tahsin Saffat,Xiaoxuan Liu,Zitong Yang,Banghua Zhu,Yi Ma*

Main category: cs.CL

TL;DR: GAUSS is a benchmark that evaluates LLMs' mathematical abilities across 12 core skill dimensions grouped into three domains, providing comprehensive and interpretable profiles of models' mathematical intelligence.


<details>
  <summary>Details</summary>
Motivation: To create a more fine-grained and interpretable evaluation of LLMs' mathematical abilities by categorizing problems according to cognitive skills and designing tasks that isolate specific abilities, moving beyond single-score metrics.

Method: Developed a benchmark with 12 skill dimensions across three domains (knowledge/understanding, problem solving/communication, meta-skills/creativity), categorizing problems by cognitive skills to isolate specific mathematical abilities.

Result: The benchmark successfully creates comprehensive skill profiles that faithfully represent models' underlying mathematical intelligence, as demonstrated by profiling GPT-5-thinking and comparing it with o4-mini-high.

Conclusion: GAUSS provides multidimensional, skill-based evaluation that reveals detailed strengths and weaknesses of LLMs, offering more interpretable insights than traditional single-score benchmarks.

Abstract: We introduce \textbf{GAUSS} (\textbf{G}eneral \textbf{A}ssessment of
\textbf{U}nderlying \textbf{S}tructured \textbf{S}kills in Mathematics), a
benchmark that evaluates LLMs' mathematical abilities across twelve core skill
dimensions, grouped into three domains: knowledge and understanding, problem
solving and communication, and meta-skills and creativity. By categorizing
problems according to cognitive skills and designing tasks that isolate
specific abilities, GAUSS constructs comprehensive, fine-grained, and
interpretable profiles of models' mathematical abilities. These profiles
faithfully represent their underlying mathematical intelligence. To exemplify
how to use the \textsc{GAUSS} benchmark, we have derived the skill profile of
\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its
differences relative to \textsc{o4-mini-high}, thereby underscoring the value
of multidimensional, skill-based evaluation.

</details>


### [126] [Event Causality Identification with Synthetic Control](https://arxiv.org/abs/2509.18156)
*Haoyu Wang,Fengze Liu,Jiayao Zhang,Dan Roth,Kyle Richardson*

Main category: cs.CL

TL;DR: This paper proposes a novel approach to Event Causality Identification (ECI) using the Rubin Causal Model, treating events as treatment-outcome pairs and using synthetic control methods to generate 'twins' for causal inference, outperforming traditional methods including GPT-4.


<details>
  <summary>Details</summary>
Motivation: Traditional ECI methods relying on linguistic patterns and multi-hop relational inference risk false causality identification due to informal causality usage and specious graphical inference. There's a need for more robust causal identification methods.

Method: Adopts the Rubin Causal Model by treating the first event as treatment and second as outcome. Uses synthetic control method to generate 'twins' from relevant historical data through text embedding synthesis and inversion techniques, enabling conceptual manipulation in text domain.

Result: The approach demonstrates superior performance on the causality benchmark COPES-hard, showing more robust causal relation identification compared to previous methods including GPT-4.

Conclusion: The synthetic control method combined with Rubin Causal Model provides a more reliable framework for event causality identification, effectively addressing limitations of traditional pattern-based approaches and achieving state-of-the-art results.

Abstract: Event causality identification (ECI), a process that extracts causal
relations between events from text, is crucial for distinguishing causation
from correlation. Traditional approaches to ECI have primarily utilized
linguistic patterns and multi-hop relational inference, risking false causality
identification due to informal usage of causality and specious graphical
inference. In this paper, we adopt the Rubin Causal Model to identify event
causality: given two temporally ordered events, we see the first event as the
treatment and the second one as the observed outcome. Determining their
causality involves manipulating the treatment and estimating the resultant
change in the likelihood of the outcome. Given that it is only possible to
implement manipulation conceptually in the text domain, as a work-around, we
try to find a twin for the protagonist from existing corpora. This twin should
have identical life experiences with the protagonist before the treatment but
undergoes an intervention of treatment. However, the practical difficulty of
locating such a match limits its feasibility. Addressing this issue, we use the
synthetic control method to generate such a twin' from relevant historical
data, leveraging text embedding synthesis and inversion techniques. This
approach allows us to identify causal relations more robustly than previous
methods, including GPT-4, which is demonstrated on a causality benchmark,
COPES-hard.

</details>


### [127] [ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization](https://arxiv.org/abs/2509.18158)
*Seungyoun Yi,Minsoo Khang,Sungrae Park*

Main category: cs.CL

TL;DR: ZERA is a novel Automatic Prompt Optimization framework that jointly optimizes both system and user prompts using structured feedback and minimal examples, achieving fast convergence to high-quality prompts across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing APO methods focus only on user prompts, rely on unstructured feedback, require large sample sizes and long iteration cycles, making them costly and brittle.

Method: ZERA scores prompts using eight generalizable criteria with automatically inferred weights, and revises prompts based on these structured critiques. It jointly optimizes both system and user prompts through principled, low-overhead refinement.

Result: Experimental results across five LLMs and nine diverse datasets show consistent improvements over strong baselines in reasoning, summarization, and code generation tasks.

Conclusion: ZERA enables fast convergence to high-quality prompts using minimal examples and short iteration cycles, with ablation studies confirming the contribution of each component to effective prompt construction.

Abstract: Automatic Prompt Optimization (APO) improves large language model (LLM)
performance by refining prompts for specific tasks. However, prior APO methods
typically focus only on user prompts, rely on unstructured feedback, and
require large sample sizes and long iteration cycles-making them costly and
brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a
novel framework that jointly optimizes both system and user prompts through
principled, low-overhead refinement. ZERA scores prompts using eight
generalizable criteria with automatically inferred weights, and revises prompts
based on these structured critiques. This enables fast convergence to
high-quality prompts using minimal examples and short iteration cycles. We
evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,
summarization, and code generation tasks. Experimental results demonstrate
consistent improvements over strong baselines. Further ablation studies
highlight the contribution of each component to more effective prompt
construction. Our implementation including all prompts is publicly available at
https://github.com/younatics/zera-agent.

</details>


### [128] [Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning](https://arxiv.org/abs/2509.18163)
*Haodong Zhao,Chenyan Zhao,Yansi Li,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: LLMs' reasoning is vulnerable to external information quality - helpful context improves accuracy but misleading information causes catastrophic performance drops, amplified by step-by-step thinking processes.


<details>
  <summary>Details</summary>
Motivation: To investigate how auxiliary information (helpful, irrelevant, or misleading) causally impacts LLMs' reasoning capabilities in real-world scenarios where models are augmented with external data.

Method: Introduce SciAux dataset derived from ScienceQA to systematically test model robustness against different types of auxiliary information, examining how step-by-step thinking processes interact with information quality.

Result: Thinking mode is a double-edged sword: helpful context improves accuracy but misleading information causes catastrophic performance drops that are amplified by the thinking process. Thinking reinforces errors when provided with misinformation.

Conclusion: The challenge is not just making models "think" but endowing them with critical evaluation faculties to assess the information their reasoning is based on. Current thinking processes lack robustness against misinformation.

Abstract: The capacity of Large Language Models (LLMs) to reason is fundamental to
their application in complex, knowledge-intensive domains. In real-world
scenarios, LLMs are often augmented with external information that can be
helpful, irrelevant, or even misleading. This paper investigates the causal
impact of such auxiliary information on the reasoning process of LLMs with
explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset
derived from ScienceQA, to systematically test the robustness of the model
against these types of information. Our findings reveal a critical
vulnerability: the model's deliberative "thinking mode" is a double-edged
sword. While helpful context improves accuracy, misleading information causes a
catastrophic drop in performance, which is amplified by the thinking process.
Instead of conferring robustness, thinking reinforces the degree of error when
provided with misinformation. This highlights that the challenge is not merely
to make models "think", but to endow them with the critical faculty to evaluate
the information upon which their reasoning is based. The SciAux dataset is
available at https://huggingface.co/datasets/billhdzhao/SciAux.

</details>


### [129] [SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework](https://arxiv.org/abs/2509.18167)
*Junlin Wang,Zehao Wu,Shaowei Lu,Yanlan Li,Xinghao Huang*

Main category: cs.CL

TL;DR: A multi-agent framework with process supervision to optimize retriever-generator coordination in RAG systems, achieving better accuracy and interpretability without modifying existing components.


<details>
  <summary>Details</summary>
Motivation: Standard RAG systems suffer from suboptimal coordination between independently developed retriever and generator components, leading to irrelevant document retrieval and poor evidence utilization.

Method: Proposes a process-supervised multi-agent framework with Decision Maker and Knowledge Selector agents, trained with PPO using LLM-as-a-Judge for process-level rewards and tree-structured rollout for diverse reasoning paths.

Result: Achieves higher accuracy, more stable convergence, and produces more interpretable reasoning trajectories on single-hop and multi-hop QA benchmarks compared to standard RAG baselines.

Conclusion: The modular framework effectively bridges the retriever-generator gap, is plug-and-play compatible with existing RAG systems, and practical for real-world applications.

Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access external knowledge sources, but the effectiveness of RAG relies on the
coordination between the retriever and the generator. Since these components
are developed independently, their interaction is often suboptimal: the
retriever may return irrelevant or redundant documents, while the generator may
fail to fully leverage retrieved evidence. In this work, we propose a
process-supervised multi-agent framework to bridge the gap between retriever
and generator. The framework introduces two lightweight agents: a Decision
Maker, which determines when to continue retrieval or stop for answer
generation, and a Knowledge Selector, which filters retrieved documents to
retain only the most useful evidence. To provide fine-grained supervision, we
employ an LLM-as-a-Judge that evaluates each intermediate action with
process-level rewards, ensuring more accurate credit assignment than relying
solely on final answer correctness. We further adopt a tree-structured rollout
strategy to explore diverse reasoning paths, and train both agents with
Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on
single-hop and multi-hop question answering benchmarks show that our approach
achieves higher accuracy, more stable convergence, and produces more
interpretable reasoning trajectories compared with standard RAG baselines.
Importantly, the proposed framework is modular and plug-and-play, requiring no
modification to the retriever or generator, making it practical for real-world
RAG applications.

</details>


### [130] [ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers](https://arxiv.org/abs/2509.18175)
*Aditi Debsharma,Bhushan Jagyasi,Surajit Sen,Priyanka Pandey,Devicharith Dovari,Yuvaraj V. C,Rosalin Parida,Gopali Contractor*

Main category: cs.CL

TL;DR: Proposes ERFC architecture for emotion recognition and forecasting in conversations, particularly useful for call centers to improve customer experience by predicting future emotions.


<details>
  <summary>Details</summary>
Motivation: To enhance customer experience in call centers by enabling agents to anticipate future emotions and provide appropriate resolutions, transforming unhappy customers into happy ones.

Method: ERFC architecture that considers multi-modalities, emotion attributes, context, and utterance interdependencies in conversations.

Result: Intensive experiments on IEMOCAP dataset demonstrated the feasibility of the proposed ERFC approach.

Conclusion: ERFC provides significant business value for applications like call centers where customer happiness is paramount.

Abstract: Emotion Recognition in Conversation has been seen to be widely applicable in
call center analytics, opinion mining, finance, retail, healthcare, and other
industries. In a call center scenario, the role of the call center agent is not
just confined to receiving calls but to also provide good customer experience
by pacifying the frustration or anger of the customers. This can be achieved by
maintaining neutral and positive emotion from the agent. As in any
conversation, the emotion of one speaker is usually dependent on the emotion of
other speaker. Hence the positive emotion of an agent, accompanied with the
right resolution will help in enhancing customer experience. This can change an
unhappy customer to a happy one. Imparting the right resolution at right time
becomes easier if the agent has the insight of the emotion of future
utterances. To predict the emotions of the future utterances we propose a novel
architecture, Emotion Recognition and Forecasting in Conversation. Our proposed
ERFC architecture considers multi modalities, different attributes of emotion,
context and the interdependencies of the utterances of the speakers in the
conversation. Our intensive experiments on the IEMOCAP dataset have shown the
feasibility of the proposed ERFC. This approach can provide a tremendous
business value for the applications like call center, where the happiness of
customer is utmost important.

</details>


### [131] [Evaluating Large Language Models for Detecting Antisemitism](https://arxiv.org/abs/2509.18293)
*Jay Patel,Hrudayangam Mehta,Jeremy Blackburn*

Main category: cs.CL

TL;DR: Evaluation of 8 open-source LLMs for detecting antisemitic content using in-context definitions as policy guidelines, with a new Guided-CoT prompting technique that improves performance across all models.


<details>
  <summary>Details</summary>
Motivation: Automated hate speech detection tools require continuous training to adapt to evolving social media content, and this work aims to assess LLMs' capabilities for detecting antisemitic content specifically.

Method: Leveraging in-context definitions as policy guidelines, exploring various prompting techniques including a newly designed Guided-CoT (Chain-of-Thought) prompt that handles in-context policy effectively.

Result: Guided-CoT increased performance across all evaluated models regardless of decoding configuration, model sizes, or reasoning capability. Llama 3.1 70B outperformed fine-tuned GPT-3.5. The study also revealed notable differences in semantic divergence in model-generated rationales.

Conclusion: The experiments highlight significant differences across LLMs in terms of utility, explainability, and reliability for hate speech detection, with Guided-CoT proving effective for improving antisemitic content detection performance.

Abstract: Detecting hateful content is a challenging and important problem. Automated
tools, like machine-learning models, can help, but they require continuous
training to adapt to the ever-changing landscape of social media. In this work,
we evaluate eight open-source LLMs' capability to detect antisemitic content,
specifically leveraging in-context definition as a policy guideline. We explore
various prompting techniques and design a new CoT-like prompt, Guided-CoT.
Guided-CoT handles the in-context policy well, increasing performance across
all evaluated models, regardless of decoding configuration, model sizes, or
reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.
Additionally, we examine LLM errors and introduce metrics to quantify semantic
divergence in model-generated rationales, revealing notable differences and
paradoxical behaviors among LLMs. Our experiments highlight the differences
observed across LLMs' utility, explainability, and reliability.

</details>


### [132] [Exploiting Tree Structure for Credit Assignment in RL Training of LLMs](https://arxiv.org/abs/2509.18314)
*Hieu Tran,Zonghai Yao,Hong Yu*

Main category: cs.CL

TL;DR: TEMPO is a critic-free RL algorithm that improves LLM reasoning by using prefix trees to compute nonparametric token-level credit assignment, outperforming PPO and GRPO on math and medical QA tasks.


<details>
  <summary>Details</summary>
Motivation: Sparse delayed rewards in long reasoning sequences make token-level credit assignment challenging. Existing methods like PPO are complex to train and GRPO ignores branching structure, creating a need for simpler, more effective credit assignment.

Method: P2T converts response groups into prefix trees to compute nonparametric prefix values. TEMPO builds on this with branch-gated temporal-difference corrections that provide precise token-level credit without learned value networks.

Result: TEMPO outperforms PPO and GRPO on Qwen3-1.7B/4B models across in-distribution (MATH, MedQA) and out-of-distribution benchmarks, achieving higher accuracy with similar training time.

Conclusion: The prefix tree approach enables effective token-level credit assignment without complex value models, making TEMPO a simple yet powerful alternative for verifiable-reward reasoning tasks.

Abstract: Reinforcement learning improves LLM reasoning, yet sparse delayed reward over
long sequences makes token-level credit assignment the key bottleneck. We study
the verifiable-reward setting, where the final answer is checkable and multiple
responses can be drawn per prompt. Reasoning tasks in math and medical QA align
with this setup, where only a few decision tokens significantly impact the
outcome. PPO offers token-level advantages with a learned value model, but it
is complex to train both the actor and critic models simultaneously, and it is
not easily generalizable, as the token-level values from the critic model can
make training prone to overfitting. GRPO is critic-free and supports verifiable
rewards, but spreads a single sequence-level return across tokens and ignores
branching. We introduce \textbf{Prefix-to-Tree (P2T)}, a simple procedure that
converts a group of responses into a prefix tree and computes
\emph{nonparametric} prefix values \(V(s)\) by aggregating descendant outcomes.
Built on P2T, we propose \textbf{TEMPO} (\emph{\textbf{T}ree-\textbf{E}stimated
\textbf{M}ean Prefix Value for \textbf{P}olicy \textbf{O}ptimization}), a
critic-free algorithm that augments the group-relative outcome signal of GRPO
with \emph{branch-gated} temporal-difference corrections derived from the tree.
At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO
reduces to GRPO; at branching tokens, it supplies precise token-level credit
without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,
TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and
out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and
reaches higher validation accuracy with roughly the same wall-clock time.

</details>


### [133] [Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning](https://arxiv.org/abs/2509.18316)
*Saksham Khatwani,He Cheng,Majid Afshar,Dmitriy Dligach,Yanjun Gao*

Main category: cs.CL

TL;DR: This paper explores using LLMs as reward models to judge knowledge graph reasoning paths for medical diagnosis, rather than traditional retrieval or fine-tuning approaches. The study evaluates different task formulations and training paradigms, finding that while path-judging performance improves with specific optimization, transferability to downstream tasks remains limited.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise for diagnostic reasoning but lack reliable knowledge-grounded inference. Knowledge graphs offer structured biomedical knowledge, but current approaches insert KG content into prompts rather than enabling structured reasoning. The paper aims to leverage the insight that verifying solutions is often easier than generating them from scratch.

Method: The approach treats LLMs as reward models for KG reasoning paths, where models learn to judge whether candidate paths lead to correct diagnoses. The study systematically evaluates five task formulations for knowledge path judging and eight training paradigms, testing three open-source instruct-tuned LLMs.

Result: Experiments reveal both promise and brittleness: specific reward optimization and distillation lead to strong path-judging performance, but transferability to downstream diagnostic tasks (summarization and medical QA) remains weak.

Conclusion: This provides the first systematic assessment of 'reward model style' reasoning over clinical KGs, offering insights into how structured, reward-based supervision influences diagnostic reasoning in GenAI healthcare systems.

Abstract: Large language models (LLMs) show promise for diagnostic reasoning but often
lack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as
the Unified Medical Language System (UMLS), offer structured biomedical
knowledge that can support trustworthy reasoning. Prior approaches typically
integrate KGs via retrieval augmented generation or fine tuning, inserting KG
content into prompts rather than enabling structured reasoning. We explore an
alternative paradigm: treating the LLM as a reward model of KG reasoning paths,
where the model learns to judge whether a candidate path leads to correct
diagnosis for a given patient input. This approach is inspired by recent work
that leverages reward training to enhance model reasoning abilities, and
grounded in computational theory, which suggests that verifying a solution is
often easier than generating one from scratch. It also parallels physicians'
diagnostic assessment, where they judge which sequences of findings and
intermediate conditions most plausibly support a diagnosis. We first
systematically evaluate five task formulation for knowledge path judging and
eight training paradigm. Second, we test whether the path judging abilities
generalize to downstream diagnostic tasks, including diagnosis summarization
and medical question answering. Experiments with three open source
instruct-tuned LLMs reveal both promise and brittleness: while specific reward
optimization and distillation lead to strong path-judging performance, the
transferability to downstream tasks remain weak. Our finding provides the first
systematic assessment of "reward model style" reasoning over clinical KGs,
offering insights into how structured, reward-based supervision influences
diagnostic reasoning in GenAI systems for healthcare.

</details>


### [134] [Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding](https://arxiv.org/abs/2509.18344)
*Pei-Shuo Wang,Jian-Jia Chen,Chun-Che Yang,Chi-Chih Chang,Ning-Chi Huang,Mohamed S. Abdelfattah,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: SubSpec is a plug-and-play method that accelerates parameter offloading for large language models by creating low-bit quantized substitute layers from offloaded portions, achieving significant speedups without quality loss or additional training.


<details>
  <summary>Details</summary>
Motivation: Large language models face deployment challenges on memory-limited GPUs. Existing compression methods degrade quality, while offloading maintains quality but suffers from slow inference. Current speculative decoding approaches require pretrained weights or additional training and yield only modest speedups due to insufficient alignment with target models.

Method: SubSpec constructs a highly aligned draft model by generating low-bit quantized substitute layers from offloaded target LLM portions. It shares the remaining GPU-resident layers and KV-Cache to reduce memory overhead and enhance alignment. The method is lossless and training-free.

Result: SubSpec achieves 9.1x speedup for Qwen2.5 7B on MT-Bench with 8GB VRAM limit, and an average of 12.5x speedup for Qwen2.5 32B on popular generation benchmarks with 24GB VRAM limit, with high average acceptance length.

Conclusion: SubSpec effectively addresses the limitations of existing parameter offloading methods by providing a training-free, lossless approach that achieves substantial speedups through better alignment between draft and target models, making LLM deployment more practical on memory-constrained hardware.

Abstract: The immense model sizes of large language models (LLMs) challenge deployment
on memory-limited consumer GPUs. Although model compression and parameter
offloading are common strategies to address memory limitations, compression can
degrade quality, and offloading maintains quality but suffers from slow
inference. Speculative decoding presents a promising avenue to accelerate
parameter offloading, utilizing a fast draft model to propose multiple draft
tokens, which are then verified by the target LLM in parallel with a single
forward pass. This method reduces the time-consuming data transfers in forward
passes that involve offloaded weight transfers. Existing methods often rely on
pretrained weights of the same family, but require additional training to align
with custom-trained models. Moreover, approaches that involve draft model
training usually yield only modest speedups. This limitation arises from
insufficient alignment with the target model, preventing higher token
acceptance lengths. To address these challenges and achieve greater speedups,
we propose SubSpec, a plug-and-play method to accelerate parameter offloading
that is lossless and training-free. SubSpec constructs a highly aligned draft
model by generating low-bit quantized substitute layers from offloaded target
LLM portions. Additionally, our method shares the remaining GPU-resident layers
and the KV-Cache, further reducing memory overhead and enhance alignment.
SubSpec achieves a high average acceptance length, delivering 9.1x speedup for
Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for
Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

</details>


### [135] [Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents](https://arxiv.org/abs/2509.18360)
*Chutong Meng,Philipp Koehn*

Main category: cs.CL

TL;DR: Speech Vecalign is a parallel speech document alignment method that creates longer, more robust speech-to-speech alignments without text transcripts, outperforming baseline methods and achieving competitive translation performance with significantly less data.


<details>
  <summary>Details</summary>
Motivation: To develop a more effective method for aligning parallel speech documents that doesn't rely on text transcriptions, produces longer alignments than existing methods, and is more robust against noise.

Method: Speech Vecalign monotonically aligns speech segment embeddings without using text transcripts. It was applied to 3,000 hours of unlabeled parallel English-German speech from VoxPopuli, producing about 1,000 hours of high-quality alignments.

Result: Speech Vecalign improved En-to-De and De-to-En translation performance over Global Mining by 0.37 and 0.18 ASR-BLEU respectively. The models matched or outperformed SpeechMatrix performance despite using 8 times fewer raw speech documents.

Conclusion: Speech Vecalign is an effective parallel speech alignment method that produces high-quality alignments and enables competitive speech-to-speech translation performance with significantly reduced data requirements compared to existing approaches.

Abstract: We present Speech Vecalign, a parallel speech document alignment method that
monotonically aligns speech segment embeddings and does not depend on text
transcriptions. Compared to the baseline method Global Mining, a variant of
speech mining, Speech Vecalign produces longer speech-to-speech alignments. It
also demonstrates greater robustness than Local Mining, another speech mining
variant, as it produces less noise. We applied Speech Vecalign to 3,000 hours
of unlabeled parallel English-German (En-De) speech documents from VoxPopuli,
yielding about 1,000 hours of high-quality alignments. We then trained En-De
speech-to-speech translation models on the aligned data. Speech Vecalign
improves the En-to-De and De-to-En performance over Global Mining by 0.37 and
0.18 ASR-BLEU, respectively. Moreover, our models match or outperform
SpeechMatrix model performance, despite using 8 times fewer raw speech
documents.

</details>


### [136] [Interactive Real-Time Speaker Diarization Correction with Human Feedback](https://arxiv.org/abs/2509.18377)
*Xinlu He,Yiwen Guan,Badrivishal Paurana,Zilin Dai,Jacob Whitehill*

Main category: cs.CL

TL;DR: An LLM-assisted speaker diarization correction system that enables real-time user feedback to fix speaker attribution errors, reducing diarization error rate by 9.92% and speaker confusion by 44.23%.


<details>
  <summary>Details</summary>
Motivation: Most speech processing systems operate in open-loop mode without user feedback, but human-in-the-loop workflows can potentially enable higher accuracy in speaker diarization.

Method: Streaming ASR and diarization pipeline with LLM-generated summaries, split-when-merged technique to detect multi-speaker segments, and online speaker enrollments based on user corrections.

Result: LLM-driven simulations on AMI test set show substantial reductions: 9.92% in diarization error rate and 44.23% in speaker confusion error.

Conclusion: The system effectively incorporates user feedback to improve speaker diarization accuracy, with analysis showing correction efficacy under different settings including summary vs full transcript display and enrollment limitations.

Abstract: Most automatic speech processing systems operate in "open loop" mode without
user feedback about who said what; yet, human-in-the-loop workflows can
potentially enable higher accuracy. We propose an LLM-assisted speaker
diarization correction system that lets users fix speaker attribution errors in
real time. The pipeline performs streaming ASR and diarization, uses an LLM to
deliver concise summaries to the users, and accepts brief verbal feedback that
is immediately incorporated without disrupting interactions. Moreover, we
develop techniques to make the workflow more effective: First, a
split-when-merged (SWM) technique detects and splits multi-speaker segments
that the ASR erroneously attributes to just a single speaker. Second, online
speaker enrollments are collected based on users' diarization corrections, thus
helping to prevent speaker diarization errors from occurring in the future.
LLM-driven simulations on the AMI test set indicate that our system
substantially reduces DER by 9.92% and speaker confusion error by 44.23%. We
further analyze correction efficacy under different settings, including summary
vs full transcript display, the number of online enrollments limitation, and
correction frequency.

</details>


### [137] [NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery](https://arxiv.org/abs/2509.18395)
*Minki Hong,Jangho Choi,Jihie Kim*

Main category: cs.CL

TL;DR: NormGenesis is a multicultural framework for generating socially grounded dialogues across English, Chinese, and Korean, featuring Violation-to-Resolution dialogues that model norm violation repair processes.


<details>
  <summary>Details</summary>
Motivation: To enable dialogue systems to produce responses that are not only coherent but also socially acceptable by modeling culturally appropriate behavior and social norms in communication.

Method: Proposes Violation-to-Resolution (V2R) dialogue type modeling norm violation progression, implements exemplar-based iterative refinement for pragmatic consistency, and constructs a dataset of 10,800 multi-turn dialogues with turn-level annotations for norm adherence, speaker intent, and emotional response.

Result: Human and LLM-based evaluations show NormGenesis significantly outperforms existing datasets in refinement quality, dialogue naturalness, and generalization performance. Models trained on V2R-augmented data exhibit improved pragmatic competence in ethically sensitive contexts.

Conclusion: Establishes a new benchmark for culturally adaptive dialogue modeling and provides a scalable methodology for norm-aware generation across linguistically and culturally diverse languages.

Abstract: Social norms govern culturally appropriate behavior in communication,
enabling dialogue systems to produce responses that are not only coherent but
also socially acceptable. We present NormGenesis, a multicultural framework for
generating and annotating socially grounded dialogues across English, Chinese,
and Korean. To model the dynamics of social interaction beyond static norm
classification, we propose a novel dialogue type, Violation-to-Resolution
(V2R), which models the progression of conversations following norm violations
through recognition and socially appropriate repair. To improve pragmatic
consistency in underrepresented languages, we implement an exemplar-based
iterative refinement early in the dialogue synthesis process. This design
introduces alignment with linguistic, emotional, and sociocultural expectations
before full dialogue generation begins. Using this framework, we construct a
dataset of 10,800 multi-turn dialogues annotated at the turn level for norm
adherence, speaker intent, and emotional response. Human and LLM-based
evaluations demonstrate that NormGenesis significantly outperforms existing
datasets in refinement quality, dialogue naturalness, and generalization
performance. We show that models trained on our V2R-augmented data exhibit
improved pragmatic competence in ethically sensitive contexts. Our work
establishes a new benchmark for culturally adaptive dialogue modeling and
provides a scalable methodology for norm-aware generation across linguistically
and culturally diverse languages.

</details>


### [138] [Evaluating the Creativity of LLMs in Persian Literary Text Generation](https://arxiv.org/abs/2509.18401)
*Armin Tourajmehr,Mohammad Reza Modarres,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: This paper evaluates LLMs' ability to generate culturally relevant Persian literary text using adapted Torrance Tests of Creative Thinking dimensions and analyzes their use of literary devices.


<details>
  <summary>Details</summary>
Motivation: Prior research has focused primarily on English literary generation with limited exploration of non-English traditions and standardized creativity assessment methods.

Method: Built a dataset of Persian literary texts across 20 topics, adapted Torrance Tests of Creative Thinking to assess four creativity dimensions (originality, fluency, flexibility, elaboration), used LLM as automated judge validated against human judgments, and analyzed use of four literary devices (simile, metaphor, hyperbole, antithesis).

Result: Strong agreement between LLM judge and human judgments via intraclass correlation coefficients, highlighting both strengths and limitations of LLMs in Persian literary generation.

Conclusion: LLMs show promise but require further refinement for Persian literary text generation, underscoring the need for continued development in non-English literary traditions.

Abstract: Large language models (LLMs) have demonstrated notable creative abilities in
generating literary texts, including poetry and short stories. However, prior
research has primarily centered on English, with limited exploration of
non-English literary traditions and without standardized methods for assessing
creativity. In this paper, we evaluate the capacity of LLMs to generate Persian
literary text enriched with culturally relevant expressions. We build a dataset
of user-generated Persian literary spanning 20 diverse topics and assess model
outputs along four creativity dimensions-originality, fluency, flexibility, and
elaboration-by adapting the Torrance Tests of Creative Thinking. To reduce
evaluation costs, we adopt an LLM as a judge for automated scoring and validate
its reliability against human judgments using intraclass correlation
coefficients, observing strong agreement. In addition, we analyze the models'
ability to understand and employ four core literary devices: simile, metaphor,
hyperbole, and antithesis. Our results highlight both the strengths and
limitations of LLMs in Persian literary text generation, underscoring the need
for further refinement.

</details>


### [139] [Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations](https://arxiv.org/abs/2509.18439)
*Oscar J. Ponce-Ponte,David Toro-Tobon,Luis F. Figueroa,Michael Gionfriddo,Megan Branda,Victor M. Montori,Saturnino Luz,Juan P. Brito*

Main category: cs.CL

TL;DR: This study developed an automated method to measure Shared Decision-Making (SDM) in patient-doctor conversations using language modeling and conversational alignment scores, achieving significant associations with established SDM outcome measures.


<details>
  <summary>Details</summary>
Motivation: Shared decision-making is essential for patient-centered care, but no scalable automated methodology exists to measure it. Current manual assessment methods are resource-intensive and not scalable for widespread evaluation.

Method: Used 157 video-recorded patient-doctor conversations (42,559 sentences) from an atrial fibrillation trial. Employed deep learning models and fine-tuned BERT models via next sentence prediction task with context-response pairs and negative sampling. Calculated four types of conversational alignment scores from top-performing models.

Result: Fine-tuned BERTbase achieved highest recall@1 (0.640). Conversational alignment scores from both DL and BERT models showed significant associations with SDM outcomes (OPTION12 and Decisional Conflict Scale). BERT model size didn't affect associations.

Conclusion: The study successfully introduces an automated, scalable methodology to measure SDM using explainable conversational alignment scores, with potential for large-scale evaluation of SDM strategies.

Abstract: Shared decision-making (SDM) is necessary to achieve patient-centred care.
Currently no methodology exists to automatically measure SDM at scale. This
study aimed to develop an automated approach to measure SDM by using language
modelling and the conversational alignment (CA) score. A total of 157
video-recorded patient-doctor conversations from a randomized multi-centre
trial evaluating SDM decision aids for anticoagulation in atrial fibrillations
were transcribed and segmented into 42,559 sentences. Context-response pairs
and negative sampling were employed to train deep learning (DL) models and
fine-tuned BERT models via the next sentence prediction (NSP) task. Each
top-performing model was used to calculate four types of CA scores. A
random-effects analysis by clinician, adjusting for age, sex, race, and trial
arm, assessed the association between CA scores and SDM outcomes: the
Decisional Conflict Scale (DCS) and the Observing Patient Involvement in
Decision-Making 12 (OPTION12) scores. p-values were corrected for multiple
comparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,
mean age 70 SD 10.8), clinicians on average spoke more words than patients
(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1
of 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1
with 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)
scores generated with the DL without stylebook were associated with OPTION12.
The Max CA score generated with the fine-tuned BERTbase (110M) was associated
with the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an
impact the association between CA scores and SDM. This study introduces an
automated, scalable methodology to measure SDM in patient-doctor conversations
through explainable CA scores, with potential to evaluate SDM strategies at
scale.

</details>


### [140] [CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density](https://arxiv.org/abs/2509.18458)
*Daniel Kaiser,Arnoldo Frigessi,Ali Ramezani-Kebrya,Benjamin Ricaud*

Main category: cs.CL

TL;DR: CogniLoad is a synthetic benchmark based on Cognitive Load Theory that generates logic puzzles with tunable parameters to precisely analyze LLM reasoning limitations by controlling intrinsic difficulty, distractor interference, and task length.


<details>
  <summary>Details</summary>
Motivation: Current long-context reasoning benchmarks fail to distinguish between critical factors like task complexity, distractor interference, and length, making precise failure analysis difficult.

Method: CogniLoad generates natural-language logic puzzles with independently tunable parameters: intrinsic difficulty (d) for intrinsic load, distractor-to-signal ratio (ρ) for extraneous load, and task length (N) as proxy for germane load demands.

Result: Evaluation of 22 state-of-the-art reasoning LLMs revealed distinct performance sensitivities, with task length being a dominant constraint, varied tolerances to intrinsic complexity, and U-shaped responses to distractor ratios.

Conclusion: CogniLoad provides systematic, factorial control over cognitive load dimensions, offering a reproducible, scalable, and diagnostically rich tool for dissecting LLM reasoning limitations and guiding future model development.

Abstract: Current benchmarks for long-context reasoning in Large Language Models (LLMs)
often blur critical factors like intrinsic task complexity, distractor
interference, and task length. To enable more precise failure analysis, we
introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load
Theory (CLT). CogniLoad generates natural-language logic puzzles with
independently tunable parameters that reflect CLT's core dimensions: intrinsic
difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\rho$)
regulates extraneous load; and task length ($N$) serves as an operational proxy
for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,
CogniLoad reveals distinct performance sensitivities, identifying task length
as a dominant constraint and uncovering varied tolerances to intrinsic
complexity and U-shaped responses to distractor ratios. By offering systematic,
factorial control over these cognitive load dimensions, CogniLoad provides a
reproducible, scalable, and diagnostically rich tool for dissecting LLM
reasoning limitations and guiding future model development.

</details>


### [141] [LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling](https://arxiv.org/abs/2509.18467)
*Zeyu Liu,Souvik Kundu,Lianghao Jiang,Anni Li,Srikanth Ronanki,Sravan Bodapati,Gourav Datta,Peter A. Beerel*

Main category: cs.CL

TL;DR: LAWCAT is a linear attention framework that efficiently transfers pre-trained transformer capabilities to linear-complexity models using causal Conv1D layers and normalized gated linear attention, achieving strong long-context performance with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Transformers have quadratic computational complexity that limits their use in latency-sensitive long-context applications, while training linear-complexity alternatives from scratch is resource-intensive.

Method: LAWCAT integrates causal Conv1D layers for local dependency modeling and employs normalized gated linear attention to improve generalization across context lengths, enabling efficient knowledge distillation from pre-trained transformers.

Result: Distilling Mistral-7B with only 1K-length sequences achieves over 90% passkey retrieval accuracy up to 22K tokens. Llama3.2-1B LAWCAT variant shows competitive performance on long-context benchmarks while requiring less than 0.1% pre-training tokens compared to full pre-training.

Conclusion: LAWCAT provides an efficient pathway to high-performance linear models suitable for edge deployment, reducing reliance on extensive long-sequence training data and computational resources while offering faster prefill speeds than FlashAttention-2 for long sequences.

Abstract: Although transformer architectures have achieved state-of-the-art performance
across diverse domains, their quadratic computational complexity with respect
to sequence length remains a significant bottleneck, particularly for
latency-sensitive long-context applications. While recent linear-complexity
alternatives are increasingly powerful, effectively training them from scratch
is still resource-intensive. To overcome these limitations, we propose LAWCAT
(Linear Attention with Convolution Across Time), a novel linearization
framework designed to efficiently transfer the capabilities of pre-trained
transformers into a performant linear attention architecture. LAWCAT integrates
causal Conv1D layers to enhance local dependency modeling and employs
normalized gated linear attention to improve generalization across varying
context lengths. Our comprehensive evaluations demonstrate that, distilling
Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval
accuracy up to 22K tokens, significantly extending its effective context
window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance
on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark
(QA2\&QA3, 0K-16K context length), requiring less than 0.1\% pre-training
tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster
prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT
thus provides an efficient pathway to high-performance, long-context linear
models suitable for edge deployment, reducing reliance on extensive
long-sequence training data and computational resources.

</details>


### [142] [Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference](https://arxiv.org/abs/2509.18487)
*Ben Finkelshtein,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen White*

Main category: cs.CL

TL;DR: A systematic evaluation of LLM-based graph reasoning methods across multiple axes including interaction modes, dataset domains, structural regimes, and feature characteristics, revealing that code generation achieves the strongest performance and remains effective on heterophilic graphs.


<details>
  <summary>Details</summary>
Motivation: To provide a principled understanding of LLM capabilities in graph machine learning tasks, as current approaches lack systematic evaluation despite increasing use in domains like fraud detection and recommendation systems.

Method: Conducted large-scale controlled evaluation across key axes: LLM-graph interaction modes (prompting, tool-use, code generation), dataset domains (citation, web-link, e-commerce, social networks), structural regimes (homophilic vs heterophilic graphs), feature characteristics (short- vs long-text attributes), and model configurations. Analyzed dependencies through methodical truncation of features, edge deletion, and label removal.

Result: (1) LLMs as code generators achieve strongest overall performance, especially on long-text or high-degree graphs where prompting exceeds token limits. (2) All interaction strategies remain effective on heterophilic graphs, challenging assumptions about LLM collapse under low homophily. (3) Code generation adapts reliance between structure, features, and labels to leverage most informative input type.

Conclusion: The findings provide comprehensive understanding of LLM-graph interaction strengths/limitations and highlight key design principles for future approaches, offering practical guidance for text-rich graph machine learning applications.

Abstract: Large language models (LLMs) are increasingly used for text-rich graph
machine learning tasks such as node classification in high-impact domains like
fraud detection and recommendation systems. Yet, despite a surge of interest,
the field lacks a principled understanding of the capabilities of LLMs in their
interaction with graph data. In this work, we conduct a large-scale, controlled
evaluation across several key axes of variability to systematically assess the
strengths and weaknesses of LLM-based graph reasoning methods in text-based
applications. The axes include the LLM-graph interaction mode, comparing
prompting, tool-use, and code generation; dataset domains, spanning citation,
web-link, e-commerce, and social networks; structural regimes contrasting
homophilic and heterophilic graphs; feature characteristics involving both
short- and long-text node attributes; and model configurations with varying LLM
sizes and reasoning capabilities. We further analyze dependencies by
methodically truncating features, deleting edges, and removing labels to
quantify reliance on input types. Our findings provide practical and actionable
guidance. (1) LLMs as code generators achieve the strongest overall performance
on graph data, with especially large gains on long-text or high-degree graphs
where prompting quickly exceeds the token budget. (2) All interaction
strategies remain effective on heterophilic graphs, challenging the assumption
that LLM-based methods collapse under low homophily. (3) Code generation is
able to flexibly adapt its reliance between structure, features, or labels to
leverage the most informative input type. Together, these findings provide a
comprehensive view of the strengths and limitations of current LLM-graph
interaction modes and highlight key design principles for future approaches.

</details>


### [143] [A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition](https://arxiv.org/abs/2509.18514)
*Mohamad Elzohbi,Richard Zhao*

Main category: cs.CL

TL;DR: A methodology for inserting phrases in Arabic poems to match specific rhythms using ByT5 model with rule-based grapheme-to-beat transformation and conditional denoising.


<details>
  <summary>Details</summary>
Motivation: To develop a system that can automatically insert phrases into Arabic poems while maintaining both rhythmic alignment and semantic coherence, enabling co-creative applications in classical Arabic poetry composition.

Method: Uses ByT5 transformer model with rule-based grapheme-to-beat transformation for rhythm extraction, conditional denoising objective for fine-tuning, curriculum learning (pre-training on general Arabic data then poetic data), and explores cross-lingual transfer from English to Arabic.

Result: Experimental results show the models achieve high rhythmic alignment while maintaining semantic coherence in the generated poetic phrases.

Conclusion: The proposed model successfully enables rhythmic phrase insertion in Arabic poetry and has potential for co-creative applications in classical Arabic poem composition.

Abstract: This paper presents a methodology for inserting phrases in Arabic poems to
conform to a specific rhythm using ByT5, a byte-level multilingual
transformer-based model. Our work discusses a rule-based grapheme-to-beat
transformation tailored for extracting the rhythm from fully diacritized Arabic
script. Our approach employs a conditional denoising objective to fine-tune
ByT5, where the model reconstructs masked words to match a target rhythm. We
adopt a curriculum learning strategy, pre-training on a general Arabic dataset
before fine-tuning on poetic dataset, and explore cross-lingual transfer from
English to Arabic. Experimental results demonstrate that our models achieve
high rhythmic alignment while maintaining semantic coherence. The proposed
model has the potential to be used in co-creative applications in the process
of composing classical Arabic poems.

</details>


### [144] [Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector](https://arxiv.org/abs/2509.18535)
*Mo Mu,Dianqiao Lei,Chang Li*

Main category: cs.CL

TL;DR: A lightweight framework for detecting AI-generated text that focuses on structural features rather than word-level patterns, making it robust against paraphrasing and other modifications.


<details>
  <summary>Details</summary>
Motivation: Current AI text detectors are vulnerable to paraphrasing, suffer from biases in ChatGPT's word patterns and training data, degrade on modified text, and often require large models or online LLM interaction.

Method: Encodes sentence embeddings from pre-trained language models and models their relationships via attention. Uses contrastive learning to mitigate embedding biases and incorporates a causal graph with counterfactual methods to isolate structural features from topic biases.

Result: Experiments on two curated datasets (abstract comparisons and revised life FAQs) validate the effectiveness of the method in detecting both original and PSP-modified AI-generated texts.

Conclusion: The proposed framework successfully detects AI-generated text by focusing on invariant structural features that remain consistent under word-level changes, providing a robust solution to current detection limitations.

Abstract: The widespread adoption of ChatGPT has raised concerns about its misuse,
highlighting the need for robust detection of AI-generated text. Current
word-level detectors are vulnerable to paraphrasing or simple prompts (PSP),
suffer from biases induced by ChatGPT's word-level patterns (CWP) and training
data content, degrade on modified text, and often require large models or
online LLM interaction. To tackle these issues, we introduce a novel task to
detect both original and PSP-modified AI-generated texts, and propose a
lightweight framework that classifies texts based on their internal structure,
which remains invariant under word-level changes. Our approach encodes sentence
embeddings from pre-trained language models and models their relationships via
attention. We employ contrastive learning to mitigate embedding biases from
autoregressive generation and incorporate a causal graph with counterfactual
methods to isolate structural features from topic-related biases. Experiments
on two curated datasets, including abstract comparisons and revised life FAQs,
validate the effectiveness of our method.

</details>


### [145] [CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs](https://arxiv.org/abs/2509.18536)
*Jin Young Kim,Ji Won Yoon*

Main category: cs.CL

TL;DR: CCQA is a novel reasoning method that uses cycle consistency to improve question answering performance in small language models (SLMs) by generating questions from reasoning paths and selecting answers based on similarity to original questions.


<details>
  <summary>Details</summary>
Motivation: Existing inference-time reasoning strategies work well for large language models but often fail to improve performance in smaller models, creating a need for effective reasoning methods tailored for SLMs.

Method: CCQA generates a question from each reasoning path and answer, evaluates similarity to the original question using cycle consistency, and selects the candidate with highest similarity. It uses a lightweight Flan-T5 model for question generation since SLMs struggle with this task.

Result: CCQA consistently outperforms existing state-of-the-art methods across eight models on mathematical and commonsense reasoning benchmarks.

Conclusion: The method establishes a new practical baseline for efficient reasoning in SLMs and demonstrates that cycle consistency can effectively improve reasoning performance in smaller language models.

Abstract: Recently, inference-time reasoning strategies have further improved the
accuracy of large language models (LLMs), but their effectiveness on smaller
models remains unclear. Based on the observation that conventional approaches
often fail to improve performance in this context, we propose
\textbf{C}ycle-\textbf{C}onsistency in \textbf{Q}uestion \textbf{A}nswering
(CCQA), a novel reasoning method that can be effectively applied to SLMs.
Inspired by cycle consistency, CCQA generates a question from each reasoning
path and answer, evaluates each by its similarity to the original question, and
then selects the candidate solution with the highest similarity score as the
final response. Since conventional SLMs struggle to generate accurate questions
from their own reasoning paths and answers, we employ a lightweight Flan-T5
model specialized for question generation to support this process efficiently.
From the experimental results, it is verified that CCQA consistently
outperforms existing state-of-the-art (SOTA) methods across eight models on
mathematical and commonsense reasoning benchmarks. Furthermore, our method
establishes a new practical baseline for efficient reasoning in SLMs. Source
code can be found at https://github.com/scai-research/ccqa_official.

</details>


### [146] [Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity](https://arxiv.org/abs/2509.18577)
*Yeongbin Seo,Gayoung Kim,Jaehyung Kim,Jinyoung Yeo*

Main category: cs.CL

TL;DR: A prior-based data filtering method using corpus-level term frequency statistics as a fast alternative to perplexity-based filtering for LLM pretraining.


<details>
  <summary>Details</summary>
Motivation: Perplexity-based filtering is time-consuming and unreliable with noisy/out-of-distribution data, requiring a more efficient and effective data selection approach.

Method: Estimates token priors using term frequency statistics, filters documents based on mean and standard deviation of token priors without requiring model inference.

Result: Achieves highest average performance across 20 downstream benchmarks while reducing time cost by over 1000x compared to PPL-based filtering.

Conclusion: The prior-based filter is simple yet powerful, applicable to symbolic languages and multilingual corpora, providing an efficient alternative to perplexity-based methods.

Abstract: As large language models (LLMs) are pretrained on massive web corpora,
careful selection of data becomes essential to ensure effective and efficient
learning. While perplexity (PPL)-based filtering has shown strong performance,
it suffers from drawbacks: substantial time costs and inherent unreliability of
the model when handling noisy or out-of-distribution samples. In this work, we
propose a simple yet powerful alternative: a prior-based data filtering method
that estimates token priors using corpus-level term frequency statistics,
inspired by linguistic insights on word roles and lexical density. Our approach
filters documents based on the mean and standard deviation of token priors,
serving as a fast proxy to PPL while requiring no model inference. Despite its
simplicity, the prior-based filter achieves the highest average performance
across 20 downstream benchmarks, while reducing time cost by over 1000x
compared to PPL-based filtering. We further demonstrate its applicability to
symbolic languages such as code and math, and its dynamic adaptability to
multilingual corpora without supervision

</details>


### [147] [TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning](https://arxiv.org/abs/2509.18585)
*Yu Chen,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: TsqLoRA is a parameter-efficient fine-tuning method that combines data-quality-driven selection with sensitivity-aware low-rank adaptation to improve efficiency while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Fully fine-tuning large models is computationally expensive, and existing parameter-efficient methods overlook layer sensitivity variations and training data importance.

Method: Integrates quality-aware sampling for selecting informative training data and dynamic rank allocation that adjusts each layer's rank based on sensitivity to parameter updates.

Result: Experimental results show TsqLoRA improves fine-tuning efficiency while maintaining or improving performance on various NLP tasks.

Conclusion: TsqLoRA provides an effective approach for efficient fine-tuning by addressing both data quality and layer sensitivity considerations.

Abstract: Fine-tuning large pre-trained models for downstream tasks has become a
fundamental approach in natural language processing. Fully fine-tuning all
model parameters is computationally expensive and memory-intensive, especially
in resource-constrained environments. Existing parameter-efficient fine-tuning
methods reduce the number of trainable parameters but typically overlook the
varying sensitivity of different model layers and the importance of training
data. In this work, we propose TsqLoRA, a novel method that integrates
data-quality-driven selection with sensitivity-aware low-rank adaptation,
consisted of two main components: a quality-aware sampling mechanism for
selecting the most informative training data, and a dynamic rank allocation
module that adjusts the rank of each layer based on its sensitivity to
parameter updates. The experimental results demonstrate that TsqLoRA improves
fine-tuning efficiency while maintaining or even improving performance on a
variety of NLP tasks. Our code will be available at
https://github.com/Benjamin-Ricky/TsqLoRA.

</details>


### [148] [UniECG: Understanding and Generating ECG in One Unified Model](https://arxiv.org/abs/2509.18588)
*Jiarui Jin,Haoyu Wang,Xiang Lan,Jun Li,Gaofeng Cheng,Hongyan Li,Shenda Hong*

Main category: cs.CL

TL;DR: UniECG is the first unified model that can both interpret ECG signals for medical diagnosis and generate ECG signals from text descriptions, addressing limitations of current vision-language models in handling ECG data.


<details>
  <summary>Details</summary>
Motivation: Current unified models like GPT-5 fail to correctly understand ECG signals for medical diagnosis and cannot generate ECG signals, creating a gap in ECG analysis capabilities.

Method: A decoupled two-stage training approach: first learning evidence-based ECG interpretation (ECG-to-Text), then injecting ECG generation capabilities (Text-to-ECG) through latent space alignment.

Result: UniECG can autonomously choose to interpret or generate ECG based on user input, significantly extending the capability boundaries of current ECG models.

Conclusion: The proposed UniECG model successfully bridges the gap in ECG analysis by providing both interpretation and generation capabilities in a unified framework, with code and checkpoints to be made publicly available.

Abstract: Recent unified models such as GPT-5 have achieved encouraging progress on
vision-language tasks. However, these unified models typically fail to
correctly understand ECG signals and provide accurate medical diagnoses, nor
can they correctly generate ECG signals. To address these limitations, we
propose UniECG, the first unified model for ECG capable of concurrently
performing evidence-based ECG interpretation and text-conditioned ECG
generation tasks. Through a decoupled two-stage training approach, the model
first learns evidence-based interpretation skills (ECG-to-Text), and then
injects ECG generation capabilities (Text-to-ECG) via latent space alignment.
UniECG can autonomously choose to interpret or generate an ECG based on user
input, significantly extending the capability boundaries of current ECG models.
Our code and checkpoints will be made publicly available at
https://github.com/PKUDigitalHealth/UniECG upon acceptance.

</details>


### [149] [A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users](https://arxiv.org/abs/2509.18632)
*Nishant Balepur,Matthew Shu,Yoo Yeon Sung,Seraphina Goldfarb-Tarrant,Shi Feng,Fumeng Yang,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: Planorama study reveals that user preferences, model preferences, and agent success metrics do not accurately predict which LLM-generated plans actually help users complete tasks, highlighting a misalignment between common alignment feedback and true helpfulness.


<details>
  <summary>Details</summary>
Motivation: To test whether current LLM alignment methods (like RLHF and ChatbotArena) that rely on user preferences actually reflect what helps users accomplish complex tasks through generated plans.

Method: Developed Planorama interface where 126 users answered 300 multi-step questions using LLM plans, collecting 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA success) and user preferences, then replicated the setup with agents and reward models.

Result: Found that preferences and agent success do not predict actual user success with plans; users perform similarly with preferred and dispreferred plans; surface-level cues like brevity strongly influence preferences but don't correlate with helpfulness.

Conclusion: Aligning helpful LLMs requires feedback from real user interactions rather than just preferences about what appears helpful, calling for new approaches in NLP research.

Abstract: To assist users in complex tasks, LLMs generate plans: step-by-step
instructions towards a goal. While alignment methods aim to ensure LLM plans
are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,
assuming this reflects what helps them. We test this with Planorama: an
interface where 126 users answer 300 multi-step questions with LLM plans. We
get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA
success) and user preferences on plans, and recreate the setup in agents and
reward models to see if they simulate or prefer what helps users. We expose: 1)
user/model preferences and agent success do not accurately predict which plans
help users, so common alignment feedback can misalign with helpfulness; 2) this
gap is not due to user-specific preferences, as users are similarly successful
when using plans they prefer/disprefer; 3) surface-level cues like brevity and
question similarity strongly link to preferences, but such biases fail to
predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from
real user interactions, not just preferences of what looks helpful, so we
discuss the plan NLP researchers can execute to solve this problem.

</details>


### [150] [Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2509.18655)
*Lingwen Deng,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: CAPE-KG is a consistency-aware framework for parameter-preserving knowledge editing that addresses inconsistencies in multi-hop question answering by ensuring knowledge graph construction, updates, and retrieval remain aligned with editing requirements.


<details>
  <summary>Details</summary>
Motivation: Existing PPKE approaches based on knowledge graphs for multi-hop QA suffer from consistency issues leading to knowledge contamination, unstable updates, and unreliable retrieval behaviors that undermine reasoning reliability.

Method: CAPE-KG ensures KG construction, update, and retrieval are always aligned with MHQA task requirements, maintaining coherent reasoning over both unedited and edited knowledge through a consistency-aware framework.

Result: Extensive experiments on MQuAKE benchmark show accuracy improvements in PPKE performance for MHQA, demonstrating the effectiveness of addressing consistency issues.

Conclusion: CAPE-KG successfully improves PPKE reliability in multi-hop reasoning by maintaining consistency throughout the knowledge editing process, making parameter-preserving knowledge editing more effective for complex QA tasks.

Abstract: Parameter-Preserving Knowledge Editing (PPKE) enables updating models with
new or corrected information without retraining or parameter adjustment. Recent
PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)
capabilities to multi-hop question answering (MHQA). However, these methods
often lack consistency, leading to knowledge contamination, unstable updates,
and retrieval behaviors that fail to reflect the intended edits. Such
inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We
present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge
Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures
KG construction, update, and retrieval are always aligned with the requirements
of the MHQA task, maintaining coherent reasoning over both unedited and edited
knowledge. Extensive experiments on the MQuAKE benchmark show accuracy
improvements in PPKE performance for MHQA, demonstrating the effectiveness of
addressing consistency in PPKE.

</details>


### [151] [Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction](https://arxiv.org/abs/2509.18658)
*Huanxin Sheng,Xinyi Liu,Hangfeng He,Jieyu Zhao,Jian Kang*

Main category: cs.CL

TL;DR: This paper presents a framework using conformal prediction to analyze uncertainty in LLM-as-a-judge evaluations by providing prediction intervals for LLM-based scoring, with applications to natural language generation assessment.


<details>
  <summary>Details</summary>
Motivation: The uncertainty of LLM-as-a-judge evaluations remains underexplored, which limits its reliability and deployment in applications. Current methods lack proper uncertainty quantification for LLM-based scoring.

Method: The framework uses conformal prediction to construct continuous prediction intervals from a single evaluation run, with ordinal boundary adjustment for discrete rating tasks. It also proposes a midpoint-based score as a low-bias alternative to raw model scores and weighted averages.

Result: Extensive experiments show that conformal prediction provides valid prediction intervals with coverage guarantees. The interval midpoint and judge reprompting are explored as useful techniques for better judgment.

Conclusion: The proposed framework successfully addresses uncertainty in LLM-as-a-judge evaluations through conformal prediction, offering reliable prediction intervals and improved scoring methods for more trustworthy natural language generation assessment.

Abstract: LLM-as-a-judge has become a promising paradigm for using large language
models (LLMs) to evaluate natural language generation (NLG), but the
uncertainty of its evaluation remains underexplored. This lack of reliability
may limit its deployment in many applications. This work presents the first
framework to analyze the uncertainty by offering a prediction interval of
LLM-based scoring via conformal prediction. Conformal prediction constructs
continuous prediction intervals from a single evaluation run, and we design an
ordinal boundary adjustment for discrete rating tasks. We also suggest a
midpoint-based score within the interval as a low-bias alternative to raw model
score and weighted average. We perform extensive experiments and analysis,
which show that conformal prediction can provide valid prediction interval with
coverage guarantees. We also explore the usefulness of interval midpoint and
judge reprompting for better judgment.

</details>


### [152] [MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service](https://arxiv.org/abs/2509.18713)
*Yizhe Huang,Yang Liu,Ruiyu Zhao,Xiaolong Zhong,Xingming Yue,Ling Jiang*

Main category: cs.CL

TL;DR: MemOrb is a plug-and-play memory layer that improves LLM-based agents' reliability in customer service by distilling interactions into strategy reflections stored in shared memory.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents in customer service often forget across sessions, repeat errors, and lack mechanisms for continual self-improvement, making them unreliable in dynamic settings where stability and consistency are critical.

Method: Propose MemOrb, a lightweight verbal reinforcement memory layer that distills multi-turn interactions into compact strategy reflections stored in a shared memory bank, retrieved to guide decision-making without requiring fine-tuning.

Result: MemOrb significantly improves both success rate and stability, achieving up to 63 percentage-point gain in multi-turn success rate and delivering more consistent performance across repeated trials.

Conclusion: Structured reflection is a powerful mechanism for enhancing long-term reliability of frozen LLM agents in customer service scenarios.

Abstract: Large Language Model-based agents(LLM-based agents) are increasingly deployed
in customer service, yet they often forget across sessions, repeat errors, and
lack mechanisms for continual self-improvement. This makes them unreliable in
dynamic settings where stability and consistency are critical. To better
evaluate these properties, we emphasize two indicators: task success rate as a
measure of overall effectiveness, and consistency metrics such as Pass$^k$ to
capture reliability across multiple trials. To address the limitations of
existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal
reinforcement memory layer that distills multi-turn interactions into compact
strategy reflections. These reflections are stored in a shared memory bank and
retrieved to guide decision-making, without requiring any fine-tuning.
Experiments show that MemOrb significantly improves both success rate and
stability, achieving up to a 63 percentage-point gain in multi-turn success
rate and delivering more consistent performance across repeated trials. Our
results demonstrate that structured reflection is a powerful mechanism for
enhancing long-term reliability of frozen LLM agents in customer service
scenarios.

</details>


### [153] [LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR](https://arxiv.org/abs/2509.18722)
*Pattara Tipaksorn,Sumonmas Thatphithakkul,Vataya Chunwijitra,Kwanchiva Thangthai*

Main category: cs.CL

TL;DR: LOTUSDIS is a publicly available Thai meeting corpus for far-field conversational ASR, featuring 114 hours of spontaneous dialogue with overlapping speech recorded by multiple microphones at varying distances (0.12m to 10m).


<details>
  <summary>Details</summary>
Motivation: To address the mismatch between pre-training data and Thai far-field speech, and to provide distance-diverse training data for robust automatic speech recognition (ASR) systems.

Method: Collected 114 hours of spontaneous, unscripted dialogue from three participants using nine independent single-channel devices across six microphone types at different distances. Provided standard train/dev/test splits and reproducible baseline system with Whisper model variants tested under zero-shot and fine-tuned conditions.

Result: Fine-tuning on LOTUSDIS dramatically improved robustness: Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and far-field WER from 81.6 to 49.5, with largest gains on distant microphones. Off-the-shelf models showed strong degradation with distance.

Conclusion: Distance-diverse training data is crucial for robust ASR. The corpus is available under CC-BY-SA 4.0 with training/evaluation scripts to promote reproducible research in far-field conversational ASR.

Abstract: We present LOTUSDIS, a publicly available Thai meeting corpus designed to
advance far-field conversational ASR. The dataset comprises 114 hours of
spontaneous, unscripted dialogue collected in 15-20 minute sessions with three
participants, where overlapping speech is frequent and natural. Speech was
recorded simultaneously by nine independent single-channel devices spanning six
microphone types at distances from 0.12 m to 10 m, preserving the authentic
effects of reverberation, noise, and device coloration without relying on
microphone arrays. We provide standard train, dev, test splits and release a
reproducible baseline system. We benchmarked several Whisper variants under
zero-shot and fine-tuned conditions. Off-the-shelf models showed strong
degradation with distance, confirming a mismatch between pre-training data and
Thai far-field speech. Fine-tuning on LOTUSDIS dramatically improved
robustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and
far-field WER from 81.6 to 49.5, with especially large gains on the most
distant microphones. These results underscore the importance of
distance-diverse training data for robust ASR. The corpus is available under
CC-BY-SA 4.0. We also release training and evaluation scripts as a baseline
system to promote reproducible research in this field.

</details>


### [154] [Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models](https://arxiv.org/abs/2509.18742)
*Yunan Wang,Jianxin Li,Ziwei Zhang*

Main category: cs.CL

TL;DR: DyGRASP is a novel method that combines LLMs and temporal GNNs to efficiently handle Dynamic Text-Attribute Graphs (DyTAGs) by capturing both recent and global temporal semantics that existing methods overlook.


<details>
  <summary>Details</summary>
Motivation: Existing methods like GNNs and LLMs primarily focus on static TAGs and fail to capture the recent-global temporal semantics in DyTAGs, while also facing efficiency issues when applying LLMs to abundant evolving text data.

Method: DyGRASP uses: 1) node-centric implicit reasoning with sliding window for recent temporal semantics, 2) explicit reasoning with tailored prompts and RNN-like chain for global semantic dynamics, and 3) integration layers to merge recent/global semantics with graph structure.

Result: Extensive experiments show DyGRASP achieves up to 34% improvement in Hit@10 for destination node retrieval task and demonstrates strong generalization across different temporal GNNs and LLMs.

Conclusion: DyGRASP effectively addresses the limitations of existing methods by efficiently capturing both recent and global temporal semantics in DyTAGs, achieving superior performance and generalization capabilities.

Abstract: Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph
interactions and associated text attributes, are prevalent in real-world
applications. Existing methods, such as Graph Neural Networks (GNNs) and Large
Language Models (LLMs), mostly focus on static TAGs. Extending these existing
methods to DyTAGs is challenging as they largely neglect the recent-global
temporal semantics: the recent semantic dependencies among interaction texts
and the global semantic evolution of nodes over time. Furthermore, applying
LLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To
tackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic
Processing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to
efficiently and effectively reason on DyTAGs. Specifically, we first design a
node-centric implicit reasoning method together with a sliding window mechanism
to efficiently capture recent temporal semantics. In addition, to capture
global semantic dynamics of nodes, we leverage explicit reasoning with tailored
prompts and an RNN-like chain structure to infer long-term semantics. Lastly,
we intricately integrate the recent and global temporal semantics as well as
the dynamic graph structural information using updating and merging layers.
Extensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,
achieving up to 34% improvement in Hit@10 for destination node retrieval task.
Besides, DyGRASP exhibits strong generalization across different temporal GNNs
and LLMs.

</details>


### [155] [False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models](https://arxiv.org/abs/2509.18750)
*Julie Kallini,Dan Jurafsky,Christopher Potts,Martijn Bartelds*

Main category: cs.CL

TL;DR: Token overlap in multilingual subword tokenizers facilitates cross-lingual transfer rather than causing interference, with performance improving as vocabulary overlap increases.


<details>
  <summary>Details</summary>
Motivation: To determine whether overlapping tokens across languages in multilingual models facilitate cross-lingual transfer or introduce interference, addressing mixed evidence from prior work.

Method: Controlled experiments training bilingual autoregressive models on multiple language pairs with systematically varied vocabulary overlap, analyzing hidden representations and introducing semantic similarity of shared tokens as a new dimension.

Result: Models with token overlap outperform disjoint vocabulary models on XNLI and XQuAD, with transfer performance improving as overlap increases. Overlap creates embedding spaces that capture cross-lingual semantic relationships.

Conclusion: Substantial shared vocabulary remains a beneficial design choice for multilingual tokenizers, as token overlap facilitates cross-lingual transfer and improves performance.

Abstract: Subword tokenizers trained on multilingual corpora naturally produce
overlapping tokens across languages. Does token overlap facilitate
cross-lingual transfer or instead introduce interference between languages?
Prior work offers mixed evidence, partly due to varied setups and confounders,
such as token frequency or subword segmentation granularity. To address this
question, we devise a controlled experiment where we train bilingual
autoregressive models on multiple language pairs under systematically varied
vocabulary overlap settings. Crucially, we explore a new dimension to
understanding how overlap affects transfer: the semantic similarity of tokens
shared across languages. We first analyze our models' hidden representations
and find that overlap of any kind creates embedding spaces that capture
cross-lingual semantic relationships, while this effect is much weaker in
models with disjoint vocabularies. On XNLI and XQuAD, we find that models with
overlap outperform models with disjoint vocabularies, and that transfer
performance generally improves as overlap increases. Overall, our findings
highlight the advantages of token overlap in multilingual models and show that
substantial shared vocabulary remains a beneficial design choice for
multilingual tokenizers.

</details>


### [156] [When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models](https://arxiv.org/abs/2509.18762)
*Yingming Zheng,Hanqi Li,Kai Yu,Lu Chen*

Main category: cs.CL

TL;DR: Long-context SFT improves short-context performance in LLMs, contrary to long-context pretraining effects. Analysis reveals both MHA and FFN components benefit independently, with long-context SFT promoting contextual knowledge while short-context SFT favors parametric knowledge.


<details>
  <summary>Details</summary>
Motivation: To understand how SFT data length influences LLM behavior on short-context tasks, as the effects of data length in continued pretraining have been studied but implications for SFT remain unclear.

Method: Systematically investigated SFT data length effects, decoupled and analyzed Multi-Head Attention (MHA) and Feed-Forward Network (FFN) components, studied their interaction, and tested hybrid training approaches.

Result: Long-context SFT improves short-context performance, both MHA and FFN independently benefit from long-context SFT, and hybrid training mitigates knowledge preference bias.

Conclusion: Hybrid training offers explainable guidance for fine-tuning LLMs by balancing contextual and parametric knowledge preferences.

Abstract: Large language models (LLMs) have achieved impressive performance across
natural language processing (NLP) tasks. As real-world applications
increasingly demand longer context windows, continued pretraining and
supervised fine-tuning (SFT) on long-context data has become a common approach.
While the effects of data length in continued pretraining have been extensively
studied, their implications for SFT remain unclear. In this work, we
systematically investigate how SFT data length influences LLM behavior on
short-context tasks. Counterintuitively, we find that long-context SFT improves
short-context performance, contrary to the commonly observed degradation from
long-context pretraining. To uncover the underlying mechanisms of this
phenomenon, we first decouple and analyze two key components, Multi-Head
Attention (MHA) and Feed-Forward Network (FFN), and show that both
independently benefit from long-context SFT. We further study their interaction
and reveal a knowledge preference bias: long-context SFT promotes contextual
knowledge, while short-context SFT favors parametric knowledge, making
exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that
hybrid training mitigates this bias, offering explainable guidance for
fine-tuning LLMs.

</details>


### [157] [Financial Risk Relation Identification through Dual-view Adaptation](https://arxiv.org/abs/2509.18775)
*Wei-Ning Chiu,Yu-Hsiang Wang,Andy Hsiao,Yu-Shiang Huang,Chuan-Ju Wang*

Main category: cs.CL

TL;DR: A systematic method for extracting inter-firm risk relations using Form 10-K filings and NLP techniques, outperforming traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional risk assessment methods relying on expert judgment are subjective, labor-intensive, and difficult to scale, creating a need for automated, systematic approaches to identify inter-firm risk connections.

Method: Uses Form 10-K filings as data source, applies unsupervised fine-tuning based on chronological and lexical patterns using NLP to capture implicit risk connections, and develops a domain-specific financial encoder with quantitative risk relation scores.

Result: Extensive experiments show the method outperforms strong baselines across multiple evaluation settings.

Conclusion: The proposed approach provides a scalable, transparent, and interpretable solution for identifying inter-firm risk relations, addressing limitations of traditional manual analysis methods.

Abstract: A multitude of interconnected risk events -- ranging from regulatory changes
to geopolitical tensions -- can trigger ripple effects across firms.
Identifying inter-firm risk relations is thus crucial for applications like
portfolio management and investment strategy. Traditionally, such assessments
rely on expert judgment and manual analysis, which are, however, subjective,
labor-intensive, and difficult to scale. To address this, we propose a
systematic method for extracting inter-firm risk relations using Form 10-K
filings -- authoritative, standardized financial documents -- as our data
source. Leveraging recent advances in natural language processing, our approach
captures implicit and abstract risk connections through unsupervised
fine-tuning based on chronological and lexical patterns in the filings. This
enables the development of a domain-specific financial encoder with a deeper
contextual understanding and introduces a quantitative risk relation score for
transparency, interpretable analysis. Extensive experiments demonstrate that
our method outperforms strong baselines across multiple evaluation settings.

</details>


### [158] [AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field](https://arxiv.org/abs/2509.18776)
*Chen Liang,Zhaoqi Huang,Haofen Wang,Fu Chai,Chunying Yu,Huanhuan Wei,Zhengjie Liu,Yanpeng Li,Hongjun Wang,Ruifeng Luo,Xianzhong Zhao*

Main category: cs.CL

TL;DR: AECBench is a comprehensive benchmark to evaluate LLMs in the Architecture, Engineering, and Construction domain, revealing performance declines across cognitive levels despite proficiency in basic tasks.


<details>
  <summary>Details</summary>
Motivation: To assess the robustness and reliability of LLMs in the safety-critical AEC field, where their adoption is increasing but specialized domain performance remains unevaluated.

Method: Created AECBench with 23 tasks across 5 cognitive levels (Knowledge Memorization, Understanding, Reasoning, Calculation, Application), using a 4,800-question dataset from authentic AEC practice and implementing LLM-as-a-Judge evaluation with expert rubrics.

Result: Evaluation of 9 LLMs showed clear performance decline across cognitive levels - proficient in basic Knowledge Memorization and Understanding tasks but significant deficits in interpreting table knowledge, complex reasoning/calculation, and domain-specific document generation.

Conclusion: The study establishes groundwork for future development of robust LLM integration into safety-critical engineering practices, highlighting current limitations in specialized AEC domain capabilities.

Abstract: Large language models (LLMs), as a novel information technology, are seeing
increasing adoption in the Architecture, Engineering, and Construction (AEC)
field. They have shown their potential to streamline processes throughout the
building lifecycle. However, the robustness and reliability of LLMs in such a
specialized and safety-critical domain remain to be evaluated. To address this
challenge, this paper establishes AECBench, a comprehensive benchmark designed
to quantify the strengths and limitations of current LLMs in the AEC domain.
The benchmark defines 23 representative tasks within a five-level
cognition-oriented evaluation framework encompassing Knowledge Memorization,
Understanding, Reasoning, Calculation, and Application. These tasks were
derived from authentic AEC practice, with scope ranging from codes retrieval to
specialized documents generation. Subsequently, a 4,800-question dataset
encompassing diverse formats, including open-ended questions, was crafted
primarily by engineers and validated through a two-round expert review.
Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable
and consistent methodology for evaluating complex, long-form responses
leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear
performance decline across five cognitive levels was revealed. Despite
demonstrating proficiency in foundational tasks at the Knowledge Memorization
and Understanding levels, the models showed significant performance deficits,
particularly in interpreting knowledge from tables in building codes, executing
complex reasoning and calculation, and generating domain-specific documents.
Consequently, this study lays the groundwork for future research and
development aimed at the robust and reliable integration of LLMs into
safety-critical engineering practices.

</details>


### [159] [Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing](https://arxiv.org/abs/2509.18792)
*Sabri Boughorbel,Fahim Dalvi,Nadir Durrani,Majd Hawasly*

Main category: cs.CL

TL;DR: Model diffing analysis reveals that SimPO-enhanced Gemma-2-9b-it shows significant improvements in safety, multilingual capabilities, and instruction-following, while reducing self-reference and hallucination management.


<details>
  <summary>Details</summary>
Motivation: Traditional benchmarking fails to explain why one model outperforms another, so mechanistic interpretability through model diffing is needed to understand specific capability differences during fine-tuning.

Method: Using model diffing and crosscoders to analyze latent representations that differentiate between Gemma-2-9b-it and its SimPO-enhanced variant.

Result: SimPO acquired latent concepts enhance safety mechanisms (+32.8%), multilingual capabilities (+43.8%), and instruction-following (+151.7%), while reducing emphasis on model self-reference (-44.1%) and hallucination management (-68.5%).

Conclusion: Model diffing provides fine-grained insights beyond leaderboard metrics, offering a transparent framework for comparing LLMs by attributing performance gaps to concrete mechanistic capabilities.

Abstract: As fine-tuning becomes the dominant paradigm for improving large language
models (LLMs), understanding what changes during this process is increasingly
important. Traditional benchmarking often fails to explain why one model
outperforms another. In this work, we use model diffing, a mechanistic
interpretability approach, to analyze the specific capability differences
between Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we
identify and categorize latent representations that differentiate the two
models. We find that SimPO acquired latent concepts predominantly enhance
safety mechanisms (+32.8%), multilingual capabilities (+43.8%), and
instruction-following (+151.7%), while its additional training also reduces
emphasis on model self-reference (-44.1%) and hallucination management
(-68.5%). Our analysis shows that model diffing can yield fine-grained insights
beyond leaderboard metrics, attributing performance gaps to concrete
mechanistic capabilities. This approach offers a transparent and targeted
framework for comparing LLMs.

</details>


### [160] [MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction](https://arxiv.org/abs/2509.18813)
*Liting Zhang,Shiwan Zhao,Aobo Kong,Qicheng Li*

Main category: cs.CL

TL;DR: MAPEX is a multi-agent framework for keyphrase extraction that dynamically adapts to document length using dual-path strategy, outperforming state-of-the-art methods by 2.44% in F1@5.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised prompt-based methods use uniform prompting regardless of document length or LLM backbone, limiting exploitation of LLMs' reasoning capabilities for complex keyphrase extraction tasks.

Method: MAPEX coordinates LLM-based agents through expert recruitment, candidate extraction, topic guidance, knowledge augmentation, and post-processing modules. Uses dual-path strategy: knowledge-driven extraction for short texts and topic-guided extraction for long texts.

Result: Extensive experiments on six benchmark datasets across three LLMs show strong generalization, outperforming state-of-the-art unsupervised method by 2.44% and standard LLM baselines by 4.01% in F1@5 on average.

Conclusion: MAPEX demonstrates effective multi-agent collaboration for keyphrase extraction, providing a flexible framework that adapts to document complexity and LLM capabilities.

Abstract: Keyphrase extraction is a fundamental task in natural language processing.
However, existing unsupervised prompt-based methods for Large Language Models
(LLMs) often rely on single-stage inference pipelines with uniform prompting,
regardless of document length or LLM backbone. Such one-size-fits-all designs
hinder the full exploitation of LLMs' reasoning and generation capabilities,
especially given the complexity of keyphrase extraction across diverse
scenarios. To address these challenges, we propose MAPEX, the first framework
that introduces multi-agent collaboration into keyphrase extraction. MAPEX
coordinates LLM-based agents through modules for expert recruitment, candidate
extraction, topic guidance, knowledge augmentation, and post-processing. A
dual-path strategy dynamically adapts to document length: knowledge-driven
extraction for short texts and topic-guided extraction for long texts.
Extensive experiments on six benchmark datasets across three different LLMs
demonstrate its strong generalization and universality, outperforming the
state-of-the-art unsupervised method by 2.44\% and standard LLM baselines by
4.01\% in F1@5 on average. Code is available at
https://github.com/NKU-LITI/MAPEX.

</details>


### [161] [Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?](https://arxiv.org/abs/2509.18843)
*Damian Stachura,Joanna Konieczna,Artur Nowak*

Main category: cs.CL

TL;DR: Open-weight LLMs like DeepSeek-V3 are now comparable to proprietary models, and this study shows they can effectively replace larger closed-source models in biomedical question-answering tasks, sometimes even outperforming them with ensembling strategies.


<details>
  <summary>Details</summary>
Motivation: To determine if small open-weight LLMs can effectively replace larger closed-source models in biomedical question-answering, particularly in the context of the BioASQ challenge Task 13B Phase B.

Method: Compared open-weight models against top proprietary systems (GPT-4o, GPT-4.1, Claude 3.5/3.7 Sonnet) using techniques like embedding-based snippet retrieval, in-context learning, structured outputs, and ensemble approaches for exact-answer questions.

Result: Open-weight LLMs demonstrated comparable performance to proprietary models, and in some cases even surpassed closed counterparts when ensembling strategies were applied.

Conclusion: Open-weight LLMs are viable alternatives to proprietary models for biomedical question-answering, with ensembling approaches providing additional performance benefits.

Abstract: Open-weight versions of large language models (LLMs) are rapidly advancing,
with state-of-the-art models like DeepSeek-V3 now performing comparably to
proprietary LLMs. This progression raises the question of whether small
open-weight LLMs are capable of effectively replacing larger closed-source
models. We are particularly interested in the context of biomedical
question-answering, a domain we explored by participating in Task 13B Phase B
of the BioASQ challenge. In this work, we compare several open-weight models
against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and
Claude 3.7 Sonnet. To enhance question answering capabilities, we use various
techniques including retrieving the most relevant snippets based on embedding
distance, in-context learning, and structured outputs. For certain submissions,
we utilize ensemble approaches to leverage the diverse outputs generated by
different models for exact-answer questions. Our results demonstrate that
open-weight LLMs are comparable to proprietary ones. In some instances,
open-weight LLMs even surpassed their closed counterparts, particularly when
ensembling strategies were applied. All code is publicly available at
https://github.com/evidenceprime/BioASQ-13b.

</details>


### [162] [Multi-Hierarchical Feature Detection for Large Language Model Generated Text](https://arxiv.org/abs/2509.18862)
*Luyan Zhang,Xinyu Xie*

Main category: cs.CL

TL;DR: Multi-feature integration for AI text detection provides minimal performance gains (0.4-0.5%) with substantial computational overhead (4.2x), suggesting modern neural models already capture most relevant detection signals efficiently.


<details>
  <summary>Details</summary>
Motivation: To test whether combining semantic, syntactic, and statistical features significantly improves AI text detection beyond single neural models, as this assumption hasn't been rigorously tested with modern LLM-generated text.

Method: Implemented MHFD (Multi-Hierarchical Feature Detection) integrating DeBERTa-based semantic analysis, syntactic parsing, and statistical probability features through adaptive fusion.

Result: MHFD achieves 89.7% accuracy in in-domain detection and 84.2% in cross-domain detection, showing only modest improvements of 0.4-2.6% over existing methods despite theoretical expectations.

Conclusion: Multi-feature integration provides minimal benefits with substantial computational costs, indicating that modern neural language models may already capture most relevant detection signals efficiently.

Abstract: With the rapid advancement of large language model technology, there is
growing interest in whether multi-feature approaches can significantly improve
AI text detection beyond what single neural models achieve. While intuition
suggests that combining semantic, syntactic, and statistical features should
provide complementary signals, this assumption has not been rigorously tested
with modern LLM-generated text. This paper provides a systematic empirical
investigation of multi-hierarchical feature integration for AI text detection,
specifically testing whether the computational overhead of combining multiple
feature types is justified by performance gains. We implement MHFD
(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic
analysis, syntactic parsing, and statistical probability features through
adaptive fusion. Our investigation reveals important negative results: despite
theoretical expectations, multi-feature integration provides minimal benefits
(0.4-0.5% improvement) while incurring substantial computational costs (4.2x
overhead), suggesting that modern neural language models may already capture
most relevant detection signals efficiently. Experimental results on multiple
benchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in
in-domain detection and maintains 84.2% stable performance in cross-domain
detection, showing modest improvements of 0.4-2.6% over existing methods.

</details>


### [163] [Diversity Boosts AI-Generated Text Detection](https://arxiv.org/abs/2509.18880)
*Advik Raj Basani,Pin-Yu Chen*

Main category: cs.CL

TL;DR: DivEye is a novel AI-generated text detection framework that uses surprisal-based features to capture unpredictability fluctuations, outperforming existing detectors and providing interpretable insights.


<details>
  <summary>Details</summary>
Motivation: To combat LLM misuse in education, business, journalism, and social media by detecting synthetic text that can mask misinformation, addressing limitations of prior detectors that struggle with high-quality generations and lack interpretability.

Method: Captures how unpredictability fluctuates across text using surprisal-based features, leveraging the observation that human-authored text exhibits richer variability in lexical and structural unpredictability than LLM outputs.

Result: Outperforms existing zero-shot detectors by up to 33.2%, achieves competitive performance with fine-tuned baselines, robust to paraphrasing and adversarial attacks, generalizes well across domains and models, and improves existing detectors by up to 18.7% when used as auxiliary signal.

Conclusion: Rhythmic unpredictability is a powerful and underexplored signal for LLM detection, with DivEye providing both effective detection and interpretable insights into why texts are flagged.

Abstract: Detecting AI-generated text is an increasing necessity to combat misuse of
LLMs in education, business compliance, journalism, and social media, where
synthetic fluency can mask misinformation or deception. While prior detectors
often rely on token-level likelihoods or opaque black-box classifiers, these
approaches struggle against high-quality generations and offer little
interpretability. In this work, we propose DivEye, a novel detection framework
that captures how unpredictability fluctuates across a text using
surprisal-based features. Motivated by the observation that human-authored text
exhibits richer variability in lexical and structural unpredictability than LLM
outputs, DivEye captures this signal through a set of interpretable statistical
features. Our method outperforms existing zero-shot detectors by up to 33.2%
and achieves competitive performance with fine-tuned baselines across multiple
benchmarks. DivEye is robust to paraphrasing and adversarial attacks,
generalizes well across domains and models, and improves the performance of
existing detectors by up to 18.7% when used as an auxiliary signal. Beyond
detection, DivEye provides interpretable insights into why a text is flagged,
pointing to rhythmic unpredictability as a powerful and underexplored signal
for LLM detection.

</details>


### [164] [Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass](https://arxiv.org/abs/2509.18901)
*Nicholas Popovič,Michael Färber*

Main category: cs.CL

TL;DR: JEDI is an encoder-only architecture that jointly performs extractive atomic fact decomposition and interpretable inference for NLI tasks, eliminating the need for resource-intensive generative LLMs during inference.


<details>
  <summary>Details</summary>
Motivation: Existing methods for atomic fact decomposition in NLI rely on resource-heavy generative LLMs, which are inefficient for inference. The goal is to develop a more efficient approach that maintains interpretability and robustness.

Method: Proposed JEDI architecture uses encoder-only models for joint extractive decomposition and inference. Trained on a large corpus of synthetic rationales covering multiple NLI benchmarks.

Result: JEDI achieves competitive accuracy in-distribution and significantly improves robustness out-of-distribution and in adversarial settings compared to models using only extractive rationale supervision.

Conclusion: Interpretability and robust generalization in NLI can be effectively achieved using encoder-only architectures and synthetic rationales, providing a more efficient alternative to generative LLMs.

Abstract: Recent works in Natural Language Inference (NLI) and related tasks, such as
automated fact-checking, employ atomic fact decomposition to enhance
interpretability and robustness. For this, existing methods rely on
resource-intensive generative large language models (LLMs) to perform
decomposition. We propose JEDI, an encoder-only architecture that jointly
performs extractive atomic fact decomposition and interpretable inference
without requiring generative models during inference. To facilitate training,
we produce a large corpus of synthetic rationales covering multiple NLI
benchmarks. Experimental results demonstrate that JEDI achieves competitive
accuracy in distribution and significantly improves robustness out of
distribution and in adversarial settings over models based solely on extractive
rationale supervision. Our findings show that interpretability and robust
generalization in NLI can be realized using encoder-only architectures and
synthetic rationales. Code and data available at https://jedi.nicpopovic.com

</details>


### [165] [DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment](https://arxiv.org/abs/2509.18987)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: This paper proposes using Dynamic Time Warping (DTW) to align speech and text embeddings for End-to-End Speech Translation, achieving more accurate alignments and comparable performance while being faster than previous methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for bridging the modality gap in E2E-ST require alignment tools not available for all languages, and existing embedding alignment methods using nearest-neighbor search produce inaccurate alignments.

Method: Adapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during training to bridge the modality gap between speech and text representations.

Result: The method produces more accurate alignments, achieves comparable E2E-ST results while being significantly faster, and outperforms previous work in low resource settings on 5 out of 6 language directions.

Conclusion: DTW-based alignment effectively bridges the modality gap in E2E-ST, offering improved accuracy and efficiency over existing methods, particularly in low-resource scenarios.

Abstract: End-to-End Speech Translation (E2E-ST) is the task of translating source
speech directly into target text bypassing the intermediate transcription step.
The representation discrepancy between the speech and text modalities has
motivated research on what is known as bridging the modality gap.
State-of-the-art methods addressed this by aligning speech and text
representations on the word or token level. Unfortunately, this requires an
alignment tool that is not available for all languages. Although this issue has
been addressed by aligning speech and text embeddings using nearest-neighbor
similarity search, it does not lead to accurate alignments. In this work, we
adapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during
training. Our experiments demonstrate the effectiveness of our method in
bridging the modality gap in E2E-ST. Compared to previous work, our method
produces more accurate alignments and achieves comparable E2E-ST results while
being significantly faster. Furthermore, our method outperforms previous work
in low resource settings on 5 out of 6 language directions.

</details>


### [166] [Investigating Test-Time Scaling with Reranking for Machine Translation](https://arxiv.org/abs/2509.19020)
*Shaomu Tan,Ryosuke Mitani,Ritvik Choudhary,Toshiyuki Sekiya*

Main category: cs.CL

TL;DR: Test-Time Scaling (TTS) for machine translation shows that generating multiple candidates and selecting the best can improve translation quality for high-resource languages, allowing smaller models to match larger ones, but may degrade quality in low-resource scenarios due to metric limitations.


<details>
  <summary>Details</summary>
Motivation: Scaling model parameters improves NLP systems but is computationally expensive. Test-Time Scaling offers an alternative by allocating more computation at inference, but it hasn't been systematically explored for machine translation.

Method: Systematic study of TTS for MT using best-of-N framework on WMT24 benchmarks, covering six high-resource and one low-resource language pairs, five model sizes (3B-72B), and various TTS compute budgets (N up to 1024).

Result: TTS improves translation quality for high-resource languages according to multiple neural MT metrics and human evaluation. Smaller models with large N can match or surpass larger models at N=1. Under fixed compute budgets, larger models are more efficient, and TTS can degrade quality in low-resource cases due to metric blind spots.

Conclusion: TTS is effective for high-resource machine translation but has limitations in low-resource scenarios. It provides a computational trade-off where smaller models with increased inference computation can achieve comparable performance to larger models.

Abstract: Scaling model parameters has become the de facto strategy for improving NLP
systems, but it comes with substantial computational costs. Test-Time Scaling
(TTS) offers an alternative by allocating more computation at inference:
generating multiple candidates and selecting the best. While effective in tasks
such as mathematical reasoning, TTS has not been systematically explored for
machine translation (MT). In this paper, we present the first systematic study
of TTS for MT, investigating a simple but practical best-of-N framework on
WMT24 benchmarks. Our experiments cover six high-resource and one low-resource
language pairs, five model sizes (3B-72B), and various TTS compute budget (N up
to 1024). Our results show that a) For high-resource languages, TTS generally
improves translation quality according to multiple neural MT evaluation
metrics, and our human evaluation confirms these gains; b) Augmenting smaller
models with large $N$ can match or surpass larger models at $N{=}1$ with more
compute cost; c) Under fixed compute budgets, larger models are typically more
efficient, and TTS can degrade quality due to metric blind spots in
low-resource cases.

</details>


### [167] [Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus](https://arxiv.org/abs/2509.19033)
*Chiara Alzetta,Serena Auriemma,Alessandro Bondielli,Luca Dini,Chiara Fazzone,Alessio Miaschi,Martina Miliani,Marta Sartor*

Main category: cs.CL

TL;DR: Analysis of research trends in Italian Computational Linguistics and Natural Language Processing through 10 years of CLiC-it conference proceedings (2014-2024), tracking shifts from lexical/semantic resources to language modeling and multimodality.


<details>
  <summary>Details</summary>
Motivation: To track the evolution of research goals and priorities in the Italian CL/NLP community over the past decade, particularly examining the impact of Transformer-based LLMs on research directions.

Method: Compiled proceedings from 10 editions of CLiC-it conference into a corpus, analyzing both metadata (author provenance, gender, affiliations) and paper content to identify trends and developments.

Result: Identified a significant shift in research focus from Lexical and Semantic Resources to Language Modelling and Multimodality, reflecting the broader field's transformation driven by LLMs.

Conclusion: Provides valuable insights into emerging trends and key developments in Italian CL/NLP research, supporting informed decisions and future directions for both Italian and international research communities.

Abstract: Over the past decade, Computational Linguistics (CL) and Natural Language
Processing (NLP) have evolved rapidly, especially with the advent of
Transformer-based Large Language Models (LLMs). This shift has transformed
research goals and priorities, from Lexical and Semantic Resources to Language
Modelling and Multimodality. In this study, we track the research trends of the
Italian CL and NLP community through an analysis of the contributions to
CLiC-it, arguably the leading Italian conference in the field. We compile the
proceedings from the first 10 editions of the CLiC-it conference (from 2014 to
2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its
metadata, including author provenance, gender, affiliations, and more, as well
as the content of the papers themselves, which address various topics. Our goal
is to provide the Italian and international research communities with valuable
insights into emerging trends and key developments over time, supporting
informed decisions and future directions in the field.

</details>


### [168] [Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering](https://arxiv.org/abs/2509.19094)
*Alireza Salemi,Cheng Li,Mingyang Zhang,Qiaozhu Mei,Zhuowan Li,Spurthi Amba Hombaiah,Weize Kong,Tao Chen,Hamed Zamani,Michael Bendersky*

Main category: cs.CL

TL;DR: Pathways of Thoughts (PoT) is an inference-stage method that enables personalized question answering by modeling LLM reasoning as an iterative decision process with cognitive operations, producing diverse candidate responses that are aggregated based on user preferences.


<details>
  <summary>Details</summary>
Motivation: Personalized QA is essential for adapting to user-specific information needs but remains challenging due to inferring preferences from noisy contexts and generating responses that are correct, appropriate, and aligned with user expectations.

Method: PoT models LLM reasoning as an iterative decision process where the model dynamically selects among cognitive operations (reasoning, revision, personalization, clarification) to explore multiple reasoning trajectories and produce diverse candidate responses, then aggregates them based on inferred user preferences.

Result: Experiments on LaMP-QA benchmark show PoT consistently outperforms baselines with up to 13.1% relative improvement. Human evaluation shows annotators prefer PoT outputs in 66% of cases with only 15% ties.

Conclusion: PoT effectively addresses personalized QA challenges by leveraging diverse reasoning paths and preference-based aggregation, demonstrating significant improvements over existing methods without requiring task-specific fine-tuning.

Abstract: Personalization is essential for adapting question answering (QA) systems to
user-specific information needs, thereby improving both accuracy and user
satisfaction. However, personalized QA remains relatively underexplored due to
challenges such as inferring preferences from long, noisy, and implicit
contexts, and generating responses that are simultaneously correct,
contextually appropriate, and aligned with user expectations and background
knowledge. To address these challenges, we propose Pathways of Thoughts (PoT),
an inference-stage method that applies to any large language model (LLM)
without requiring task-specific fine-tuning. The approach models the reasoning
of an LLM as an iterative decision process, where the model dynamically selects
among cognitive operations such as reasoning, revision, personalization, and
clarification. This enables exploration of multiple reasoning trajectories,
producing diverse candidate responses that capture different perspectives. PoT
then aggregates and reweights these candidates according to inferred user
preferences, yielding a final personalized response that benefits from the
complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA
benchmark for personalized QA show that PoT consistently outperforms
competitive baselines, achieving up to a 13.1% relative improvement. Human
evaluation corroborates these results, with annotators preferring outputs from
PoT in 66% of cases and reporting ties in only 15% of cases.

</details>


### [169] [Are most sentences unique? An empirical examination of Chomskyan claims](https://arxiv.org/abs/2509.19108)
*Hiram Ring*

Main category: cs.CL

TL;DR: This paper empirically investigates the linguistic claim that most sentences are unique by analyzing corpora of different genres using NLTK Python library to count exact string matches.


<details>
  <summary>Details</summary>
Motivation: To test the common linguistic assertion (e.g., by Pinker and Chomsky) that virtually every sentence people use is unique and novel, using empirical data from large corpora.

Method: Used the NLTK Python library to parse corpora across different genres and count exact string matches to identify duplicate sentences.

Result: Found that while unique sentences often constitute the majority of corpora, this varies significantly by genre, and duplicate sentences represent a non-negligible portion of all corpora analyzed.

Conclusion: The claim that most sentences are completely unique is genre-dependent and overstated; duplicate sentences are more common than traditionally assumed in linguistic theory.

Abstract: A repeated claim in linguistics is that the majority of linguistic utterances
are unique. For example, Pinker (1994: 10), summarizing an argument by Noam
Chomsky, states that "virtually every sentence that a person utters or
understands is a brand-new combination of words, appearing for the first time
in the history of the universe." With the increased availability of large
corpora, this is a claim that can be empirically investigated. The current
paper addresses the question by using the NLTK Python library to parse corpora
of different genres, providing counts of exact string matches in each. Results
show that while completely unique sentences are often the majority of corpora,
this is highly constrained by genre, and that duplicate sentences are not an
insignificant part of any individual corpus.

</details>


### [170] [Human-Annotated NER Dataset for the Kyrgyz Language](https://arxiv.org/abs/2509.19109)
*Timur Turatali,Anton Alekseev,Gulira Jumalieva,Gulnara Kabaeva,Sergey Nikolenko*

Main category: cs.CL

TL;DR: KyrgyzNER is the first manually annotated NER dataset for Kyrgyz language, containing 1,499 news articles with 39,075 entity mentions across 27 classes. The paper evaluates various NER models and finds multilingual RoBERTa performs best, highlighting challenges and opportunities for low-resource language processing.


<details>
  <summary>Details</summary>
Motivation: To address the lack of named entity recognition resources for the Kyrgyz language by creating the first manually annotated dataset and evaluating state-of-the-art models on this low-resource language.

Method: Created KyrgyzNER dataset from 1,499 news articles (10,900 sentences, 39,075 entity mentions across 27 classes). Evaluated traditional sequence labeling (CRF) and multilingual transformer models (including RoBERTa) fine-tuned on the dataset.

Result: Multilingual RoBERTa achieved the best performance with promising precision-recall balance, though all models struggled with rare entity categories. Other multilingual models yielded comparable results.

Conclusion: Multilingual pretrained models show potential for Kyrgyz language processing despite challenges with rare entities. Future work should explore more granular annotation schemes for better evaluation of Kyrgyz language processing pipelines.

Abstract: We introduce KyrgyzNER, the first manually annotated named entity recognition
dataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG
news portal, the dataset contains 10,900 sentences and 39,075 entity mentions
across 27 named entity classes. We show our annotation scheme, discuss the
challenges encountered in the annotation process, and present the descriptive
statistics. We also evaluate several named entity recognition models, including
traditional sequence labeling approaches based on conditional random fields and
state-of-the-art multilingual transformer-based models fine-tuned on our
dataset. While all models show difficulties with rare entity categories, models
such as the multilingual RoBERTa variant pretrained on a large corpus across
many languages achieve a promising balance between precision and recall. These
findings emphasize both the challenges and opportunities of using multilingual
pretrained models for processing languages with limited resources. Although the
multilingual RoBERTa model performed best, other multilingual models yielded
comparable results. This suggests that future work exploring more granular
annotation schemes may offer deeper insights for Kyrgyz language processing
pipelines evaluation.

</details>


### [171] [Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering](https://arxiv.org/abs/2509.19125)
*Kun Zhu,Lizi Liao,Yuxuan Gu,Lei Huang,Xiaocheng Feng,Bing Qin*

Main category: cs.CL

TL;DR: A novel context-aware hierarchical taxonomy generation framework that uses LLM-guided multi-aspect encoding with dynamic clustering to organize scientific literature more effectively than existing methods.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of scientific literature requires efficient organization methods, but current taxonomy construction approaches using unsupervised clustering or direct LLM prompting lack coherence and granularity.

Method: Proposes a framework that uses LLMs to identify key aspects of papers (methodology, dataset, evaluation), generates aspect-specific summaries, then encodes and clusters them along each aspect to form coherent hierarchies.

Result: Significantly outperforms prior approaches, achieving state-of-the-art performance in taxonomy coherence, granularity, and interpretability. Also introduces a new benchmark of 156 expert-crafted taxonomies covering 11.6k papers.

Conclusion: The proposed context-aware hierarchical taxonomy generation framework effectively addresses limitations of existing methods and provides superior organization of scientific literature.

Abstract: The rapid growth of scientific literature demands efficient methods to
organize and synthesize research findings. Existing taxonomy construction
methods, leveraging unsupervised clustering or direct prompting of large
language models (LLMs), often lack coherence and granularity. We propose a
novel context-aware hierarchical taxonomy generation framework that integrates
LLM-guided multi-aspect encoding with dynamic clustering. Our method leverages
LLMs to identify key aspects of each paper (e.g., methodology, dataset,
evaluation) and generates aspect-specific paper summaries, which are then
encoded and clustered along each aspect to form a coherent hierarchy. In
addition, we introduce a new evaluation benchmark of 156 expert-crafted
taxonomies encompassing 11.6k papers, providing the first naturally annotated
dataset for this task. Experimental results demonstrate that our method
significantly outperforms prior approaches, achieving state-of-the-art
performance in taxonomy coherence, granularity, and interpretability.

</details>


### [172] [Anecdoctoring: Automated Red-Teaming Across Language and Place](https://arxiv.org/abs/2509.19143)
*Alejandro Cuevas,Saloni Dash,Bharat Kumar Nayak,Dan Vann,Madeleine I. G. Daepp*

Main category: cs.CL

TL;DR: Anecdoting is a novel red-teaming approach that generates adversarial prompts across languages and cultures to test AI disinformation risks, using multilingual misinformation claims clustered into narratives with knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Disinformation is a top AI misuse risk, but current red-teaming evaluations are US- and English-centric, lacking global robustness across diverse languages and cultures.

Method: Collect misinformation claims from fact-checking sites in English, Spanish, Hindi (US and India), cluster claims into narratives, characterize clusters with knowledge graphs to augment attacker LLM, and generate adversarial prompts.

Result: The method produces higher attack success rates and offers interpretability benefits compared to few-shot prompting.

Conclusion: Results highlight the need for globally scalable disinformation mitigations grounded in real-world adversarial misuse.

Abstract: Disinformation is among the top risks of generative artificial intelligence
(AI) misuse. Global adoption of generative AI necessitates red-teaming
evaluations (i.e., systematic adversarial probing) that are robust across
diverse languages and cultures, but red-teaming datasets are commonly US- and
English-centric. To address this gap, we propose "anecdoctoring", a novel
red-teaming approach that automatically generates adversarial prompts across
languages and cultures. We collect misinformation claims from fact-checking
websites in three languages (English, Spanish, and Hindi) and two geographies
(US and India). We then cluster individual claims into broader narratives and
characterize the resulting clusters with knowledge graphs, with which we
augment an attacker LLM. Our method produces higher attack success rates and
offers interpretability benefits relative to few-shot prompting. Results
underscore the need for disinformation mitigations that scale globally and are
grounded in real-world adversarial misuse.

</details>


### [173] [Measuring AI "Slop" in Text](https://arxiv.org/abs/2509.19163)
*Chantal Shaib,Tuhin Chakrabarty,Diego Garcia-Olano,Byron C. Wallace*

Main category: cs.CL

TL;DR: This paper develops a taxonomy and measurement framework for AI 'slop' - low-quality AI-generated text - through expert interviews and proposes interpretable dimensions for assessment.


<details>
  <summary>Details</summary>
Motivation: There is currently no agreed-upon definition of AI 'slop' nor means to measure its occurrence, despite it being an increasingly popular term for low-quality AI-generated text.

Method: Developed taxonomy through interviews with NLP, writing, and philosophy experts; conducted span-level annotation to assess binary 'slop' judgments and correlate them with latent dimensions like coherence and relevance.

Result: Found that binary 'slop' judgments are somewhat subjective but correlate with latent dimensions such as coherence and relevance. The framework can evaluate AI-generated text in detection and binary preference tasks.

Conclusion: The proposed framework offers new insights into linguistic and stylistic factors that contribute to quality judgments of AI-generated text.

Abstract: AI "slop" is an increasingly popular term used to describe low-quality
AI-generated text, but there is currently no agreed upon definition of this
term nor a means to measure its occurrence. In this work, we develop a taxonomy
of "slop" through interviews with experts in NLP, writing, and philosophy, and
propose a set of interpretable dimensions for its assessment in text. Through
span-level annotation, we find that binary "slop" judgments are (somewhat)
subjective, but such determinations nonetheless correlate with latent
dimensions such as coherence and relevance. Our framework can be used to
evaluate AI-generated text in both detection and binary preference tasks,
potentially offering new insights into the linguistic and stylistic factors
that contribute to quality judgments.

</details>


### [174] [Soft Tokens, Hard Truths](https://arxiv.org/abs/2509.19170)
*Natasha Butt,Ariel Kwiatkowski,Ismail Labiad,Julia Kempe,Yann Ollivier*

Main category: cs.CL

TL;DR: First scalable RL method for learning continuous Chain-of-Thought tokens without distillation, enabling hundreds of tokens with minimal overhead and improving reasoning diversity.


<details>
  <summary>Details</summary>
Motivation: Continuous tokens have greater theoretical expressivity than discrete tokens for reasoning paths, but practical training has been limited by computational costs and distillation requirements.

Method: Uses 'soft' tokens (mixtures of tokens with noise) for RL exploration, enabling scalable training of continuous CoTs without reference discrete CoTs.

Result: Matches discrete-token CoTs for pass@1 and surpasses them for pass@32 on math reasoning benchmarks, showing greater CoT diversity. Best performance comes from training with continuous CoTs then using discrete tokens at inference.

Conclusion: Continuous CoT RL training provides a 'softer touch' that better preserves base model predictions on out-of-domain tasks while improving reasoning performance.

Abstract: The use of continuous instead of discrete tokens during the Chain-of-Thought
(CoT) phase of reasoning LLMs has garnered attention recently, based on the
intuition that a continuous mixture of discrete tokens could simulate a
superposition of several reasoning paths simultaneously. Theoretical results
have formally proven that continuous tokens have much greater expressivity and
can solve specific problems more efficiently. However, practical use of
continuous tokens has been limited by strong training difficulties: previous
works either just use continuous tokens at inference time on a pre-trained
discrete-token model, or must distill the continuous CoT from ground-truth
discrete CoTs and face computational costs that limit the CoT to very few
tokens.
  This is the first work introducing a scalable method to learn continuous CoTs
via reinforcement learning (RL), without distilling from reference discrete
CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input
embedding to provide RL exploration. Computational overhead is minimal,
enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning
benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs
match discrete-token CoTs for pass@1 and surpass them for pass@32, showing
greater CoT diversity. In systematic comparisons, the best-performing scenario
is to train with continuous CoT tokens then use discrete tokens for inference,
meaning the "soft" models can be deployed in a standard way. Finally, we show
continuous CoT RL training better preserves the predictions of the base model
on out-of-domain tasks, thus providing a softer touch to the base model.

</details>


### [175] [Online Process Reward Leanring for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.19199)
*Xiaoqian Liu,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li,Junge Zhang,Jianbin Jiao*

Main category: cs.CL

TL;DR: OPRL is a novel credit-assignment strategy for agentic RL that learns implicit process rewards from trajectory preferences to address sparse reward challenges, achieving state-of-the-art performance with higher sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Sparse and unverifiable rewards in LLM agent training make temporal credit assignment extremely challenging. Existing process supervision methods suffer from biased annotation, reward hacking, high variance, or fail when state overlap is rare.

Method: OPRL optimizes an implicit process reward model (PRM) alternately with the agent's policy using a trajectory-based DPO objective to transform trajectory preferences into step rewards. These are combined with episode-level advantages for policy updates.

Result: OPRL achieves superior performance over frontier LLMs and strong RL baselines across WebShop, VisualSokoban, and SOTOPIA benchmarks, with state-of-the-art results, higher sample efficiency, and lower training variance.

Conclusion: OPRL provides an effective solution for agentic learning with sparse rewards, demonstrating efficient exploration and potential for real-world applications through its self-reinforcing reward learning loop.

Abstract: Large language models (LLMs) are increasingly trained with reinforcement
learning (RL) as autonomous agents that reason and act over long horizons in
interactive environments.
  However, sparse and sometimes unverifiable rewards make temporal credit
assignment extremely challenging.
  Recent work attempts to integrate process supervision into agent learning but
suffers from biased annotation, reward hacking, high-variance from overly
fine-grained signals or failtures when state overlap is rare.
  We therefore introduce Online Process Reward Learning (OPRL), a general
credit-assignment strategy for agentic RL that integrates seamlessly with
standard on-policy algorithms without relying on additional rollouts or
explicit step labels.
  In OPRL, we optimize an implicit process reward model (PRM) alternately with
the agent's policy to transform trajectory preferences into implicit step
rewards through a trajectory-based DPO objective.
  These step rewards are then used to compute step-level advantages, which are
combined with episode-level advantages from outcome rewards for policy update,
creating a self-reinforcing loop.
  Theoretical findings guarantee that the learned step rewards are consistent
with trajectory preferences and act as potential-based shaping rewards,
providing bounded gradients to stabilize training.
  Empirically, we evaluate OPRL on three distinct agent benmarks, including
WebShop and VisualSokoban, as well as open-ended social interactions with
unverfiable rewards in SOTOPIA.
  Crucially, OPRL shows superior performance over frontier LLMs and strong RL
baselines across domains, achieving state-of-the-art results with higher
sample-efficiency and lower variance during training.
  Further analysis also demonstrates the efficient exploration by OPRL using
fewer actions, underscoring its potential for agentic learning in real-world
scenarios.

</details>


### [176] [Steering Multimodal Large Language Models Decoding for Context-Aware Safety](https://arxiv.org/abs/2509.19212)
*Zheyuan Liu,Zhangchen Xu,Guangyao Dou,Xiangchi Yuan,Zhaoxuan Tan,Radha Poovendran,Meng Jiang*

Main category: cs.CL

TL;DR: SafeCoDe is a lightweight decoding framework that improves multimodal LLM safety by dynamically adjusting token generation based on visual context to balance oversensitivity and undersensitivity.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with context-aware safety decisions, often failing to balance unjustified refusals of benign queries (oversensitivity) and missed detection of visually grounded risks (undersensitivity).

Method: SafeCoDe uses a two-stage approach: (1) contrastive decoding that highlights tokens sensitive to visual context by comparing real vs Gaussian-noised images, and (2) global-aware token modulation that integrates scene-level reasoning with token-level adjustment.

Result: Extensive experiments across diverse MLLM architectures and safety benchmarks show SafeCoDe consistently improves context-sensitive refusal behaviors while preserving model helpfulness.

Conclusion: SafeCoDe effectively addresses the safety alignment gap in MLLMs through a model-agnostic decoding framework that enables dynamic, context-aware safety decisions.

Abstract: Multimodal Large Language Models (MLLMs) are increasingly deployed in
real-world applications, yet their ability to make context-aware safety
decisions remains limited. Existing methods often fail to balance
oversensitivity (unjustified refusals of benign queries) and undersensitivity
(missed detection of visually grounded risks), leaving a persistent gap in
safety alignment. To address this issue, we introduce Safety-aware Contrastive
Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that
dynamically adjusts token generation based on multimodal context. SafeCoDe
operates in two stages: (1) a contrastive decoding mechanism that highlights
tokens sensitive to visual context by contrasting real and Gaussian-noised
images, and (2) a global-aware token modulation strategy that integrates
scene-level reasoning with token-level adjustment to adapt refusals according
to the predicted safety verdict. Extensive experiments across diverse MLLM
architectures and safety benchmarks, covering undersensitivity,
oversensitivity, and general safety evaluations, show that SafeCoDe
consistently improves context-sensitive refusal behaviors while preserving
model helpfulness.

</details>


### [177] [Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction](https://arxiv.org/abs/2509.19224)
*Tariq Abdul-Quddoos,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: Comparative analysis of attention-based models (Bert Base, BioBert, Clinical Bert variants, RoBerta, Clinical Longformer) for EHR information extraction using CMED dataset from n2c2 2022 challenges.


<details>
  <summary>Details</summary>
Motivation: To determine which pre-trained attention-based models perform best for extracting contextual medication event information from Electronic Health Records (EHRs), as current models show varying effectiveness across different clinical NLP tasks.

Method: Fine-tuned multiple pre-trained models on CMED dataset for medication extraction, medical event detection, and multi-dimensional medication event context classification. Applied processing methods to make EHRs compatible with models and evaluated using recall, precision, and F1-score metrics.

Result: Models pre-trained on clinical data (BioBert, Clinical Bert variants) performed better for medication and medication event detection, while Bert Base (pre-trained on general domain data) showed superior performance for classifying medication event contexts.

Conclusion: Different pre-trained models excel at different EHR information extraction tasks - clinical domain models are better for detection tasks, while general domain models may be more effective for context classification tasks in medication event analysis.

Abstract: Attention-based models have become the leading approach in modeling medical
language for Natural Language Processing (NLP) in clinical notes. These models
outperform traditional techniques by effectively capturing contextual rep-
resentations of language. In this research a comparative analysis is done
amongst pre- trained attention based models namely Bert Base, BioBert, two
variations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task
related to Electronic Health Record (EHR) information extraction. The tasks
from Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges
(n2c2) are considered for this comparison, with the Contextualized Medication
Event Dataset (CMED) given for these task. CMED is a dataset of unstructured
EHRs and annotated notes that contain task relevant information about the EHRs.
The goal of the challenge is to develop effective solutions for extracting
contextual information related to patient medication events from EHRs using
data driven methods. Each pre-trained model is fine-tuned and applied on CMED
to perform medication extraction, medical event detection, and
multi-dimensional medication event context classification. Pro- cessing methods
are also detailed for breaking down EHRs for compatibility with the applied
models. Performance analysis has been carried out using a script based on
constructing medical terms from the evaluation portion of CMED with metrics
including recall, precision, and F1-Score. The results demonstrate that models
pre-trained on clinical data are more effective in detecting medication and
medication events, but Bert Base, pre- trained on general domain data showed to
be the most effective for classifying the context of events related to
medications.

</details>


### [178] [CompLLM: Compression for Long Context Q&A](https://arxiv.org/abs/2509.19228)
*Gabriele Berton,Jayakrishnan Unnikrishnan,Son Tran,Mubarak Shah*

Main category: cs.CL

TL;DR: CompLLM is a soft context compression technique that divides long contexts into segments for independent compression, enabling linear scaling, generalization to long sequences, and reusable cached segments.


<details>
  <summary>Details</summary>
Motivation: LLMs face computational challenges with long contexts due to quadratic self-attention complexity. Existing compression methods process context as a single unit, leading to quadratic complexity and inability to reuse computations.

Method: CompLLM divides context into segments and compresses each independently, achieving linear scaling, scalability from short to long sequences, and reusability through segment caching.

Result: With 2x compression rate, CompLLM speeds up Time To First Token by up to 4x, reduces KV cache size by 50%, and achieves comparable or better performance than uncompressed context on long sequences.

Conclusion: CompLLM demonstrates effective and practical utility for processing long contexts in LLMs through its efficient segment-based compression approach.

Abstract: Large Language Models (LLMs) face significant computational challenges when
processing long contexts due to the quadratic complexity of self-attention.
While soft context compression methods, which map input text to smaller latent
representations, have shown promise, their real-world adoption is limited.
Existing techniques typically compress the context as a single unit, which
leads to quadratic compression complexity and an inability to reuse
computations across queries with overlapping contexts. In this work, we
introduce CompLLM, a soft compression technique designed for practical
deployment. Instead of processing the context holistically, CompLLM divides it
into segments and compresses each one independently. This simple design choice
yields three critical properties: efficiency, as the compression step scales
linearly with the context length; scalability, enabling models trained on short
sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and
reusability, allowing compressed segments to be cached and reused across
different queries. Our experiments show that with a 2x compression rate, at
high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x
and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance
comparable to that obtained with the uncompressed context, and even surpasses
it on very long sequences, demonstrating its effectiveness and practical
utility.

</details>


### [179] [Reinforcement Learning on Pre-Training Data](https://arxiv.org/abs/2509.19249)
*Siheng Li,Kejiao Li,Zenan Xu,Guanhua Huang,Evander Yang,Kun Li,Haoyuan Wu,Jiajia Wu,Zihao Zheng,Chenchen Zhang,Kun Shi,Kyrierl Deng,Qi Yi,Ruibin Xiong,Tingqiang Xu,Yuhao Jiang,Jianfeng Yan,Yuyuan Zeng,Guanghui Xu,Jinbao Xue,Zhijiang Xu,Zheng Fang,Shuai Li,Qibin Liu,Xiaoxue Li,Zhuoyu Li,Yangyu Tao,Fei Gao,Cheng Jiang,Bo Chao Wang,Kai Liu,Jianchen Zhu,Wai Lam,Wayyt Wang,Bo Zhou,Di Wang*

Main category: cs.CL

TL;DR: RLPT is a new training paradigm that uses reinforcement learning on pre-training data to scale LLMs without human annotation, achieving significant performance improvements on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: The growing gap between computational resources and limited high-quality text data constrains conventional scaling approaches for LLMs, requiring new methods that don't rely on human annotation.

Method: RLPT uses reinforcement learning with a next-segment reasoning objective, where the model is rewarded for accurately predicting subsequent text segments from pre-training data, enabling autonomous exploration of meaningful trajectories.

Result: RLPT achieved substantial improvements on multiple benchmarks: +3.0 on MMLU, +5.1 on MMLU-Pro, +8.1 on GPQA-Diamond, +6.0 on KOR-Bench, +6.6 on AIME24, and +5.3 on AIME25 when applied to Qwen3-4B-Base.

Conclusion: RLPT demonstrates favorable scaling behavior and strong potential for continued gains, extending LLM reasoning boundaries and enhancing RLVR performance while eliminating dependency on human annotation.

Abstract: The growing disparity between the exponential scaling of computational
resources and the finite growth of high-quality text data now constrains
conventional scaling approaches for large language models (LLMs). To address
this challenge, we introduce Reinforcement Learning on Pre-Training data
(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast
to prior approaches that scale training primarily through supervised learning,
RLPT enables the policy to autonomously explore meaningful trajectories to
learn from pre-training data and improve its capability through reinforcement
learning (RL). While existing RL strategies such as reinforcement learning from
human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)
rely on human annotation for reward construction, RLPT eliminates this
dependency by deriving reward signals directly from pre-training data.
Specifically, it adopts a next-segment reasoning objective, rewarding the
policy for accurately predicting subsequent text segments conditioned on the
preceding context. This formulation allows RL to be scaled on pre-training
data, encouraging the exploration of richer trajectories across broader
contexts and thereby fostering more generalizable reasoning skills. Extensive
experiments on both general-domain and mathematical reasoning benchmarks across
multiple models validate the effectiveness of RLPT. For example, when applied
to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,
$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and
AIME25, respectively. The results further demonstrate favorable scaling
behavior, suggesting strong potential for continued gains with more compute. In
addition, RLPT provides a solid foundation, extending the reasoning boundaries
of LLMs and enhancing RLVR performance.

</details>


### [180] [Extracting Conceptual Spaces from LLMs Using Prototype Embeddings](https://arxiv.org/abs/2509.19269)
*Nitesh Kumar,Usashi Chatterjee,Steven Schockaert*

Main category: cs.CL

TL;DR: The paper proposes a method to extract conceptual spaces from LLMs by encoding features through prototype descriptions and fine-tuning to align embeddings with conceptual dimensions.


<details>
  <summary>Details</summary>
Motivation: Conceptual spaces are valuable for explainable AI but difficult to learn from LLMs, as existing methods extract embeddings but fail to encode the underlying perceptual features that define conceptual dimensions.

Method: Features are encoded by embedding descriptions of corresponding prototypes (e.g., 'very sweet food' for sweetness), and the LLM is fine-tuned to align these prototype embeddings with conceptual space dimensions.

Result: Empirical analysis shows the proposed approach is highly effective at extracting meaningful conceptual spaces from LLMs.

Conclusion: The prototype-based fine-tuning strategy successfully addresses the challenge of learning conceptual spaces from LLMs, enabling better extraction of cognitively meaningful dimensions for explainable AI.

Abstract: Conceptual spaces represent entities and concepts using cognitively
meaningful dimensions, typically referring to perceptual features. Such
representations are widely used in cognitive science and have the potential to
serve as a cornerstone for explainable AI. Unfortunately, they have proven
notoriously difficult to learn, although recent LLMs appear to capture the
required perceptual features to a remarkable extent. Nonetheless, practical
methods for extracting the corresponding conceptual spaces are currently still
lacking. While various methods exist for extracting embeddings from LLMs,
extracting conceptual spaces also requires us to encode the underlying
features. In this paper, we propose a strategy in which features (e.g.
sweetness) are encoded by embedding the description of a corresponding
prototype (e.g. a very sweet food). To improve this strategy, we fine-tune the
LLM to align the prototype embeddings with the corresponding conceptual space
dimensions. Our empirical analysis finds this approach to be highly effective.

</details>


### [181] [SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data](https://arxiv.org/abs/2509.19270)
*Erik Božík,Marek Šuppa*

Main category: cs.CL

TL;DR: SloPalSpeech is a new large-scale Slovak ASR dataset with 2,806 hours of parliamentary speech, used to fine-tune Whisper models and achieve up to 70% WER reduction on Slovak benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of training data for low-resource languages like Slovak in Automatic Speech Recognition (ASR) systems.

Method: Created SloPalSpeech dataset from parliamentary proceedings, developed processing pipeline for alignment and segmentation, fine-tuned multiple OpenAI Whisper models (small, medium, large-v3, large-v3-turbo).

Result: Significant WER reductions on Slovak benchmarks (Common Voice, FLEURS), with Whisper-small achieving up to 70% WER reduction and approaching baseline performance of much larger Whisper-large-v3 model.

Conclusion: The dataset and fine-tuned models are publicly released to support future research in low-resource speech recognition for Slovak and similar languages.

Abstract: Automatic Speech Recognition (ASR) for low-resource languages like Slovak is
hindered by the scarcity of training data. To address this, we introduce
SloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of
speech from parliamentary proceedings. We developed a robust processing
pipeline to align and segment long-form recordings into clean, 30-second
audio-transcript pairs suitable for model training. We use this dataset to
fine-tune several OpenAI Whisper models (small, medium, large-v3, and
large-v3-turbo), achieving significant Word Error Rate (WER) reductions on
standard Slovak benchmarks like Common Voice and FLEURS. For instance, the
fine-tuned Whisper-small model's WER dropped by up to 70\%, approaching the
baseline performance of the much larger Whisper-large-v3 model. To foster
future research in low-resource speech recognition, we publicly release the
complete SloPalSpeech dataset, the fully segmented transcripts (60 million
words), and all our fine-tuned models.

</details>


### [182] [WolBanking77: Wolof Banking Speech Intent Classification Dataset](https://arxiv.org/abs/2509.19271)
*Abdou Karim Kandji,Frédéric Precioso,Cheikh Ba,Samba Ndiaye,Augustin Ndione*

Main category: cs.CL

TL;DR: This paper introduces WolBanking77, a new intent classification dataset for Wolof language focusing on banking domain, addressing the gap in low-resource languages and regions with high illiteracy rates.


<details>
  <summary>Details</summary>
Motivation: To address the research gap in intent classification for low-resource languages like Wolof, which is spoken by over 10 million people in West Africa but has limited written resources due to high illiteracy rates (42% in Senegal).

Method: Created WolBanking77 dataset containing 9,791 text sentences and over 4 hours of spoken sentences in Wolof banking domain. Conducted experiments with various text and voice state-of-the-art models including NLP and ASR models.

Result: Promising results reported with baseline f1-score and word error rate metrics on models trained with WolBanking77 dataset. Detailed analyses of data contents provided.

Conclusion: The paper presents a valuable resource for intent classification research in low-resource languages and plans to maintain, update, and release open-source code for the dataset.

Abstract: Intent classification models have made a lot of progress in recent years.
However, previous studies primarily focus on high-resource languages datasets,
which results in a gap for low-resource languages and for regions with a high
rate of illiterate people where languages are more spoken than read or written.
This is the case in Senegal, for example, where Wolof is spoken by around 90\%
of the population, with an illiteracy rate of 42\% for the country. Wolof is
actually spoken by more than 10 million people in West African region. To
tackle such limitations, we release a Wolof Intent Classification Dataset
(WolBanking77), for academic research in intent classification. WolBanking77
currently contains 9,791 text sentences in the banking domain and more than 4
hours of spoken sentences. Experiments on various baselines are conducted in
this work, including text and voice state-of-the-art models. The results are
very promising on this current dataset. This paper also provides detailed
analyses of the contents of the data. We report baseline f1-score and word
error rate metrics respectively on NLP and ASR models trained on WolBanking77
dataset and also comparisons between models. We plan to share and conduct
dataset maintenance, updates and to release open-source code.

</details>


### [183] [DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture](https://arxiv.org/abs/2509.19274)
*Arijit Maji,Raghvendra Kumar,Akash Ghosh,Anushka,Nemil Shah,Abhilekh Borah,Vanshika Shah,Nishant Mishra,Sriparna Saha*

Main category: cs.CL

TL;DR: DRISHTIKON is a multimodal multilingual benchmark focused exclusively on Indian culture, featuring 64,000+ text-image pairs across 15 languages to evaluate generative AI's cultural understanding.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks have generic global scope, lacking deep coverage of India's diverse cultural regions and traditions. There's a need for culturally specific evaluation tools.

Method: Created a benchmark with aligned text-image pairs spanning all Indian states/UTs, covering festivals, attire, cuisines, art forms, and heritage. Evaluated various VLMs including open-source, proprietary, reasoning-specialized, and Indic-focused models.

Result: Current models show significant limitations in reasoning over culturally grounded multimodal inputs, especially for low-resource languages and less-documented traditions.

Conclusion: DRISHTIKON fills a vital gap in inclusive AI research by providing a robust testbed to advance culturally aware, multimodally competent language technologies.

Abstract: We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual
benchmark centered exclusively on Indian culture, designed to evaluate the
cultural understanding of generative AI systems. Unlike existing benchmarks
with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage
across India's diverse regions, spanning 15 languages, covering all states and
union territories, and incorporating over 64,000 aligned text-image pairs. The
dataset captures rich cultural themes including festivals, attire, cuisines,
art forms, and historical heritage amongst many more. We evaluate a wide range
of vision-language models (VLMs), including open-source small and large models,
proprietary systems, reasoning-specialized VLMs, and Indic-focused models,
across zero-shot and chain-of-thought settings. Our results expose key
limitations in current models' ability to reason over culturally grounded,
multimodal inputs, particularly for low-resource languages and less-documented
traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a
robust testbed to advance culturally aware, multimodally competent language
technologies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [184] [A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services](https://arxiv.org/abs/2509.18101)
*Guanzhong Pan,Haibo Wang*

Main category: cs.AI

TL;DR: This paper presents a cost-benefit analysis framework to help organizations decide between using commercial LLM services vs. on-premise deployment of open-source models, considering factors like data privacy, switching costs, and long-term expenses.


<details>
  <summary>Details</summary>
Motivation: Organizations face a critical decision between commercial LLM services (convenient but with privacy/control concerns) and local deployment (more control but higher upfront costs). There's a need for a systematic framework to determine when on-premise deployment becomes economically viable.

Method: The authors conduct a cost-benefit analysis comparing hardware requirements, operational expenses, and performance benchmarks of open-source models (Qwen, Llama, Mistral, etc.) against major cloud providers' subscription fees.

Result: The study provides estimated breakeven points based on usage levels and performance needs, showing when local deployment becomes cost-effective compared to commercial services.

Conclusion: The framework offers organizations practical guidance for planning their LLM strategies by quantifying the economic viability of on-premise deployment versus cloud subscription services.

Abstract: Large language models (LLMs) are becoming increasingly widespread.
Organizations that want to use AI for productivity now face an important
decision. They can subscribe to commercial LLM services or deploy models on
their own infrastructure. Cloud services from providers such as OpenAI,
Anthropic, and Google are attractive because they provide easy access to
state-of-the-art models and are easy to scale. However, concerns about data
privacy, the difficulty of switching service providers, and long-term operating
costs have driven interest in local deployment of open-source models. This
paper presents a cost-benefit analysis framework to help organizations
determine when on-premise LLM deployment becomes economically viable compared
to commercial subscription services. We consider the hardware requirements,
operational expenses, and performance benchmarks of the latest open-source
models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost
of deploying these models locally with the major cloud providers subscription
fee. Our findings provide an estimated breakeven point based on usage levels
and performance needs. These results give organizations a practical framework
for planning their LLM strategies.

</details>


### [185] [SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture](https://arxiv.org/abs/2509.18123)
*Yeonju Lee,Rui Qi Chen,Joseph Oboamah,Po Nien Su,Wei-zhen Liang,Yeyin Shi,Lu Gan,Yongsheng Chen,Xin Qiao,Jing Li*

Main category: cs.AI

TL;DR: SPADE is an LLM-based framework that detects irrigation patterns and anomalies in soil moisture time-series data using ChatGPT-4.1, achieving superior performance over existing methods without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing soil moisture analysis methods rely on threshold-based rules or data-intensive ML/DL models that lack adaptability and interpretability, creating a need for more flexible and explainable solutions.

Method: SPADE converts time-series data to textual format and uses ChatGPT-4.1 with domain-informed prompts for zero-shot analysis, identifying irrigation events, estimating gains, detecting anomalies, and generating structured reports.

Result: SPADE outperformed existing methods in anomaly detection with higher recall and F1 scores, accurately classified anomaly types, and achieved high precision/recall in irrigation event detection on real-world farm data.

Conclusion: LLMs like ChatGPT-4.1 show strong potential as scalable, adaptable tools for precision agriculture, integrating qualitative knowledge with data-driven reasoning to provide actionable soil moisture insights.

Abstract: Accurate interpretation of soil moisture patterns is critical for irrigation
scheduling and crop management, yet existing approaches for soil moisture
time-series analysis either rely on threshold-based rules or data-hungry
machine learning or deep learning models that are limited in adaptability and
interpretability. In this study, we introduce SPADE (Soil moisture Pattern and
Anomaly DEtection), an integrated framework that leverages large language
models (LLMs) to jointly detect irrigation patterns and anomalies in soil
moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced
reasoning and instruction-following capabilities, enabling zero-shot analysis
without requiring task-specific annotation or fine-tuning. By converting
time-series data into a textual representation and designing domain-informed
prompt templates, SPADE identifies irrigation events, estimates net irrigation
gains, detects, classifies anomalies, and produces structured, interpretable
reports. Experiments were conducted on real-world soil moisture sensor data
from commercial and experimental farms cultivating multiple crops across the
United States. Results demonstrate that SPADE outperforms the existing method
in anomaly detection, achieving higher recall and F1 scores and accurately
classifying anomaly types. Furthermore, SPADE achieved high precision and
recall in detecting irrigation events, indicating its strong capability to
capture irrigation patterns accurately. SPADE's reports provide
interpretability and usability of soil moisture analytics. This study
highlights the potential of LLMs as scalable, adaptable tools for precision
agriculture, which is capable of integrating qualitative knowledge and
data-driven reasoning to produce actionable insights for accurate soil moisture
monitoring and improved irrigation scheduling from soil moisture time-series
data.

</details>


### [186] [Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI](https://arxiv.org/abs/2509.18132)
*Xiuyi Fan*

Main category: cs.AI

TL;DR: Proposes Explainable Uncertainty Estimation (XUE) to bridge the gap between explainable AI and uncertainty quantification in medical AI systems, enabling clinically meaningful confidence communication.


<details>
  <summary>Details</summary>
Motivation: Current medical AI systems fail to explicitly quantify or communicate uncertainty in ways that align with clinical reasoning, limiting AI adoption in medicine due to the disconnect between explainability and uncertainty estimation.

Method: Systematically maps medical uncertainty to AI uncertainty concepts, identifies implementation challenges, and outlines technical directions including multimodal uncertainty quantification, model-agnostic visualization techniques, and uncertainty-aware decision support systems.

Result: The analysis highlights the need for AI systems that generate reliable predictions while articulating confidence levels in clinically meaningful ways, contributing to trustworthy medical AI development.

Conclusion: XUE bridges explainability and uncertainty, paving the way for AI systems aligned with real-world clinical complexities by providing guiding principles for effective implementation.

Abstract: Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.

</details>


### [187] [HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics](https://arxiv.org/abs/2509.18168)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: HSGM is a hierarchical framework that reduces quadratic complexity in long document semantic parsing by decomposing text into segments, building local semantic graphs, and creating a global graph memory with incremental updates.


<details>
  <summary>Details</summary>
Motivation: Semantic parsing of long documents faces challenges with quadratic growth in pairwise composition and memory requirements, making it computationally expensive for ultra-long texts.

Method: Hierarchical Segment-Graph Memory (HSGM) decomposes input into meaningful segments, constructs Local Semantic Graphs on each segment, extracts summary nodes to form a Global Graph Memory, and supports incremental updates with hierarchical query processing.

Result: HSGM achieves 2-4× inference speedup, >60% reduction in peak memory, and ≥95% of baseline accuracy on long-document AMR parsing, semantic role labeling, and legal event extraction benchmarks.

Conclusion: HSGM enables scalable, accurate semantic modeling for ultra-long texts, supporting real-time and resource-constrained NLP applications by reducing worst-case complexity from O(N²) to O(Nk + (N/k)²).

Abstract: Semantic parsing of long documents remains challenging due to quadratic
growth in pairwise composition and memory requirements. We introduce
\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that
decomposes an input of length $N$ into $M$ meaningful segments, constructs
\emph{Local Semantic Graphs} on each segment, and extracts compact
\emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports
\emph{incremental updates} -- only newly arrived segments incur local graph
construction and summary-node integration -- while \emph{Hierarchical Query
Processing} locates relevant segments via top-$K$ retrieval over summary nodes
and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to
$O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive
Frobenius-norm bounds on the approximation error introduced by node
summarization and sparsification thresholds. Empirically, on three benchmarks
-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),
and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference
speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of
baseline accuracy. Our approach unlocks scalable, accurate semantic modeling
for ultra-long texts, enabling real-time and resource-constrained NLP
applications.

</details>


### [188] [Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM](https://arxiv.org/abs/2509.18178)
*Ling Yue,Nithin Somasekharan,Tingwen Zhang,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent is a multi-agent framework that automates the entire OpenFOAM CFD workflow from natural language prompts, achieving 88.2% success rate on benchmark tests.


<details>
  <summary>Details</summary>
Motivation: CFD simulation tools like OpenFOAM have steep learning curves and complex manual setup processes that create significant barriers for users.

Method: Uses a multi-agent framework with Model Context Protocol (MCP) for composable services, Hierarchical Multi-Index RAG for context retrieval, and dependency-aware generation for configuration consistency. Handles pre-processing, meshing, HPC script generation, and post-simulation visualization.

Result: Achieved 88.2% success rate on 110 simulation tasks, significantly outperforming MetaOpenFOAM (55.5%).

Conclusion: Foam-Agent dramatically lowers the expertise barrier for CFD and demonstrates how specialized multi-agent systems can democratize complex scientific computing.

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.

</details>


### [189] [Large Language Models and Operations Research: A Structured Survey](https://arxiv.org/abs/2509.18180)
*Yang Wang,Kai Li*

Main category: cs.AI

TL;DR: This survey paper explores how large language models (LLMs) can enhance operations research (OR) by addressing limitations of traditional expert-based approaches through automatic modeling, auxiliary optimization, and direct solving methods.


<details>
  <summary>Details</summary>
Motivation: Traditional OR methods struggle with large-scale, dynamic, and multi-constraint problems due to reliance on expert modeling and manual parameter adjustment. LLMs offer potential solutions through semantic understanding and reasoning capabilities.

Method: The paper organizes LLM-OR integration into three main directions: automatic modeling (translating natural language to mathematical models/code), auxiliary optimization (generating heuristics and evolving algorithms), and direct solving of optimization tasks.

Result: The survey reviews recent progress, evaluation benchmarks, and domain-specific applications, identifying key challenges including unstable semantic-to-structure mapping, fragmented research, limited generalization, and insufficient evaluation systems.

Conclusion: The paper outlines future research avenues for advancing LLMs' role in OR, suggesting potential pathways to overcome current limitations and enhance the integration of AI techniques with traditional operations research methodologies.

Abstract: Operations research (OR) provides fundamental methodologies for complex
system decision-making, with established applications in transportation, supply
chain management, and production scheduling. Traditional approaches, which
depend on expert-based modeling and manual parameter adjustment, often face
challenges in handling large-scale, dynamic, and multi-constraint problems.
Recently, large language models (LLMs) have shown potential to address these
limitations through semantic understanding, structured generation, and
reasoning control. LLMs can translate natural language descriptions into
mathematical models or executable code, generate heuristics, evolve algorithms,
and directly tackle optimization tasks. This paper surveys recent progress on
the integration of LLMs into OR, organizing methods into three main directions:
automatic modeling, auxiliary optimization, and direct solving. It further
reviews evaluation benchmarks and domain-specific applications, and summarizes
key open issues such as unstable semantic-to-structure mapping, fragmented
research progress, limited generalization, and insufficient evaluation systems.
Finally, the survey outlines possible research avenues for advancing the role
of LLMs in OR.

</details>


### [190] [Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling](https://arxiv.org/abs/2509.18181)
*Mustafa Sameen,Xiaojian Zhang,Xilei Zhao*

Main category: cs.AI

TL;DR: The paper introduces SAPA framework that uses LLMs to synthesize psychological attitudes from travel data to improve ridesourcing mode choice prediction, achieving 75.9% improvement in PR-AUC.


<details>
  <summary>Details</summary>
Motivation: Existing ridesourcing mode choice models have limited accuracy due to inability to capture psychological factors and severe class imbalance issues, as ridesourcing trips are rare in daily travel.

Method: SAPA framework uses LLMs to generate traveler personas from survey data, trains propensity-score models, assigns quantitative scores to latent variables, and integrates these with trip attributes in a final classifier.

Result: Experiments on large-scale multi-year travel surveys show SAPA outperforms state-of-the-art baselines by up to 75.9% in PR-AUC on held-out test sets.

Conclusion: SAPA provides an accurate tool for ridesourcing prediction and offers a transferable methodology for various applications.

Abstract: Accurate modeling of ridesourcing mode choices is essential for designing and
implementing effective traffic management policies for reducing congestion,
improving mobility, and allocating resources more efficiently. Existing models
for predicting ridesourcing mode choices often suffer from limited predictive
accuracy due to their inability to capture key psychological factors, and are
further challenged by severe class imbalance, as ridesourcing trips comprise
only a small fraction of individuals' daily travel. To address these
limitations, this paper introduces the Synthesizing Attitudes, Predicting
Actions (SAPA) framework, a hierarchical approach that uses Large Language
Models (LLMs) to synthesize theory-grounded latent attitudes to predict
ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler
personas from raw travel survey data and then trains a propensity-score model
on demographic and behavioral features, enriched by those personas, to produce
an individual-level score. Next, the LLM assigns quantitative scores to
theory-driven latent variables (e.g., time and cost sensitivity), and a final
classifier integrates the propensity score, latent-variable scores (with their
interaction terms), and observable trip attributes to predict ridesourcing mode
choice. Experiments on a large-scale, multi-year travel survey show that SAPA
significantly outperforms state-of-the-art baselines, improving ridesourcing
choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.
This study provides a powerful tool for accurately predicting ridesourcing mode
choices, and provides a methodology that is readily transferable to various
applications.

</details>


### [191] [An Outcome-Based Educational Recommender System](https://arxiv.org/abs/2509.18186)
*Nursultan Askarbekuly,Timur Fayzrakhmanov,Sladjan Babarogić,Ivan Luković*

Main category: cs.AI

TL;DR: OBER is an Outcome-Based Educational Recommender that evaluates recommendation algorithms based on learning mastery rather than just clicks or ratings, using a unified framework that integrates learning outcomes and assessment directly into the data schema.


<details>
  <summary>Details</summary>
Motivation: Most educational recommender systems focus on click- or rating-based relevance metrics, which don't measure true pedagogical impact or learning outcomes. There's a need to evaluate recommendations based on actual mastery they foster.

Method: OBER uses a minimalist entity-relation model, log-driven mastery formula, and plug-in architecture. It was tested in a two-week randomized split test with over 5,700 learners comparing fixed expert trajectory, collaborative filtering, and knowledge-based filtering methods.

Result: Collaborative filtering maximized learner retention, but the fixed expert path achieved the highest mastery levels. The framework successfully derived business, relevance, and learning metrics from the same logs.

Conclusion: OBER provides a method-agnostic framework that allows practitioners to balance relevance and engagement against outcome mastery without extra testing overhead, and is extensible to future adaptive or context-aware recommenders.

Abstract: Most educational recommender systems are tuned and judged on click- or
rating-based relevance, leaving their true pedagogical impact unclear. We
introduce OBER-an Outcome-Based Educational Recommender that embeds learning
outcomes and assessment items directly into the data schema, so any algorithm
can be evaluated on the mastery it fosters. OBER uses a minimalist
entity-relation model, a log-driven mastery formula, and a plug-in
architecture. Integrated into an e-learning system in non-formal domain, it was
evaluated trough a two-week randomized split test with over 5 700 learners
across three methods: fixed expert trajectory, collaborative filtering (CF),
and knowledge-based (KB) filtering. CF maximized retention, but the fixed path
achieved the highest mastery. Because OBER derives business, relevance, and
learning metrics from the same logs, it lets practitioners weigh relevance and
engagement against outcome mastery with no extra testing overhead. The
framework is method-agnostic and readily extensible to future adaptive or
context-aware recommenders.

</details>


### [192] [MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation](https://arxiv.org/abs/2509.18198)
*Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin*

Main category: cs.AI

TL;DR: MMCD framework enhances autonomous vehicle safety through multi-modal collaborative decision-making and cross-modal knowledge distillation to handle missing data modalities.


<details>
  <summary>Details</summary>
Motivation: Address limitations of single vehicle systems with limited sensor range and practical challenges of sensor failures/missing connected vehicles in multi-modal approaches.

Method: Fuses multi-modal observations from ego and collaborative vehicles using teacher-student model with cross-modal knowledge distillation for robustness to missing modalities.

Result: Improves driving safety by up to 20.7% in connected autonomous driving and aerial-ground vehicle collaboration scenarios.

Conclusion: MMCD framework effectively enhances decision-making under challenging conditions and outperforms existing baselines in accident detection and safe driving.

Abstract: Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.

</details>


### [193] [Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations](https://arxiv.org/abs/2509.18215)
*Timotheus Kampik,Kristijonas Čyras,José Ruiz Alarcón*

Main category: cs.AI

TL;DR: A formal approach for explaining changes in inference within Quantitative Bipolar Argumentation Frameworks (QBAFs) by tracing strength inconsistencies in partial orders over argument strengths.


<details>
  <summary>Details</summary>
Motivation: To provide explanations for changes in conclusions when updating QBAFs and drawing new inferences, specifically focusing on inconsistencies in argument strength rankings.

Method: Defines strength inconsistencies, traces their causes to specific arguments, identifies sufficient/necessary/counterfactual explanations, and develops a heuristic-based search approach with implementation.

Result: Shows that strength inconsistency explanations exist if and only if an update leads to strength inconsistency, and provides an implementation for finding these explanations.

Conclusion: The approach enables systematic explanation of inference changes in QBAFs through strength inconsistency analysis, with practical implementation support.

Abstract: This paper presents a formal approach to explaining change of inference in
Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions
from a QBAF and updating the QBAF to then again draw conclusions (and so on),
our approach traces changes -- which we call strength inconsistencies -- in the
partial order over argument strengths that a semantics establishes on some
arguments of interest, called topic arguments. We trace the causes of strength
inconsistencies to specific arguments, which then serve as explanations. We
identify sufficient, necessary, and counterfactual explanations for strength
inconsistencies and show that strength inconsistency explanations exist if and
only if an update leads to strength inconsistency. We define a heuristic-based
approach to facilitate the search for strength inconsistency explanations, for
which we also provide an implementation.

</details>


### [194] [nDNA -- the Semantic Helix of Artificial Cognition](https://arxiv.org/abs/2509.18216)
*Amitava Das*

Main category: cs.AI

TL;DR: Neural DNA (nDNA) is proposed as a semantic-genotypic representation that captures AI models' latent cognitive identity through intrinsic geometry of belief, enabling lineage tracing and evolutionary analysis of artificial cognition.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks measure AI model behavior but fail to capture their internal cognitive identity. The authors aim to understand what shapes models' 'soul' beyond mere output fluency by examining their latent geometry.

Method: nDNA synthesizes three dimensions of latent geometry: spectral curvature (conceptual flow curvature across layers), thermodynamic length (semantic effort for representational transitions), and belief vector field (semantic torsion fields guiding belief orientations).

Result: The approach yields a stable, coordinate-free neural DNA fingerprint that enables tracing model lineages across pretraining, fine-tuning, alignment, pruning, distillation, and merges while detecting drift and measuring inheritance.

Conclusion: This work establishes Neural Genomics as a new field where AI models are treated as digital semantic organisms with traceable inner cognition, enabling comparative analysis, risk diagnosis, and governance of artificial cognitive evolution.

Abstract: As AI foundation models grow in capability, a deeper question emerges: What
shapes their internal cognitive identity -- beyond fluency and output?
Benchmarks measure behavior, but the soul of a model resides in its latent
geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic
representation that captures this latent identity through the intrinsic
geometry of belief. At its core, nDNA is synthesized from three principled and
indispensable dimensions of latent geometry: spectral curvature, which reveals
the curvature of conceptual flow across layers; thermodynamic length, which
quantifies the semantic effort required to traverse representational
transitions through layers; and belief vector field, which delineates the
semantic torsion fields that guide a model's belief directional orientations.
Like biological DNA, it encodes ancestry, mutation, and semantic inheritance,
found in finetuning and alignment scars, cultural imprints, and architectural
drift. In naming it, we open a new field: Neural Genomics, where models are not
just tools, but digital semantic organisms with traceable inner cognition.
  Modeling statement. We read AI foundation models as semantic fluid--dynamics:
meaning is transported through layers like fluid in a shaped conduit; nDNA is
the physics-grade readout of that flow -- a geometry-first measure of how
meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free
neural DNA fingerprint tied to on-input behavior; with this fingerprint we
cross into biology: tracing lineages across pretraining, fine-tuning,
alignment, pruning, distillation, and merges; measuring inheritance between
checkpoints; detecting drift as traits shift under new data or objectives; and,
ultimately, studying the evolution of artificial cognition to compare models,
diagnose risks, and govern change over time.

</details>


### [195] [Similarity Field Theory: A Mathematical Framework for Intelligence](https://arxiv.org/abs/2509.18218)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: Similarity Field Theory is a mathematical framework that formalizes similarity relations and their evolution, providing a structural basis for dynamic systems and a generative definition of intelligence.


<details>
  <summary>Details</summary>
Motivation: To establish a foundational mathematical framework for understanding similarity relations and their evolution in dynamic systems, particularly for characterizing intelligent systems.

Method: Defines similarity fields over entities, system evolution sequences, concepts as fibers, and generative operators. Proves theorems about asymmetry and stability constraints.

Result: Developed Similarity Field Theory with formal definitions and theorems, providing tools to characterize intelligent systems and interpret large language models.

Conclusion: The framework offers a foundational language for comparing and constructing intelligent systems, with applications to understanding societal cognition through models like LLMs.

Abstract: We posit that persisting and transforming similarity relations form the
structural basis of any comprehensible dynamic system. This paper introduces
Similarity Field Theory, a mathematical framework that formalizes the
principles governing similarity values among entities and their evolution. We
define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of
entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed
relational field (asymmetry and non-transitivity are allowed); (2) the
evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by
$p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers
$F_{\alpha}(K) = { E \in U \mid S(E,K) \ge \alpha }$, i.e., superlevel sets of
the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that
produces new entities. Within this framework, we formalize a generative
definition of intelligence: an operator $G$ is intelligent with respect to a
concept $K$ if, given a system containing entities belonging to the fiber of
$K$, it generates new entities that also belong to that fiber. Similarity Field
Theory thus offers a foundational language for characterizing, comparing, and
constructing intelligent systems. We prove two theorems: (i) asymmetry blocks
mutual inclusion; and (ii) stability requires either an anchor coordinate or
eventual confinement within a level set of $f$. These results ensure that the
evolution of similarity fields is both constrained and interpretable,
culminating in an exploration of how the framework allows us to interpret large
language models and use them as experimental probes into societal cognition.

</details>


### [196] [Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models](https://arxiv.org/abs/2509.18221)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TL;DR: VL-RiskFormer is a hierarchical multimodal Transformer with LLM inference head for proactive health risk prediction using medical imaging, clinical narratives, and wearable data.


<details>
  <summary>Details</summary>
Motivation: Address the rising global burden of chronic diseases and handle multimodal, heterogeneous clinical data through a unified AI framework for individual health risk prediction.

Method: Dual-stream architecture with four innovations: cross-modal pre-training with momentum encoders and debiased InfoNCE losses, time fusion block for irregular visit sequences, disease ontology map adapter with ICD-10 codes and graph attention mechanism.

Result: On MIMIC-IV longitudinal cohort, achieved average AUROC of 0.90 with expected calibration error of 2.7%.

Conclusion: VL-RiskFormer demonstrates effective multimodal integration for proactive health risk prediction with strong performance on real-world clinical data.

Abstract: With the rising global burden of chronic diseases and the multimodal and
heterogeneous clinical data (medical imaging, free-text recordings, wearable
sensor streams, etc.), there is an urgent need for a unified multimodal AI
framework that can proactively predict individual health risks. We propose
VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer
with a large language model (LLM) inference head embedded in its top layer. The
system builds on the dual-stream architecture of existing visual-linguistic
models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with
cross-modal comparison and fine-grained alignment of radiological images,
fundus maps, and wearable device photos with corresponding clinical narratives
using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion
block that integrates irregular visit sequences into the causal Transformer
decoder through adaptive time interval position coding; (iii) a disease
ontology map adapter that injects ICD-10 codes into visual and textual channels
in layers and infers comorbid patterns with the help of a graph attention
mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an
average AUROC of 0.90 with an expected calibration error of 2.7 percent.

</details>


### [197] [From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation](https://arxiv.org/abs/2509.18226)
*Yu Fu,Linyue Cai,Ruoyu Wu,Yong Zhao*

Main category: cs.AI

TL;DR: ChefMind is a hybrid architecture combining Chain of Exploration, Knowledge Graph, Retrieval-Augmented Generation, and LLM to address challenges in personalized recipe recommendation, achieving superior performance over baseline models.


<details>
  <summary>Details</summary>
Motivation: Personalized recipe recommendation faces challenges in handling fuzzy user intent, ensuring semantic accuracy, and providing sufficient detail coverage.

Method: Proposes ChefMind, a hybrid architecture combining Chain of Exploration (CoE) to refine ambiguous queries, Knowledge Graph (KG) for semantic reasoning, Retrieval-Augmented Generation (RAG) for contextual details, and LLM to integrate outputs into coherent recommendations.

Result: Evaluation on Xiachufang dataset and manually annotated queries shows ChefMind achieves superior performance with average score of 8.7 vs 6.4-6.7 for ablation models, and reduces unprocessed queries to 1.6%.

Conclusion: ChefMind demonstrates robust performance in handling fuzzy user demands and outperforms individual component models in accuracy, relevance, completeness, and clarity.

Abstract: Personalized recipe recommendation faces challenges in handling fuzzy user
intent, ensuring semantic accuracy, and providing sufficient detail coverage.
We propose ChefMind, a hybrid architecture combining Chain of Exploration
(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large
Language Model (LLM). CoE refines ambiguous queries into structured conditions,
KG offers semantic reasoning and interpretability, RAG supplements contextual
culinary details, and LLM integrates outputs into coherent recommendations. We
evaluate ChefMind on the Xiachufang dataset and manually annotated queries,
comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that
ChefMind achieves superior performance in accuracy, relevance, completeness,
and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.
Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in
handling fuzzy demands.

</details>


### [198] [An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems](https://arxiv.org/abs/2509.18229)
*Anthony Patera,Rohan Abeyaratne*

Main category: cs.AI

TL;DR: The paper introduces an "N-Plus-1" GPT Agency to improve reliability of mechanical engineering problem solving by running multiple GPT instances and comparing results, achieving higher accuracy through ensemble methods.


<details>
  <summary>Details</summary>
Motivation: GPT produces unreliable solutions for mechanical engineering problems with only 85% success probability, making it unsuitable for education and engineering practice without reliability improvements.

Method: An agency launches N independent GPT instances (Agent Solve) to generate solutions, then uses Agent Compare to summarize, compare, and recommend the best solution based on Condorcet's Jury Theorem principles.

Result: The ensemble approach significantly improves reliability when per-instance success probability exceeds 50%, with Agent Compare able to incorporate alternative interpretations and solution procedures from secondary solutions.

Conclusion: The N-Plus-1 Agency provides a transparent, pedagogical framework that outperforms single GPT instances and compares favorably to commercial multi-agent models like Grok Heavy, focusing on educational value.

Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.

</details>


### [199] [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)
*Zihan Dong,Xinyu Fan,Zixiang Tang,Yunqing Li*

Main category: cs.AI

TL;DR: ComputerAgent introduces a lightweight hierarchical reinforcement learning framework for desktop application control that outperforms large MLLMs while being dramatically smaller and faster.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM-based approaches for desktop automation suffer from high latency, poor efficiency on long-horizon tasks, and inability to run on-device due to large model sizes.

Method: Uses a two-level hierarchical RL framework with manager and subpolicy options, triple-modal state encoder (screenshot, task ID, numeric state), meta-actions with early-stop mechanism, and compact vision backbone with small policy networks (15M parameters total).

Result: Achieves 92.1% success on simple tasks (<8 steps) and 58.8% on hard tasks (≥8 steps), matching/exceeding 200B-parameter MLLM baselines while reducing model size by 4 orders of magnitude and halving inference time.

Conclusion: Hierarchical RL provides a practical, scalable alternative to monolithic MLLM-based automation for computer control, enabling efficient on-device deployment.

Abstract: Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.

</details>


### [200] [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
*Yu Gu,Jingjing Fu,Xiaodong Liu,Jeya Maria Jose Valanarasu,Noel Codella,Reuben Tan,Qianchu Liu,Ying Jin,Sheng Zhang,Jinyu Wang,Rui Wang,Lei Song,Guanghui Qin,Naoto Usuyama,Cliff Wong,Cheng Hao,Hohin Lee,Praneeth Sanapathi,Sarah Hilado,Bian Jiang,Javier Alvarez-Valle,Mu Wei,Jianfeng Gao,Eric Horvitz,Matt Lungren,Hoifung Poon,Paul Vozila*

Main category: cs.AI

TL;DR: Medical AI benchmarks reward test-taking tricks over genuine understanding, as models show brittleness and shortcut learning despite high scores.


<details>
  <summary>Details</summary>
Motivation: To expose how current medical benchmarks fail to measure true medical understanding and real-world readiness of AI systems, revealing that high scores mask significant brittleness.

Method: Evaluated six flagship models across six widely used medical benchmarks using stress tests (removing key inputs, prompt changes) and clinician-guided rubric evaluation.

Result: Leading systems guess correctly even without key inputs, flip answers under trivial prompt changes, fabricate flawed reasoning, and benchmarks vary widely in what they truly measure.

Conclusion: Medical benchmark scores do not reflect real-world readiness; trust in healthcare AI requires demanding robustness, sound reasoning, and alignment with real medical demands beyond leaderboard wins.

Abstract: Large frontier models like GPT-5 now achieve top scores on medical
benchmarks. But our stress tests tell a different story. Leading systems often
guess correctly even when key inputs like images are removed, flip answers
under trivial prompt changes, and fabricate convincing yet flawed reasoning.
These aren't glitches; they expose how today's benchmarks reward test-taking
tricks over medical understanding. We evaluate six flagship models across six
widely used benchmarks and find that high leaderboard scores hide brittleness
and shortcut learning. Through clinician-guided rubric evaluation, we show that
benchmarks vary widely in what they truly measure yet are treated
interchangeably, masking failure modes. We caution that medical benchmark
scores do not directly reflect real-world readiness. If we want AI to earn
trust in healthcare, we must demand more than leaderboard wins and must hold
systems accountable for robustness, sound reasoning, and alignment with real
medical demands.

</details>


### [201] [Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints](https://arxiv.org/abs/2509.18382)
*Adarsha Balaji,Le Chen,Rajeev Thakur,Franck Cappello,Sandeep Madireddy*

Main category: cs.AI

TL;DR: This paper investigates compute constraint strategies (reasoning length constraint and model quantization) to reduce computational costs of reasoning language models while studying their impact on safety performance.


<details>
  <summary>Details</summary>
Motivation: Test-time compute scaling improves reasoning model performance but significantly increases computational costs. The research aims to find methods to reduce compute demand while maintaining or understanding the impact on safety.

Method: Two approaches: (1) fine-tuning reasoning models using length controlled policy optimization (LCPO) reinforcement learning to satisfy user-defined CoT reasoning length constraints, and (2) applying quantization to maximize CoT sequence generation within user-defined compute constraints.

Result: The paper studies the trade-off between computational efficiency and model safety when applying these compute constraint strategies.

Conclusion: Compute constraint strategies like reasoning length control and quantization can help reduce computational costs of reasoning models, but their impact on safety performance needs careful consideration and trade-off analysis.

Abstract: Test-time compute scaling has demonstrated the ability to improve the
performance of reasoning language models by generating longer chain-of-thought
(CoT) sequences. However, this increase in performance comes with a significant
increase in computational cost. In this work, we investigate two compute
constraint strategies: (1) reasoning length constraint and (2) model
quantization, as methods to reduce the compute demand of reasoning models and
study their impact on their safety performance. Specifically, we explore two
approaches to apply compute constraints to reasoning models: (1) fine-tuning
reasoning models using a length controlled policy optimization (LCPO) based
reinforcement learning method to satisfy a user-defined CoT reasoning length,
and (2) applying quantization to maximize the generation of CoT sequences
within a user-defined compute constraint. Furthermore, we study the trade-off
between the computational efficiency and the safety of the model.

</details>


### [202] [Gödel Test: Can Large Language Models Solve Easy Conjectures?](https://arxiv.org/abs/2509.18383)
*Moran Feldman,Amin Karbasi*

Main category: cs.AI

TL;DR: The paper proposes the Gödel Test to evaluate if large language models can solve new, simple mathematical conjectures, testing GPT-5 on five combinatorial optimization problems with mixed results showing progress but limitations.


<details>
  <summary>Details</summary>
Motivation: To determine whether large language models can solve previously unsolved mathematical conjectures beyond standard competition problems, assessing their capability for original mathematical reasoning.

Method: Evaluated GPT-5 on five conjectures in combinatorial optimization by providing source papers and assessing the model's proof generation and reasoning capabilities for each problem.

Result: GPT-5 succeeded on three easier problems (producing nearly correct solutions), refuted one conjecture with a valid alternative, failed on cross-paper synthesis, and struggled with harder analysis despite proposing correct algorithms.

Conclusion: GPT-5 shows meaningful progress in routine reasoning and occasional originality but has clear limitations in complex synthesis tasks, representing an early step toward models potentially passing the Gödel Test.

Abstract: Recent announcements from frontier AI model labs have highlighted strong
results on high-school and undergraduate math competitions. Yet it remains
unclear whether large language models can solve new, simple conjectures in more
advanced areas of mathematics. We propose the G\"odel Test: evaluating whether
a model can produce correct proofs for very simple, previously unsolved
conjectures. To this end, we study the performance of GPT-5 on five conjectures
in combinatorial optimization. For each problem, we provided one or two source
papers from which the conjecture arose, withheld our own conjecture, and then
assessed the model's reasoning in detail. On the three easier problems, GPT-5
produced nearly correct solutions; for Problem 2 it even derived a different
approximation guarantee that, upon checking, refuted our conjecture while
providing a valid solution. The model failed on Problem 4, which required
combining results from two papers. On Problem 5, a harder case without a
validated conjecture, GPT-5 proposed the same algorithm we had in mind but
failed in the analysis, suggesting the proof is more challenging than expected.
Although our sample is small, the results point to meaningful progress on
routine reasoning, occasional flashes of originality, and clear limitations
when cross-paper synthesis is required. GPT-5 may represent an early step
toward frontier models eventually passing the G\"odel Test.

</details>


### [203] [ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification](https://arxiv.org/abs/2509.18400)
*Pritish Yuvraj,Siva Devarakonda*

Main category: cs.AI

TL;DR: This paper introduces the first benchmark for HTS code classification using US Customs data, showing that a fine-tuned LLaMA-3.3-70B model (Atlas) achieves 40% 10-digit accuracy and is significantly cheaper than leading commercial LLMs.


<details>
  <summary>Details</summary>
Motivation: HTS code classification is a critical bottleneck in global trade that has received little ML attention, with misclassification causing major shipping disruptions including postal operators suspending US deliveries.

Method: Created benchmark from US Customs Rulings Online Search System (CROSS), fine-tuned Atlas model (LLaMA-3.3-70B) for HTS classification, and compared against leading LLMs like GPT-5-Thinking and Gemini-2.5-Pro-Thinking.

Result: Atlas achieved 40% fully correct 10-digit classifications and 57.5% correct 6-digit classifications, improving by 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking, while being 5-8 times cheaper.

Conclusion: While Atlas sets a strong baseline, HTS classification remains highly challenging. The authors release dataset and model to establish HTS classification as a community benchmark for future work in retrieval, reasoning, and alignment.

Abstract: Accurate classification of products under the Harmonized Tariff Schedule
(HTS) is a critical bottleneck in global trade, yet it has received little
attention from the machine learning community. Misclassification can halt
shipments entirely, with major postal operators suspending deliveries to the
U.S. due to incomplete customs documentation. We introduce the first benchmark
for HTS code classification, derived from the U.S. Customs Rulings Online
Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned
Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit
classifications and 57.5 percent correct 6-digit classifications, improvements
of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.
Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and
eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to
guarantee data privacy in high-stakes trade and compliance workflows. While
Atlas sets a strong baseline, the benchmark remains highly challenging, with
only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim
to position HTS classification as a new community benchmark task and invite
future work in retrieval, reasoning, and alignment.

</details>


### [204] [Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/abs/2509.18420)
*Nikolai Skripko*

Main category: cs.AI

TL;DR: IFEval-FC is a new benchmark that tests precise instruction following in function calling by evaluating adherence to format specifications embedded in JSON schema descriptions, which existing benchmarks overlook.


<details>
  <summary>Details</summary>
Motivation: Current function calling benchmarks like BFCL, tau^2-Bench, and ACEBench only evaluate argument correctness but fail to test adherence to format instructions in parameter descriptions, creating a gap for real-world AI agent applications.

Method: IFEval-FC encodes verifiable format instructions directly within JSON schema descriptions and includes 750 test cases with algorithmic evaluation to ensure objectivity and scalability.

Result: Even state-of-the-art proprietary models like GPT-5 and Claude 4.1 Opus frequently fail to follow basic formatting rules, revealing a practical limitation for real-world agent systems.

Conclusion: The benchmark highlights the need for better instruction following in function calling and provides an open-source tool (https://github.com/Skripkon/IFEval-FC) to address this gap in AI agent capabilities.

Abstract: Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.

</details>


### [205] [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
*Hongda Jiang,Xinyuan Zhang,Siddhant Garg,Rishab Arora,Shiun-Zu Kuo,Jiayang Xu,Christopher Brossman,Yue Liu,Aaron Colak,Ahmed Aly,Anuj Kumar,Xin Luna Dong*

Main category: cs.AI

TL;DR: Memory-QA is a novel task for answering recall questions about visual content from stored multimodal memories, addressed by the Pensieve pipeline which achieves up to 14% improvement in QA accuracy.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of creating task-oriented memories, effectively using temporal and location information, and leveraging multiple memories for answering recall questions in real-world scenarios.

Method: Proposed Pensieve pipeline with memory-specific augmentation, time- and location-aware multi-signal retrieval, and multi-memory QA fine-tuning.

Result: Superior performance over state-of-the-art solutions with up to 14% improvement on QA accuracy, demonstrated on a created multimodal benchmark.

Conclusion: Pensieve effectively addresses the unique challenges of Memory-QA, showcasing significant improvements in recall question answering through its comprehensive pipeline approach.

Abstract: We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).

</details>


### [206] [FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning](https://arxiv.org/abs/2509.18527)
*Ziwen Chen,Zhong Wang*

Main category: cs.AI

TL;DR: FERA is an AI referee prototype for foil fencing that combines pose-based action recognition with rule-based reasoning to address refereeing challenges like subjectivity and human error.


<details>
  <summary>Details</summary>
Motivation: Fencing faces refereeing challenges including subjective calls, human errors, bias, and limited availability in practice environments, which an AI system could help overcome.

Method: FERA extracts 2D joint positions from video, normalizes them, computes 101-dimensional kinematic features, uses a Transformer for multi-label move and blade classification, and applies a distilled language model with encoded right-of-way rules for decision-making and explanations.

Result: With limited hand-labeled data, 5-fold cross-validation achieves an average macro-F1 score of 0.549, outperforming baselines including TCN, BiLSTM, and vanilla Transformer.

Conclusion: While not deployment-ready, FERA demonstrates a promising path toward automated referee assistance in foil fencing and opens new opportunities for AI applications in fencing coaching.

Abstract: The sport of fencing, like many other sports, faces challenges in refereeing:
subjective calls, human errors, bias, and limited availability in practice
environments. We present FERA (Fencing Referee Assistant), a prototype AI
referee for foil fencing which integrates pose-based multi-label action
recognition and rule-based reasoning. FERA extracts 2D joint positions from
video, normalizes them, computes a 101-dimensional kinematic feature set, and
applies a Transformer for multi-label move and blade classification. To
determine priority and scoring, FERA applies a distilled language model with
encoded right-of-way rules, producing both a decision and an explanation for
each exchange. With limited hand-labeled data, a 5-fold cross-validation
achieves an average macro-F1 score of 0.549, outperforming multiple baselines,
including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla
Transformer. While not ready for deployment, these results demonstrate a
promising path towards automated referee assistance in foil fencing and new
opportunities for AI applications, such as coaching in the field of fencing.

</details>


### [207] [LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs](https://arxiv.org/abs/2509.18557)
*Tom Pawelek,Raj Patel,Charlotte Crowell,Noorbakhsh Amiri,Sudip Mittal,Shahram Rahimi,Andy Perkins*

Main category: cs.AI

TL;DR: LLMZ+ is a security framework that uses prompt whitelisting instead of detection-based approaches to protect agentic LLMs from jailbreak attacks, ensuring only contextually appropriate messages interact with the AI.


<details>
  <summary>Details</summary>
Motivation: Agentic AI systems have privileged access to data and APIs, making them valuable targets for attackers. Their nondeterministic behavior introduces substantial security risks that traditional detection-based defenses cannot adequately address.

Method: LLMZ+ implements prompt whitelisting where only contextually appropriate and safe messages are permitted to interact with the agentic LLM, ensuring all exchanges conform to predefined use cases and operational boundaries.

Result: The approach provides strong resilience against common jailbreak prompts while allowing legitimate business communications to flow seamlessly. Experimental results show false positive and false negative rates can be reduced to 0.

Conclusion: LLMZ+ offers a more streamlined and resilient security framework that reduces resource requirements for sustaining LLM information security compared to traditional detection-based approaches.

Abstract: Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.

</details>


### [208] [Solving Math Word Problems Using Estimation Verification and Equation Generation](https://arxiv.org/abs/2509.18565)
*Mitchell Piehl,Dillon Wilson,Ananya Kalita,Jugal Kalita*

Main category: cs.AI

TL;DR: A novel method that combines LLM-generated equations with external symbolic solvers and verification through estimation to improve Math Word Problem solving, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with Math Word Problems due to limitations in reasoning and mathematical abilities, despite recent prompt improvements. Current methods need better accuracy verification.

Method: First, prompt LLM to create equations from decomposed questions, use external symbolic solver for answers. Then verify by having LLM estimate the answer and compare with generated result. If verification fails, use iterative rectification.

Result: Achieves new state-of-the-art results on numeric and algebraic MWPs, improving previous best by nearly 2% on average. Also obtains satisfactory results on trigonometric MWPs. Introduces two new datasets: SVAMPClean and Trig300.

Conclusion: The proposed verification and rectification approach significantly enhances LLM performance on MWPs, demonstrating effectiveness across different mathematical domains and introducing valuable new testing datasets.

Abstract: Large Language Models (LLMs) excel at various tasks, including
problem-solving and question-answering. However, LLMs often find Math Word
Problems (MWPs) challenging because solving them requires a range of reasoning
and mathematical abilities with which LLMs seem to struggle. Recent efforts
have helped LLMs solve more complex MWPs with improved prompts. This study
proposes a novel method that initially prompts an LLM to create equations from
a decomposition of the question, followed by using an external symbolic
equation solver to produce an answer. To ensure the accuracy of the obtained
answer, inspired by an established recommendation of math teachers, the LLM is
instructed to solve the MWP a second time, but this time with the objective of
estimating the correct answer instead of solving it exactly. The estimation is
then compared to the generated answer to verify. If verification fails, an
iterative rectification process is employed to ensure the correct answer is
eventually found. This approach achieves new state-of-the-art results on
datasets used by prior published research on numeric and algebraic MWPs,
improving the previous best results by nearly two percent on average. In
addition, the approach obtains satisfactory results on trigonometric MWPs, a
task not previously attempted to the authors' best knowledge. This study also
introduces two new datasets, SVAMPClean and Trig300, to further advance the
testing of LLMs' reasoning abilities.

</details>


### [209] [Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents](https://arxiv.org/abs/2509.18633)
*Yara Mohajerani*

Main category: cs.AI

TL;DR: A geospatial agent-based model integrating climate hazard data with evolutionary learning for economic agents, showing how firms adapt to climate risks and revealing systemic supply chain impacts.


<details>
  <summary>Details</summary>
Motivation: Climate risk assessment requires modelling complex interactions between spatially heterogeneous hazards and adaptive economic systems, which existing approaches struggle to capture.

Method: Combines Mesa-based spatial modelling with CLIMADA climate impact assessment, introducing adaptive learning behaviors where firms evolve strategies for budget allocation, pricing, wages, and risk adaptation through fitness-based selection and mutation.

Result: Evolutionary adaptation enables firms to converge with baseline production levels after decades of disruption; systemic risks cause even non-exposed agents to face impacts through supply chain disruptions, with end-of-century average prices 5.6% higher under RCP8.5.

Conclusion: This open-source framework provides financial institutions and companies with tools to quantify both direct and cascading climate risks while evaluating cost-effective adaptation strategies.

Abstract: Climate risk assessment requires modelling complex interactions between
spatially heterogeneous hazards and adaptive economic systems. We present a
novel geospatial agent-based model that integrates climate hazard data with
evolutionary learning for economic agents. Our framework combines Mesa-based
spatial modelling with CLIMADA climate impact assessment, introducing adaptive
learning behaviours that allow firms to evolve strategies for budget
allocation, pricing, wages, and risk adaptation through fitness-based selection
and mutation. We demonstrate the framework using riverine flood projections
under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to
converge with baseline (no hazard) production levels after decades of
disruption due to climate stress. Our results reveal systemic risks where even
agents that are not directly exposed to floods face impacts through supply
chain disruptions, with the end-of-century average price of goods 5.6% higher
under RCP8.5 compared to the baseline. This open-source framework provides
financial institutions and companies with tools to quantify both direct and
cascading climate risks while evaluating cost-effective adaptation strategies.

</details>


### [210] [TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2509.18667)
*Qiao Xiao,Hong Ting Tsang,Jiaxin Bai*

Main category: cs.AI

TL;DR: TERAG is a cost-effective graph-based RAG framework that achieves 80% accuracy of existing methods while using only 3%-11% of output tokens by incorporating Personalized PageRank.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based RAG systems have high LLM token usage costs during graph construction, which limits large-scale adoption. The goal is to reduce costs while maintaining performance.

Method: Proposes TERAG framework that incorporates Personalized PageRank (PPR) during retrieval phase, inspired by HippoRAG, to build informative graphs with significantly lower token consumption.

Result: TERAG achieves at least 80% accuracy compared to widely used graph-based RAG methods while consuming only 3%-11% of the output tokens.

Conclusion: TERAG provides a simple yet effective solution for cost-efficient graph-based RAG, enabling large-scale adoption by dramatically reducing token usage while maintaining competitive accuracy.

Abstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.

</details>


### [211] [Implementation of airborne ML models with semantics preservation](https://arxiv.org/abs/2509.18681)
*Nicolas Valot,Louis Fabre,Benjamin Lesage,Ammar Mechouche,Claire Pagetti*

Main category: cs.AI

TL;DR: This paper clarifies the distinction between ML models and their unambiguous descriptions (MLMD), refines semantics preservation for accurate model replication, and applies these concepts to industrial use cases to build and compare target models.


<details>
  <summary>Details</summary>
Motivation: As ML systems are increasingly used in airborne applications, they must demonstrate compliance with safety regulations like EASA guidelines. The paper addresses the need to ensure ML models achieve intended functions and maintain performance in target environments.

Method: The paper defines the difference between ML models and MLMD (Machine Learning Model Description), refines the concept of semantics preservation for accurate model replication, and applies these contributions to industrial use cases.

Result: The approach enables building and comparing several target models through clear ML model descriptions and semantics preservation techniques.

Conclusion: The paper provides a framework for ensuring ML model safety and compliance in airborne systems by establishing clear model descriptions and semantics preservation methods.

Abstract: Machine Learning (ML) may offer new capabilities in airborne systems.
However, as any piece of airborne systems, ML-based systems will be required to
guarantee their safe operation. Thus, their development will have to be
demonstrated to be compliant with the adequate guidance. So far, the European
Union Aviation Safety Agency (EASA) has published a concept paper and an
EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level
objectives to confirm the ML model achieves its intended function and maintains
training performance in the target environment. The paper aims to clarify the
difference between an ML model and its corresponding unambiguous description,
referred to as the Machine Learning Model Description (MLMD). It then refines
the essential notion of semantics preservation to ensure the accurate
replication of the model. We apply our contributions to several industrial use
cases to build and compare several target models.

</details>


### [212] [Advances in Large Language Models for Medicine](https://arxiv.org/abs/2509.18690)
*Zhiyu Kan,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.AI

TL;DR: This paper provides a systematic review of large language models (LLMs) in the medical field, analyzing training techniques, healthcare applications, strengths/limitations, and proposing future research directions.


<details>
  <summary>Details</summary>
Motivation: To highlight the necessity of developing medical LLMs, provide understanding of their current state, and offer guidance for subsequent research in this rapidly advancing AI field.

Method: Systematic review of up-to-date research progress, including innovative categorization of medical LLMs into three types based on training methodologies and classification of evaluation approaches into two categories.

Result: Comprehensive analysis of training techniques for large medical models, their adaptation in healthcare settings, applications, strengths, and limitations.

Conclusion: The study proposes solutions to existing challenges and outlines future research directions to advance the field of medical LLMs.

Abstract: Artificial intelligence (AI) technology has advanced rapidly in recent years,
with large language models (LLMs) emerging as a significant breakthrough. LLMs
are increasingly making an impact across various industries, with the medical
field standing out as the most prominent application area. This paper
systematically reviews the up-to-date research progress of LLMs in the medical
field, providing an in-depth analysis of training techniques for large medical
models, their adaptation in healthcare settings, related applications, as well
as their strengths and limitations. Furthermore, it innovatively categorizes
medical LLMs into three distinct types based on their training methodologies
and classifies their evaluation approaches into two categories. Finally, the
study proposes solutions to existing challenges and outlines future research
directions based on identified issues in the field of medical LLMs. By
systematically reviewing previous and advanced research findings, we aim to
highlight the necessity of developing medical LLMs, provide a deeper
understanding of their current state of development, and offer clear guidance
for subsequent research.

</details>


### [213] [Autonomous Data Agents: A New Opportunity for Smart Data](https://arxiv.org/abs/2509.18710)
*Yanjie Fu,Dongjie Wang,Wangyang Ying,Xiangliang Zhang,Huan Liu,Jian Pei*

Main category: cs.AI

TL;DR: DataAgents represent a paradigm shift using AI agents to autonomously transform complex data into actionable knowledge through task decomposition, action reasoning, and tool calling.


<details>
  <summary>Details</summary>
Motivation: Data preparation and analysis remain labor-intensive despite growing data complexity, and there's a need to optimize how knowledge is packed into data for AI utilization.

Method: DataAgents integrate LLM reasoning with task decomposition, action reasoning, grounding, and tool calling to autonomously handle data operations like preprocessing, transformation, augmentation, and retrieval.

Result: DataAgents can dynamically plan workflows, call powerful tools, and adapt to diverse data tasks at scale, transforming unstructured data into coherent knowledge.

Conclusion: The report calls for advancing workflow optimization, establishing benchmarks, safeguarding privacy, and developing trustworthy guardrails for DataAgents as a critical trend in autonomous data-to-knowledge systems.

Abstract: As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.

</details>


### [214] [Experience Scaling: Post-Deployment Evolution For Large Language Models](https://arxiv.org/abs/2509.18771)
*Xingkun Yin,Kaibin Huang,Dong In Kim,Hongyang Du*

Main category: cs.AI

TL;DR: Experience scaling is a framework for continuous post-deployment evolution of LLMs through autonomous interaction and collaborative experience sharing, enabling sustained performance improvement beyond static training data limitations.


<details>
  <summary>Details</summary>
Motivation: Traditional scaling approaches (model size, training data, compute) are reaching saturation as human-generated text is exhausted and gains diminish, requiring new methods for continuous LLM improvement.

Method: A framework that captures raw interactions, distills them into compact reusable knowledge, and periodically refines stored content to maintain relevance and efficiency through autonomous environmental interaction and collaborative experience sharing.

Result: Validated in simulated real-world scenarios, experience scaling improves accuracy, sustains performance over time, and maintains gains when applied to novel situations across generalization to unseen tasks, repetitive queries, and over-saturated knowledge stores.

Conclusion: Structured post-deployment learning can extend LLM capabilities beyond static human-generated data limitations, offering a scalable path for continued intelligence progress through continuous evolution.

Abstract: Scaling model size, training data, and compute power have driven advances in
large language models (LLMs), but these approaches are reaching saturation as
human-generated text is exhausted and further gains diminish. We propose
experience scaling, a framework for continuous post-deployment evolution for
LLMs through autonomous interaction with the environment and collaborative
sharing of accumulated experience. The framework captures raw interactions,
distills them into compact, reusable knowledge, and periodically refines stored
content to preserve relevance and efficiency. We validate the framework in
simulated real-world scenarios involving generalization to previously unseen
but related tasks, repetitive queries, and over-saturated knowledge stores.
Across all settings, experience scaling improves accuracy, sustains performance
over time, and maintains gains when applied to novel situations. These results
demonstrate that structured post-deployment learning can extend LLM
capabilities beyond the limits of static human-generated data, offering a
scalable path for continued intelligence progress.

</details>


### [215] [The AGNTCY Agent Directory Service: Architecture and Implementation](https://arxiv.org/abs/2509.18787)
*Luca Muscariello,Vijoy Pandey,Ramiz Polic*

Main category: cs.AI

TL;DR: ADS is a distributed directory service for discovering AI agent capabilities using content-addressed storage, hierarchical taxonomies, and cryptographic signing to enable efficient verifiable discovery across Multi-Agent Systems.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient, verifiable, and multi-dimensional discovery of AI agent capabilities across heterogeneous Multi-Agent Systems by decoupling capability indexing from content location.

Method: Built on Open Agentic Schema Framework (OASF) with a two-level mapping over Kademlia-based DHT, leveraging OCI/ORAS infrastructure for artifact distribution, Sigstore for provenance, and supporting schema-driven extensibility for various agent types.

Result: A formal architectural model that provides storage and discovery layers with security and performance properties for agent registry and interoperability.

Conclusion: ADS positions itself as a comprehensive solution within the emerging landscape of agent registry and interoperability initiatives, enabling standardized discovery and integration of diverse AI agent capabilities.

Abstract: The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.

</details>


### [216] [Bounded PCTL Model Checking of Large Language Model Outputs](https://arxiv.org/abs/2509.18836)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: LLMCHECKER is a model-checking-based verification method that uses probabilistic computation tree logic (PCTL) to verify properties of LLM text generation processes by focusing on top-k tokens with cumulative probability threshold α.


<details>
  <summary>Details</summary>
Motivation: To formally verify the consistency and properties of LLM text generation processes, addressing the observation that only limited tokens are typically chosen during generation but not always consistently.

Method: Uses α-k-bounded text generation that focuses on α maximal cumulative probability on top-k tokens at each generation step, accommodating various text quantification methods like quality and bias evaluation.

Result: Successfully demonstrated applicability across multiple LLMs including Llama, Gemma, Mistral, Genstruct, and BERT, showing the first use of PCTL-based model checking for LLM text generation verification.

Conclusion: LLMCHECKER provides a novel formal verification framework for LLM text generation processes, enabling rigorous analysis of consistency and properties through PCTL model checking.

Abstract: In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.

</details>


### [217] [Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning](https://arxiv.org/abs/2509.18846)
*Hong-Jie Dai,Zheng-Hao Li,An-Tai Lu,Bo-Tsz Shain,Ming-Ta Li,Tatheer Hussain Mir,Kuang-Te Wang,Min-I Su,Pei-Kang Liu,Ming-Ju Tsai*

Main category: cs.AI

TL;DR: A modular framework for ICD-10-CM code prediction using LLMs, addressing model selection, data redundancy, and input contextualization through principled methods and structured clinical data.


<details>
  <summary>Details</summary>
Motivation: ICD coding is labor-intensive and error-prone, and current LLM approaches face challenges in model selection, input design, and data redundancy that limit their effectiveness in automated medical coding.

Method: Proposes a framework with LLM-as-judge evaluation using Plackett-Luce aggregation for model selection, embedding-based similarity measures for redundancy-aware data sampling, and structured discharge summaries with section-wise content analysis under different modeling paradigms.

Result: The selected base model after fine-tuning consistently outperforms baseline LLMs in both internal and external evaluations across two institutional datasets, with performance improving when incorporating more clinical sections.

Conclusion: The framework provides a scalable, institution-ready solution for automated medical coding by combining informed model selection, efficient data refinement, and context-aware prompting using open-source LLMs.

Abstract: Accurate International Classification of Diseases (ICD) coding is critical
for clinical documentation, billing, and healthcare analytics, yet it remains a
labour-intensive and error-prone task. Although large language models (LLMs)
show promise in automating ICD coding, their challenges in base model
selection, input contextualization, and training data redundancy limit their
effectiveness. We propose a modular framework for ICD-10 Clinical Modification
(ICD-10-CM) code prediction that addresses these challenges through principled
model selection, redundancy-aware data sampling, and structured input design.
The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce
aggregation to assess and rank open-source LLMs based on their intrinsic
comprehension of ICD-10-CM code definitions. We introduced embedding-based
similarity measures, a redundancy-aware sampling strategy to remove
semantically duplicated discharge summaries. We leverage structured discharge
summaries from Taiwanese hospitals to evaluate contextual effects and examine
section-wise content inclusion under universal and section-specific modelling
paradigms. Experiments across two institutional datasets demonstrate that the
selected base model after fine-tuning consistently outperforms baseline LLMs in
internal and external evaluations. Incorporating more clinical sections
consistently improves prediction performance. This study uses open-source LLMs
to establish a practical and principled approach to ICD-10-CM code prediction.
The proposed framework provides a scalable, institution-ready solution for
real-world deployment of automated medical coding systems by combining informed
model selection, efficient data refinement, and context-aware prompting.

</details>


### [218] [MAPO: Mixed Advantage Policy Optimization](https://arxiv.org/abs/2509.18849)
*Wenke Huang,Quan Zhang,Yiyang Fang,Jian Liang,Xuankun Rong,Huanjin Yao,Guancheng Wan,Ke Liang,Wenwen He,Mingjun Li,Leszek Rutkowski,Mang Ye,Bo Du,Dacheng Tao*

Main category: cs.AI

TL;DR: Proposes Mixed Advantage Policy Optimization (MAPO) to address advantage reversion and mirror problems in GRPO by dynamically reweighting advantage functions based on trajectory certainty.


<details>
  <summary>Details</summary>
Motivation: Existing GRPO methods suffer from advantage reversion and advantage mirror problems that hinder reasonable advantage allocation across different query samples.

Method: Introduces advantage percent deviation for high-certainty trajectories and dynamically reweights advantage functions based on trajectory certainty to adapt to sample-specific characteristics.

Result: Comparison with state-of-the-art methods and ablation studies validate the effectiveness of MAPO in improving foundation model performance on reasoning tasks.

Conclusion: MAPO provides an effective solution to advantage allocation problems in GRPO, enhancing the performance of foundation models through adaptive advantage function configuration.

Abstract: Recent advances in reinforcement learning for foundation models, such as
Group Relative Policy Optimization (GRPO), have significantly improved the
performance of foundation models on reasoning tasks. Notably, the advantage
function serves as a central mechanism in GRPO for ranking the trajectory
importance. However, existing explorations encounter both advantage reversion
and advantage mirror problems, which hinder the reasonable advantage allocation
across different query samples. In this work, we propose an easy but effective
GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the
trajectory appears with different certainty and propose the advantage percent
deviation for samples with high-certainty trajectories. Furthermore, we
dynamically reweight the advantage function for samples with varying trajectory
certainty, thereby adaptively configuring the advantage function to account for
sample-specific characteristics. Comparison with related state-of-the-art
methods, along with ablation studies on different advantage variants, validates
the effectiveness of our approach.

</details>


### [219] [Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling](https://arxiv.org/abs/2509.18864)
*Yingxin Li,Jianbo Zhao,Xueyu Ren,Jie Tang,Wangjie You,Xu Chen,Kan Zhou,Chao Feng,Jiao Ran,Yuan Meng,Zhi Wang*

Main category: cs.AI

TL;DR: ProfileBench is a benchmark for user profiling using LLMs, and Conf-Profile is a confidence-driven framework that synthesizes labels and uses confidence-weighted voting and calibration to improve accuracy without ground-truth labels.


<details>
  <summary>Details</summary>
Motivation: User profiling is crucial for understanding users, but progress is hindered by the lack of benchmarks and challenges in collecting large-scale ground-truth labels, as well as noisy user data affecting LLM reliability.

Method: Conf-Profile uses a two-stage paradigm: first synthesizing high-quality labels with confidence hints from advanced LLMs, then applying confidence-weighted voting and calibration. It distills results into a lightweight LLM and enhances reasoning via confidence-guided unsupervised reinforcement learning.

Result: Experimental results show Conf-Profile improves F1 by 13.97 on Qwen3-8B, demonstrating substantial performance gains through the two-stage training.

Conclusion: Conf-Profile effectively addresses label-free and reliable user profiling by leveraging confidence-driven methods, achieving significant improvements in performance.

Abstract: User profiling, as a core technique for user understanding, aims to infer
structural attributes from user information. Large Language Models (LLMs)
provide a promising avenue for user profiling, yet the progress is hindered by
the lack of comprehensive benchmarks. To bridge this gap, we propose
ProfileBench, an industrial benchmark derived from a real-world video platform,
encompassing heterogeneous user data and a well-structured profiling taxonomy.
However, the profiling task remains challenging due to the difficulty of
collecting large-scale ground-truth labels, and the heterogeneous and noisy
user information can compromise the reliability of LLMs. To approach label-free
and reliable user profiling, we propose a Confidence-driven Profile reasoning
framework Conf-Profile, featuring a two-stage paradigm. We first synthesize
high-quality labels by leveraging advanced LLMs with confidence hints, followed
by confidence-weighted voting for accuracy improvement and confidence
calibration for a balanced distribution. The multiple profile results,
rationales, and confidence scores are aggregated and distilled into a
lightweight LLM. We further enhance the reasoning ability via confidence-guided
unsupervised reinforcement learning, which exploits confidence for difficulty
filtering, quasi-ground truth voting, and reward weighting. Experimental
results demonstrate that Conf-Profile delivers substantial performance through
the two-stage training, improving F1 by 13.97 on Qwen3-8B.

</details>


### [220] [Memory in Large Language Models: Mechanisms, Evaluation and Evolution](https://arxiv.org/abs/2509.18868)
*Dianxing Zhang,Wendong Li,Kani Song,Jiaye Lu,Gang Li,Liuchun Yang,Sheng Li*

Main category: cs.AI

TL;DR: This paper proposes a unified framework for defining, categorizing, and evaluating LLM memory systems, including a taxonomy, evaluation protocols, and governance mechanisms for reproducible research and deployment.


<details>
  <summary>Details</summary>
Motivation: To establish a standardized framework for understanding and comparing LLM memory systems across heterogeneous setups, enabling reproducible evaluation and governance of memory capabilities.

Method: Proposes a four-part memory taxonomy (parametric, contextual, external, procedural/episodic) with a memory quadruple framework, three-setting evaluation protocol, layered evaluation approach, and DMM Gov system for memory governance.

Result: Develops a comprehensive framework that integrates temporal governance, leakage auditing, uncertainty reporting, and testable propositions for LLM memory systems.

Conclusion: The framework provides a reproducible, comparable, and governable coordinate system for LLM memory research and deployment, with four key testable propositions for future validation.

Abstract: Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.

</details>


### [221] [LongCat-Flash-Thinking Technical Report](https://arxiv.org/abs/2509.18883)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chengcheng Han,Chenhui Yang,Chi Zhang,Chong Peng,Chuyu Zhang,Cong Chen,Fengcun Li,Gang Xu,Guoyuan Lin,Hao Jiang,Hao Liang,Haomin Fu,Haoxiang Ma,Hong Liu,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiahao Liu,Jiahuan Li,Jialin Liu,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiaqi Sun,Jiaqi Zhang,Jiarong Shi,Jiawei Yang,Jingang Wang,Jinrui Ding,Jun Kuang,Jun Xu,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Li Wei,Liang Shi,Lin Qiu,Lingbin Kong,Lingchuan Liu,Linsen Guo,Longfei An,Mai Xia,Meng Zhou,Mengshen Zhu,Peng Pei,Pengcheng Jia,Qi Gu,Qi Guo,Qiong Huang,Quan Chen,Quanchi Weng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shanglin Lei,Shuai Du,Shuaikang Liu,Shuang Zhou,Shuhao Hu,Siyu Xu,Songshan Gong,Tao Liang,Tianhao Hu,Wei He,Wei Shi,Wei Wang,Wei Wu,Wei Zhuo,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Xi Su,Xiangcheng Liu,Xiangyu Xi,Xiangzhou Huang,Xiao Liu,Xiaochen Jiang,Xiaowei Shi,Xiaowen Shi,Xiaoyu Li,Xin Chen,Xinyue Zhao,Xuan Huang,Xuemiao Zhang,Xuezhi Cao,Xunliang Cai,Yajie Zhang,Yang Chen,Yang Liu,Yang Liu,Yang Zheng,Yaoming Wang,Yaqi Huo,Yerui Sun,Yifan Lu,Yiyang Li,Youshao Xiao,Yuanzhe Lei,Yuchen Xie,Yueqing Sun,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunke Zhao,Yuqing Ding,Yuwei Jiang,Zhaohua Yang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhongda Su,Ziran Li,Ziwen Wang,Ziyuan Zhuang,Zongyu Wang,Zunyuan Yang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking is a 560B parameter open-source MoE reasoning model trained with long CoT data cold-start and large-scale RL, achieving state-of-the-art performance with exceptional efficiency in agentic reasoning.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient large-scale reasoning model that can handle complex tasks while reducing computational costs, particularly in agentic reasoning scenarios.

Method: Uses a cold-start training strategy with long Chain-of-Thought data, followed by domain-parallel training across STEM, Code, and Agentic domains, fused into a single model. Powered by DORA system for large-scale RL training with 3x speedup.

Result: Achieves SOTA performance among open-source models on complex reasoning tasks. Reduces average token consumption by 64.5% (from 19,653 to 6,965) on AIME-25 without accuracy degradation.

Conclusion: The model demonstrates efficient reasoning capabilities and is released to advance reasoning systems and agentic AI research.

Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.

</details>


### [222] [How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective](https://arxiv.org/abs/2509.18905)
*Songsong Yu,Yuxin Chen,Hao Ju,Lianjie Jia,Fuxi Zhang,Shaofei Huang,Yuhan Wu,Rundi Cui,Binghao Ran,Zaibin Zhang,Zhedong Zheng,Zhipeng Zhang,Yifan Wang,Lin Song,Lijun Wang,Yanwei Li,Ying Shan,Huchuan Lu*

Main category: cs.AI

TL;DR: This paper presents a systematic investigation of Visual Spatial Reasoning (VSR) in Vision-Language Models, categorizing spatial intelligence into three capability levels and introducing SIBench, a comprehensive benchmark with 20 datasets across 23 tasks.


<details>
  <summary>Details</summary>
Motivation: Visual Spatial Reasoning is a critical cognitive ability for advancing embodied intelligence and autonomous systems, but achieving human-level VSR remains challenging despite recent progress in VLMs due to the complexity of 3D spatial representation and reasoning.

Method: The study conducts a systematic review of VSR methodologies across input modalities, model architectures, training strategies, and reasoning mechanisms. It categorizes spatial intelligence into three levels (basic perception, spatial understanding, spatial planning) and creates SIBench benchmark with nearly 20 datasets across 23 task settings.

Result: Experiments with state-of-the-art VLMs reveal a significant gap between perception and reasoning - models perform well in basic perceptual tasks but consistently underperform in understanding and planning tasks, especially in numerical estimation, multi-view reasoning, temporal dynamics, and spatial imagination.

Conclusion: The findings highlight substantial challenges in achieving spatial intelligence while providing both a systematic roadmap and comprehensive benchmark (SIBench) to drive future VSR research in VLMs.

Abstract: Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.

</details>


### [223] [Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](https://arxiv.org/abs/2509.18942)
*Xiao Han,Zimo Zhao,Wanyu Wang,Maolin Wang,Zitao Liu,Yi Chang,Xiangyu Zhao*

Main category: cs.AI

TL;DR: DEAL is a novel fine-tuning framework that integrates Low-Rank Adaptation (LoRA) with continuous fine-tuning to address catastrophic forgetting and suboptimal data efficiency in LLMs.


<details>
  <summary>Details</summary>
Motivation: Conventional fine-tuning approaches suffer from catastrophic forgetting and poor data efficiency, limiting their real-world applicability despite the critical role of fine-tuning in adapting LLMs to specific tasks.

Method: DEAL integrates Low-Rank Adaptation (LoRA) with a continuous fine-tuning strategy, incorporating knowledge retention and adaptive parameter update modules to mitigate limitations of existing FT methods while maintaining privacy-preserving efficiency.

Result: Experiments on 15 diverse datasets show DEAL consistently outperforms baseline methods, yielding substantial gains in task accuracy and resource efficiency.

Conclusion: The approach demonstrates potential to advance continual adaptation in LLMs by enhancing task performance while improving resource efficiency.

Abstract: Recent advancements in Large Language Models (LLMs) have emphasized the
critical role of fine-tuning (FT) techniques in adapting LLMs to specific
tasks, especially when retraining from scratch is computationally infeasible.
Fine-tuning enables LLMs to leverage task- or domain-specific data, producing
models that more effectively meet the requirements of targeted applications.
However, con- ventional FT approaches often suffer from catastrophic forgetting
and suboptimal data efficiency, limiting their real-world applicability. To
address these challenges, this paper proposes DEAL, a novel framework that
integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.
By incorporating knowledge retention and adaptive parameter update modules, the
framework mitigates the lim- itations of existing FT methods while maintaining
efficiency in privacy-preserving settings. Experiments on 15 diverse datasets
show that DEAL consistently outper- forms baseline methods, yielding
substantial gains in task accuracy and resource efficiency. These findings
demonstrate the potential of our approach to advance continual adaptation in
LLMs by enhancing task performance while improving resource efficiency.

</details>


### [224] [LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](https://arxiv.org/abs/2509.18970)
*Xixun Lin,Yucheng Ning,Jingwen Zhang,Yan Dong,Yilong Liu,Yongxuan Wu,Xiaohua Qi,Nan Sun,Yanmin Shang,Pengfei Cao,Lixin Zou,Xu Chen,Chuan Zhou,Jia Wu,Shirui Pan,Bin Wang,Yanan Cao,Kai Chen,Songlin Hu,Li Guo*

Main category: cs.AI

TL;DR: This paper presents the first comprehensive survey on hallucinations in LLM-based agents, analyzing their workflow, proposing a taxonomy of hallucination types, examining triggering causes, and summarizing mitigation approaches.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents are increasingly deployed in real-world applications but remain vulnerable to hallucinations that undermine system reliability, requiring systematic understanding and consolidation of recent advances.

Method: The authors analyze the complete workflow of LLM-based agents, propose a new taxonomy of hallucinations occurring at different stages, examine 18 triggering causes, and review existing studies on mitigation and detection approaches.

Result: The survey provides a comprehensive framework for understanding agent hallucinations, including their types, causes, and potential solutions, serving as a foundation for future research.

Conclusion: This work aims to inspire further efforts toward addressing hallucinations in LLM-based agents to develop more robust and reliable agent systems, highlighting promising research directions.

Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.

</details>


### [225] [From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system](https://arxiv.org/abs/2509.18980)
*Maxime Manderlier,Fabian Lecron,Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.AI

TL;DR: This paper investigates using LLMs to generate user-facing explanations from an interpretable recommendation model, evaluating explanation quality through user studies rather than automatic metrics.


<details>
  <summary>Details</summary>
Motivation: Many explainable AI works rely on automatic evaluation metrics that fail to capture users' actual needs and perceptions. The researchers aim to adopt a user-centered approach to evaluate explanation quality.

Method: The study uses constrained matrix factorization with explicit user types and interpretable scores, then translates this structure into natural language explanations using carefully designed LLM prompts. They conduct a study with 326 participants assessing explanations across five dimensions.

Result: All explanation types were generally well received with moderate statistical differences between strategies. User comments provided complementary insights beyond quantitative results.

Conclusion: LLMs can effectively generate user-facing explanations from interpretable recommendation models, and user-centered evaluation provides valuable insights that automatic metrics miss.

Abstract: We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.

</details>


### [226] [Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)](https://arxiv.org/abs/2509.18986)
*Erik Penther,Michael Grohs,Jana-Rebecca Rehse*

Main category: cs.AI

TL;DR: Comparison of four remaining time prediction approaches in a real-life warehouse process, finding deep learning models achieve highest accuracy but shallow methods like boosting are competitive with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: To evaluate different approaches for predictive process monitoring, specifically remaining time prediction, in a real-world logistics warehouse context using a substantial dataset.

Method: Compared four different remaining time prediction approaches using a real-life outbound warehouse process event log with 169,523 traces from an aviation logistics company.

Result: Deep learning models achieved the highest accuracy, but shallow methods like conventional boosting techniques achieved competitive accuracy while requiring significantly fewer computational resources.

Conclusion: While deep learning provides the best accuracy, shallow methods offer a practical alternative with good performance and much lower computational requirements, making them suitable for resource-constrained environments.

Abstract: Predictive process monitoring is a sub-domain of process mining which aims to
forecast the future of ongoing process executions. One common prediction target
is the remaining time, meaning the time that will elapse until a process
execution is completed. In this paper, we compare four different remaining time
prediction approaches in a real-life outbound warehouse process of a logistics
company in the aviation business. For this process, the company provided us
with a novel and original event log with 169,523 traces, which we can make
publicly available. Unsurprisingly, we find that deep learning models achieve
the highest accuracy, but shallow methods like conventional boosting techniques
achieve competitive accuracy and require significantly fewer computational
resources.

</details>


### [227] [Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action](https://arxiv.org/abs/2509.19030)
*Victoire Hervé,Henrik Warpefelt,Christoph Salge*

Main category: cs.AI

TL;DR: The paper introduces three nested concepts (Landmarks, Monuments, Beacons) for automated decomposition of procedurally generated content to improve algorithmic evaluation that aligns with human experience.


<details>
  <summary>Details</summary>
Motivation: Current algorithmic evaluation of procedurally generated content struggles to find metrics that align with human experience, especially for composite artifacts. There's a need for automatic decomposition methods that can identify salient sub-components.

Method: Drawing on Games Studies and Game AI research, the authors propose three player-centric concepts based on perceivability, evocativeness, and Call to Action. These concepts are designed to be generic across game genres and can be evaluated using existing techniques from research and industry.

Result: The paper presents a framework for automated decomposition of PCG content using Landmarks, Monuments, and Beacons, which can be identified and evaluated with current techniques.

Conclusion: This approach bridges humanities and technical game research, enabling better computational PCG evaluation. While emphasizing mixed-initiative and compositional PCG, the concepts apply beyond these domains and open a path toward fully automated PCG decomposition.

Abstract: Algorithmic evaluation of procedurally generated content struggles to find
metrics that align with human experience, particularly for composite artefacts.
Automatic decomposition as a possible solution requires concepts that meet a
range of properties. To this end, drawing on Games Studies and Game AI
research, we introduce the nested concepts of \textit{Landmarks},
\textit{Monuments}, and \textit{Beacons}. These concepts are based on the
artefact's perceivability, evocativeness, and Call to Action, all from a
player-centric perspective. These terms are generic to games and usable across
genres. We argue that these entities can be found and evaluated with techniques
currently used in both research and industry, opening a path towards a fully
automated decomposition of PCG, and evaluation of the salient sub-components.
Although the work presented here emphasises mixed-initiative PCG and
compositional PCG, we believe it applies beyond those domains. With this
approach, we intend to create a connection between humanities and technical
game research and allow for better computational PCG evaluation

</details>


### [228] [Towards Causal Representation Learning with Observable Sources as Auxiliaries](https://arxiv.org/abs/2509.19058)
*Kwonho Kim,Heejeong Nam,Inwoo Hwang,Sanghack Lee*

Main category: cs.AI

TL;DR: This paper introduces a framework for causal representation learning using observable sources as auxiliary variables to identify latent factors up to subspace-wise transformations and permutations with volume-preserving encoders.


<details>
  <summary>Details</summary>
Motivation: Prior causal representation learning methods limit auxiliary variables to be external to the mixing function, but system-driving latent factors can sometimes be easily observed or extracted from data, which could facilitate identification.

Method: The authors propose using observable sources as auxiliary variables for conditioning, employ volume-preserving encoders for identification, and introduce a variable-selection scheme to choose optimal auxiliary variables when multiple are available.

Result: The framework demonstrates effectiveness through experiments on synthetic graph and image data, showing it can identify entire latent variables up to subspace-wise transformations and permutations.

Conclusion: This work extends the boundaries of current causal representation learning approaches by leveraging observable sources as auxiliary variables, providing a more flexible and effective framework for latent factor identification.

Abstract: Causal representation learning seeks to recover latent factors that generate
observational data through a mixing function. Needing assumptions on latent
structures or relationships to achieve identifiability in general, prior works
often build upon conditional independence given known auxiliary variables.
However, prior frameworks limit the scope of auxiliary variables to be external
to the mixing function. Yet, in some cases, system-driving latent factors can
be easily observed or extracted from data, possibly facilitating
identification. In this paper, we introduce a framework of observable sources
being auxiliaries, serving as effective conditioning variables. Our main
results show that one can identify entire latent variables up to subspace-wise
transformations and permutations using volume-preserving encoders. Moreover,
when multiple known auxiliary variables are available, we offer a
variable-selection scheme to choose those that maximize recoverability of the
latent factors given knowledge of the latent causal graph. Finally, we
demonstrate the effectiveness of our framework through experiments on synthetic
graph and image data, thereby extending the boundaries of current approaches.

</details>


### [229] [Code Driven Planning with Domain-Adaptive Critic](https://arxiv.org/abs/2509.19077)
*Zikang Tian,Shaohui Peng,Du Huang,Jiaming Guo,Ruizhi Chen,Rui Zhang,Xishan Zhang,Yuxuan Guo,Zidong Du,Qi Guo,Ling Li,Yewen Pu,Xing Hu,Yunji Chen*

Main category: cs.AI

TL;DR: CoPiC is a novel LLM-based planning framework that uses high-level planning programs and a domain-adaptive critic to reduce query costs while improving long-term planning performance.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based planners rely on frequent queries for iterative refinement, which is costly and focuses on short-term feedback rather than long-term rewards.

Method: CoPiC generates diverse high-level planning programs using LLMs, then uses a trained domain-adaptive critic to evaluate and select plans aligned with long-term rewards.

Result: CoPiC outperforms AdaPlanner and Reflexion with 23.33% improvement in success rate and 91.27% reduction in query costs across ALFWorld, NetHack, and StarCraft II.

Conclusion: The approach demonstrates that combining programmatic planning with domain-adaptive critics enables more efficient and effective long-term planning while significantly reducing LLM query costs.

Abstract: Large Language Models (LLMs) have been widely adopted as task planners for AI
agents in sequential decision-making problems, leveraging their extensive world
knowledge. However, the gap between their general knowledge and
environment-specific requirements often leads to inaccurate plans. To address
this, existing approaches rely on frequent LLM queries to iteratively refine
plans based on immediate environmental feedback, which incurs substantial query
costs. However, this refinement is typically guided by short-term environmental
feedback, limiting LLMs from developing plans aligned with long-term rewards.
We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of
relying on frequent queries, CoPiC employs LLMs to generate a diverse set of
high-level planning programs, which iteratively produce and refine candidate
plans. A trained domain-adaptive critic then evaluates these candidates and
selects the one most aligned with long-term rewards for execution. Using
high-level planning programs as planner and domain-adaptive critic as
estimator, CoPiC improves planning while significantly reducing query costs.
Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC
outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving
an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in
query costs.

</details>


### [230] [AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration](https://arxiv.org/abs/2509.19236)
*Chunhao Tian,Yutong Wang,Xuebo Liu,Zhexuan Wang,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentInit is a novel initialization method for multi-agent systems that optimizes agent team structure through multi-round interactions, reflections, and balanced selection strategies to enhance collaboration and system performance.


<details>
  <summary>Details</summary>
Motivation: Existing MAS initialization methods fail to adequately address the collaborative needs of agents in subsequent stages, leading to suboptimal system efficiency and effectiveness.

Method: AgentInit incorporates multi-round interactions and reflections between agents during generation, uses a Natural Language to Format mechanism for consistency, and applies balanced team selection strategies based on Pareto principles to jointly consider diversity and task relevance.

Result: AgentInit consistently outperforms state-of-the-art initialization methods and pre-defined strategies across various frameworks and tasks, achieving performance improvements of up to 1.2x and 1.6x respectively, while significantly reducing token consumption.

Conclusion: The method demonstrates strong transferability to similar tasks and verifies the effectiveness of its key components, proving its capability and adaptability as a reliable MAS initialization method.

Abstract: Proper initialization is crucial for any system, particularly in multi-agent
systems (MAS), where it plays a pivotal role in determining both the system's
efficiency and effectiveness. However, existing MAS initialization methods do
not fully account for the collaborative needs of the generated agents in
subsequent stages. Inspired by the principles of effective team composition, we
propose AgentInit, which aims to optimize the structure of agent teams.
Specifically, in addition to multi-round interactions and reflections between
agents during agent generation, AgentInit incorporates a Natural Language to
Format mechanism to ensure consistency and standardization. Balanced team
selection strategies using Pareto principles are subsequently applied to
jointly consider agent team diversity and task relevance to promote effective
and efficient collaboration and enhance overall system performance. Experiments
show that AgentInit consistently outperforms state-of-the-art initialization
methods and pre-defined strategies across various frameworks and tasks,
achieving an overall performance improvement of up to 1.2 and 1.6,
respectively, while also significantly reducing token consumption. Further
analysis confirms its strong transferability to similar tasks and verifies the
effectiveness of its key components, demonstrating its capability and
adaptability as a reliable MAS initialization method. Source code and models
are available at https://github.com/1737423697/AgentInit.

</details>


### [231] [Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World](https://arxiv.org/abs/2509.19265)
*Saeed Almheiri,Rania Hossam,Mena Attia,Chenxi Wang,Preslav Nakov,Timothy Baldwin,Fajri Koto*

Main category: cs.AI

TL;DR: Cross-cultural transfer of commonsense reasoning in LLMs using lightweight alignment methods can improve performance across Arab countries and even from out-of-culture contexts like Indonesia and US.


<details>
  <summary>Details</summary>
Motivation: LLMs often reflect Western-centric biases, limiting effectiveness in diverse cultural contexts. Cross-cultural transfer potential using alignment in one culture to improve others remains underexplored.

Method: Used culturally grounded commonsense reasoning dataset covering 13 Arab countries, evaluated lightweight alignment methods (in-context learning, DITTO) alongside baselines like supervised fine-tuning and DPO.

Result: 12 culture-specific examples from one country improved performance in others by 10% on average in multilingual models. Out-of-culture demonstrations from Indonesia and US matched or surpassed in-culture alignment for MCQ reasoning.

Conclusion: Efficient cross-cultural alignment is possible and offers promising approach to adapt LLMs to low-resource cultural settings, demonstrating cultural commonsense transferability beyond the Arab world.

Abstract: Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.

</details>
